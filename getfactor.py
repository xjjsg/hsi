import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from scipy.stats import spearmanr
import os
import math
import glob
import re
import warnings

warnings.filterwarnings('ignore')

# ==============================================================================
# 0. é¡¹ç›®ç¯å¢ƒé…ç½® (Project Configuration)
# ==============================================================================
CONFIG = {
    # --- è·¯å¾„é…ç½® ---
    'DATA_DIR': './data',   # æ•°æ®æ ¹ç›®å½• HSI/data/
    'MAIN_SYMBOL': 'sz159920', # ä¸»åŠ›æ ‡çš„ä»£ç  (æ–‡ä»¶å¤¹å)
    'AUX_SYMBOL':  'sh513130', # è¾…åŠ©æ ‡çš„ä»£ç  (æ–‡ä»¶å¤¹å)
    
    # --- è®­ç»ƒå‚æ•° ---
    'DEVICE': torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    'MAX_LOOKBACK': 120,     
    'HORIZON': 20,           
    'RESAMPLE_FREQ': '3s',   
    'TRAIN_EPOCHS': 30,      # åˆå§‹è®­ç»ƒè½®æ•°
    'FINETUNE_EPOCHS': 15,   # å¾®è°ƒè®­ç»ƒè½®æ•°
    'BARRIER_THRESHOLD': 0.002, 
    
    # --- æ»šåŠ¨çª—å£ ---
    'ROLLING_WINDOW_SIZE': 60, # åªä½¿ç”¨æœ€è¿‘Nå¤©çš„æ•°æ®
    
    # --- è¾“å‡º ---
    'ARTIFACT_NAME': 'FACTOR_STRATEGY_ARTIFACT.pth',
    'FACTOR_LIB_NAME': 'factor_lib_final.csv'
}

print(f"ğŸš€ Factor Factory Multi-Day Engine | Device: {CONFIG['DEVICE']}")

# ==============================================================================
# 1. å¤šæ—¥æ•°æ®åŠ è½½æœåŠ¡ (Multi-Day Data Loader)
# ==============================================================================
class DataLoaderService:
    @staticmethod
    def get_daily_files(symbol):
        """
        æ‰«æ HSI/data/{symbol}/ ä¸‹çš„æ‰€æœ‰ csv æ–‡ä»¶
        è¿”å›å­—å…¸: { '2025-11-26': 'å®Œæ•´è·¯å¾„', ... }
        """
        dir_path = os.path.join(CONFIG['DATA_DIR'], symbol)
        if not os.path.exists(dir_path):
            print(f"âŒ ç›®å½•æœªæ‰¾åˆ°: {dir_path}")
            return {}
        
        # åŒ¹é… symbol-æ—¥æœŸ.csv çš„æ¨¡å¼
        pattern = os.path.join(dir_path, f"{symbol}-*.csv")
        files = glob.glob(pattern)
        
        date_map = {}
        for f in files:
            # æå–æ—¥æœŸ (å‡è®¾æ ¼å¼ä¸º *-YYYY-MM-DD.csv)
            match = re.search(r'(\d{4}-\d{2}-\d{2})', os.path.basename(f))
            if match:
                date_str = match.group(1)
                date_map[date_str] = f
        
        return date_map

    @staticmethod
    def load_single_day(filepath, is_aux=False):
        """åŠ è½½å•æ—¥å•æ–‡ä»¶å¹¶è¿›è¡ŒåŸºç¡€æ¸…æ´—"""
        try:
            raw = pd.read_csv(filepath)
            
            # 1. æ—¶é—´å¤„ç†
            if 'tx_local_time' in raw.columns:
                raw['datetime'] = pd.to_datetime(raw['tx_local_time'], unit='ms')
                if raw['datetime'].dt.tz is None:
                    raw['datetime'] = raw['datetime'].dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')
                df = raw.set_index('datetime').sort_index()
            else:
                df = raw

            # 2. åŠ¨æ€èšåˆ (L1-L5)
            agg_rules = {
                'price': 'last', 'tick_vol': 'sum', 'tick_amt': 'sum', 'tick_vwap': 'mean',
                'premium_rate': 'last', 'sentiment': 'last',
                'bp1':'last', 'bv1':'last', 'sp1':'last', 'sv1':'last'
            }
            if 'index_price' in df.columns: agg_rules['index_price'] = 'last'
            if not is_aux and 'fut_price' in df.columns: agg_rules['fut_price'] = 'last'
            
            for i in range(2, 6):
                if f'bp{i}' in df.columns:
                    agg_rules[f'bp{i}'] = 'last'; agg_rules[f'bv{i}'] = 'last'
                    agg_rules[f'sp{i}'] = 'last'; agg_rules[f'sv{i}'] = 'last'

            # 3. é‡é‡‡æ ·
            df = df.resample(CONFIG['RESAMPLE_FREQ']).agg(agg_rules).ffill().dropna()

            # 4. åŸºç¡€ç‰¹å¾ (æ—¥å†…ç‹¬ç«‹è®¡ç®—)
            df['mid_price'] = (df['bp1'] + df['sp1']) / 2
            df['log_ret'] = np.log(df['mid_price'] / df['mid_price'].shift(1)).fillna(0)
            
            depth_l1 = df['bv1'] + df['sv1'] + 1e-6
            df['feat_imb'] = (df['bv1'] - df['sv1']) / depth_l1
            df['feat_spread'] = (df['sp1'] - df['bp1']) / df['mid_price']
            
            if 'bv5' in df.columns:
                bv_all = sum(df[f'bv{i}'] for i in range(1, 6))
                sv_all = sum(df[f'sv{i}'] for i in range(1, 6))
                df['feat_depth_total'] = bv_all + sv_all
                df['feat_imb_5'] = (bv_all - sv_all) / (df['feat_depth_total'] + 1e-6)
            
            if 'bp5' in df.columns:
                df['feat_bid_slope'] = (df['bp1'] - df['bp5']) / 5
                df['feat_ask_slope'] = (df['sp5'] - df['sp1']) / 5
            else:
                df['feat_bid_slope'] = 0; df['feat_ask_slope'] = 0
            
            depth_amt = (df['bv1'] * df['bp1']) + (df['sv1'] * df['sp1'])
            df['feat_trade_intensity'] = df['tick_amt'] / (depth_amt + 1e-6)

            df['feat_vol_chg'] = np.log(df['tick_vol'] + 1).diff().fillna(0)
            ma_20 = df['mid_price'].rolling(20).mean()
            std_20 = df['mid_price'].rolling(20).std()
            df['feat_z_score'] = (df['mid_price'] - ma_20) / (std_20 + 1e-6)

            if is_aux: df = df.add_prefix('ctx_')
            
            return df
        except Exception as e:
            print(f"âŒ è¯»å–é”™è¯¯ {filepath}: {e}")
            return None

# ==============================================================================
# 2. ç¥ç»ç½‘ç»œç»„ä»¶ (Model Components - ä¿æŒä¸å˜)
# ==============================================================================
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())
    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)

class InceptionBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(InceptionBlock, self).__init__()
        self.b1 = nn.Conv1d(in_channels, out_channels//4, 1)
        self.b2 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, 1), nn.Conv1d(out_channels//4, out_channels//4, 3, padding=1))
        self.b3 = nn.Sequential(nn.Conv1d(in_channels, out_channels//4, 1), nn.Conv1d(out_channels//4, out_channels//4, 5, padding=2))
        self.b4 = nn.Sequential(nn.MaxPool1d(3, 1, 1), nn.Conv1d(in_channels, out_channels//4, 1))
    def forward(self, x): return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)

class AlphaLayer(nn.Module):
    def __init__(self, input_dim, window=20):
        super(AlphaLayer, self).__init__()
        self.window = window
        self.pool = nn.AvgPool1d(kernel_size=window, stride=1, padding=0)
    def forward(self, x):
        x = x.permute(0, 2, 1)
        pad = torch.zeros(x.shape[0], x.shape[1], self.window-1).to(x.device)
        x_pad = torch.cat([pad, x], dim=2)
        mean = self.pool(x_pad)
        std = torch.sqrt(torch.clamp(self.pool(torch.cat([pad, x**2], dim=2)) - mean**2, min=1e-6))
        return torch.cat([x, mean, std], dim=1).permute(0, 2, 1)

class TemporalBlock(nn.Module):
    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout=0.2):
        super(TemporalBlock, self).__init__()
        padding = (kernel_size - 1) * dilation
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, padding=padding, dilation=dilation)
        self.relu1 = nn.GELU()
        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, padding=padding, dilation=dilation)
        self.relu2 = nn.GELU()
        self.chomp = padding 
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None
    def forward(self, x):
        out = self.conv1(x)[:, :, :-self.chomp] 
        out = self.relu1(out)
        out = self.conv2(out)[:, :, :-self.chomp]
        out = self.relu2(out)
        res = x if self.downsample is None else self.downsample(x)
        return out + res

class SinusoidalPosEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div)
        pe[:, 1::2] = torch.cos(position * div)
        self.register_buffer('pe', pe.unsqueeze(0).transpose(0, 1))
    def forward(self, x):
        return x + self.pe[:x.size(0), :]

class Direction(nn.Module):
    def __init__(self, input_dim, d_model=256, nhead=16, num_layers=6, num_classes=3):
        super(Direction, self).__init__()
        self.stem = nn.Sequential(nn.Conv1d(input_dim, 64, 1), nn.BatchNorm1d(64), nn.LeakyReLU())
        self.inception = InceptionBlock(64, d_model)
        self.se = SEBlock(d_model)
        self.pos_encoder = SinusoidalPosEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=512, dropout=0.1)
        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.fc = nn.Sequential(nn.Linear(d_model, 128), nn.GELU(), nn.Dropout(0.3), nn.Linear(128, num_classes))
    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.se(self.inception(self.stem(x))) 
        x = x.permute(2, 0, 1) 
        x = self.pos_encoder(x)
        x = self.transformer(x)
        return self.fc(x.mean(dim=0))

class HybridMinerNet(nn.Module):
    def __init__(self, input_dim, d_model=128, num_layers=3, n_factors=128):
        super(HybridMinerNet, self).__init__()
        self.alpha_layer = AlphaLayer(input_dim)
        self.tcn = TemporalBlock(input_dim*3, d_model, kernel_size=3, dilation=1)
        self.pos_encoder = SinusoidalPosEncoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=256, dropout=0.1)
        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.factor_head = nn.Sequential(nn.Linear(d_model, 256), nn.GELU(), nn.Linear(256, n_factors), nn.Tanh())
        self.predictor = nn.Linear(n_factors, 1)

    def forward(self, x):
        x = self.alpha_layer(x)
        x = x.permute(0, 2, 1)
        x = self.tcn(x)
        x = x.permute(2, 0, 1)
        x = self.pos_encoder(x)
        factors = self.factor_head(self.transformer(x)[-1]) 
        return self.predictor(factors), factors

# ==============================================================================
# 3. æ‰‹å·¥å› å­ç”Ÿæˆå™¨ (Manual Factor Injection)
# ==============================================================================
class ManualFactorGenerator:
    def process(self, df):
        # é’ˆå¯¹å•æ—¥ DataFrame è¿›è¡Œå¤„ç†ï¼Œæ— éœ€æ‹…å¿ƒè·¨æ—¥é—®é¢˜
        res = pd.DataFrame(index=df.index)
        
        # 1. èµ„é‡‘æµ
        db = df['bp1'].diff(); ds = df['sp1'].diff()
        dvb = df['bv1'].diff(); dvs = df['sv1'].diff()
        delta_vb = np.select([db > 0, db < 0], [df['bv1'], 0], default=dvb)
        delta_va = np.select([ds > 0, ds < 0], [0, df['sv1']], default=dvs)
        voi = delta_vb - delta_va
        res['alpha_voi_raw'] = voi
        
        vol_ma = df['tick_vol'].rolling(10).mean().replace(0, 1)
        res['alpha_voi_smart'] = voi / vol_ma

        if 'tick_vwap' in df.columns:
            res['alpha_vwap_bias'] = (df['tick_vwap'] - df['mid_price']) / df['mid_price']

        # 2. æ·±åº¦å¾®è§‚ (L5)
        if 'bv5' in df.columns:
            sum_bid = sum(df[f'bv{i}'] for i in range(1, 6))
            sum_ask = sum(df[f'sv{i}'] for i in range(1, 6))
            total_depth = sum_bid + sum_ask + 1e-6
            res['alpha_depth_imb_l5'] = (sum_bid - sum_ask) / total_depth
            res['alpha_wall_bid'] = df['bv5'] / (df['bv1'] + 1)
            res['alpha_wall_ask'] = df['sv5'] / (df['sv1'] + 1)

        l1_imb = df['bv1'] / (df['bv1'] + df['sv1'] + 1e-6)
        micro_price = df['bp1'] * (1 - l1_imb) + df['sp1'] * l1_imb
        res['alpha_micro_dev'] = (micro_price - df['mid_price']) / df['mid_price']

        # 3. è·¨å“ç§
        if 'ctx_mid_price' in df.columns:
            res['alpha_cross_rs'] = df['log_ret'] - df['ctx_log_ret']
            ctx_lag_2 = df['ctx_log_ret'].shift(2).fillna(0)
            res['alpha_cross_lead_lag'] = ctx_lag_2 - df['log_ret']
            
            spread = np.log(df['mid_price']) - np.log(df['ctx_mid_price'])
            spread_mean = spread.rolling(120).mean()
            spread_std = spread.rolling(120).std()
            res['alpha_cross_arb_z'] = (spread - spread_mean) / (spread_std + 1e-6)
            
            if 'sentiment' in df.columns and 'ctx_sentiment' in df.columns:
                res['alpha_sent_gap'] = df['sentiment'] - df['ctx_sentiment']

        # 4. åœºæ™¯
        minutes = df.index.hour * 60 + df.index.minute
        res['feat_time_norm'] = (minutes - 570) / (900 - 570)
        
        mask_late = minutes >= 890 
        res['logic_market_closing'] = 0.0
        res.loc[mask_late, 'logic_market_closing'] = 1.0
        
        mask_open = (minutes >= 570) & (minutes <= 575)
        res['logic_market_opening'] = 0.0
        res.loc[mask_open, 'logic_market_opening'] = 1.0

        if 'fut_price' in df.columns:
            fut_ret = df['fut_price'].pct_change().fillna(0)
            res['alpha_fut_lead'] = fut_ret - df['log_ret']

        return res.fillna(0)

# ==============================================================================
# 4. æ·±åº¦æ¨¡å‹ç®¡ç†å™¨ (æ”¯æŒå¤šæ—¥ã€æ»šåŠ¨å¾®è°ƒ)
# ==============================================================================
class DeepModelManager:
    def __init__(self, name, model_cls, input_cols, lookback, n_factors=128, is_cls=False):
        self.name = name
        self.model_cls = model_cls
        self.input_cols = input_cols
        self.lookback = lookback
        self.n_factors = n_factors
        self.is_cls = is_cls
        self.trained_model = None
        self.trained_scaler = None

    def load_checkpoint(self, path, device):
        """åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆçƒ­å¯åŠ¨çš„å…³é”®ï¼‰"""
        if os.path.exists(path):
            try:
                # ã€ä¿®æ”¹ç‚¹ã€‘å¼ºåˆ¶å…è®¸åŠ è½½æ‰€æœ‰ Python å¯¹è±¡ (å¦‚ sklearn scaler)
                checkpoint = torch.load(path, map_location=device, weights_only=False)
                
                model_key = f"{self.name}_state_dict"
                if model_key in checkpoint['models']:
                    print(f"   -> å‘ç°é¢„è®­ç»ƒæƒé‡: {model_key}")
                    return checkpoint['models'][model_key]
            except Exception as e:
                print(f"   -> è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None

    def _prepare_single_day(self, df, fit=False, scaler=None):
        """å¯¹å•æ—¥æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å’Œæ»‘çª—åˆ‡ç‰‡ï¼Œé¿å…è·¨æ—¥æ±¡æŸ“"""
        raw = df[self.input_cols].values
        raw = np.nan_to_num(raw, nan=0.0, posinf=0.0, neginf=0.0)
        
        if scaler:
            data = scaler.transform(raw)
        else:
            data = raw
            
        # æ»‘çª—åˆ‡ç‰‡
        X_list = []
        for i in range(self.lookback, len(data)):
            X_list.append(data[i-self.lookback : i])
        
        if len(X_list) == 0: return None, None
        X = np.array(X_list)
        
        # ç”Ÿæˆæ ‡ç­¾ (ä»… Training é˜¶æ®µéœ€è¦)
        y = None
        if fit:
            if self.is_cls:
                # ä¸‰åŠ¿å’
                prices = df['mid_price'].values
                labels = np.zeros(len(data))
                horizon = CONFIG['HORIZON']
                threshold = CONFIG['BARRIER_THRESHOLD']
                
                valid_len = len(prices) - horizon
                for i in range(valid_len):
                    curr = prices[i]
                    future_window = prices[i+1 : i+horizon+1]
                    if np.any(future_window >= curr * (1 + threshold)):
                        labels[i] = 1 # æ¶¨
                    elif np.any(future_window <= curr * (1 - threshold)):
                        labels[i] = 2 # è·Œ
                y = labels[self.lookback:]
            else:
                # æ³¢åŠ¨ç‡
                vol = df['log_ret'].rolling(20).std().shift(-20).values * 10000
                y = np.nan_to_num(vol[self.lookback:], nan=0)
        
        return X, y

    def train(self, df_list, pretrained_path=None, production_mode=True):
        mode_str = "å®ç›˜å…¨é‡ (Production)" if production_mode else "å›æµ‹ (Backtest)"
        print(f"\nğŸ”„ [Training] å¼€å§‹è®­ç»ƒæ¨¡å‹: {self.name} | æ¨¡å¼: {mode_str}")
        
        # 1. Scaler Fit (å§‹ç»ˆä½¿ç”¨å½“å‰æ•°æ® Fitï¼Œä¿æŒå¯¹å½“å‰æ³¢åŠ¨ç‡çš„æ•æ„Ÿ)
        print("   -> è®¡ç®—å…¨å±€ç»Ÿè®¡é‡ (Scaler Fit)...")
        all_raw_data = [df[self.input_cols].values for df in df_list]
        full_matrix = np.concatenate(all_raw_data, axis=0)
        full_matrix = np.nan_to_num(full_matrix, nan=0.0, posinf=0.0, neginf=0.0)
        
        self.trained_scaler = StandardScaler()
        self.trained_scaler.fit(full_matrix)
        del full_matrix, all_raw_data 

        # 2. å‡†å¤‡è®­ç»ƒé›†
        if production_mode:
            train_df_list = df_list # å®ç›˜ç”¨æ‰€æœ‰æ•°æ®
        else:
            train_days = int(len(df_list) * 0.8) # å›æµ‹ç•™ä¸€éƒ¨åˆ†éªŒè¯
            train_df_list = df_list[:train_days]
            
        X_all, y_all = [], []
        for df in train_df_list:
            X_day, y_day = self._prepare_single_day(df, fit=True, scaler=self.trained_scaler)
            if X_day is not None:
                min_len = min(len(X_day), len(y_day))
                X_all.append(X_day[:min_len])
                y_all.append(y_day[:min_len])
        
        X_train = np.concatenate(X_all, axis=0)
        y_train = np.concatenate(y_all, axis=0)
        print(f"   -> è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train)}")
        
        ds = TensorDataset(torch.FloatTensor(X_train), 
                           torch.LongTensor(y_train) if self.is_cls else torch.FloatTensor(y_train))
        dl = DataLoader(ds, batch_size=64, shuffle=True)
        
        # 3. åˆå§‹åŒ–æ¨¡å‹
        input_dim = len(self.input_cols)
        if self.is_cls:
            model = self.model_cls(input_dim, d_model=256, num_layers=6).to(CONFIG['DEVICE'])
            loss_fn = nn.CrossEntropyLoss(weight=torch.FloatTensor([0.2, 1.0, 1.0]).to(CONFIG['DEVICE']))
            base_lr = 5e-5
        else:
            model = self.model_cls(input_dim, d_model=128, num_layers=3).to(CONFIG['DEVICE'])
            loss_fn = nn.MSELoss()
            base_lr = 1e-4

        # 4. çƒ­å¯åŠ¨ (Warm Start)
        is_finetuning = False
        if pretrained_path:
            state_dict = self.load_checkpoint(pretrained_path, CONFIG['DEVICE'])
            if state_dict:
                try:
                    model.load_state_dict(state_dict)
                    print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œè¿›å…¥å¾®è°ƒæ¨¡å¼...")
                    is_finetuning = True
                    base_lr = base_lr * 0.2 # å¾®è°ƒæ—¶é™ä½å­¦ä¹ ç‡
                except Exception as e:
                    print(f"âš ï¸ æƒé‡åŠ è½½å¤±è´¥ (ç»“æ„å¯èƒ½å·²å˜æ›´): {e}")

        # è®¾ç½® Epochs
        epochs = CONFIG['FINETUNE_EPOCHS'] if is_finetuning else CONFIG['TRAIN_EPOCHS']
        
        opt = optim.AdamW(model.parameters(), lr=base_lr)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=1e-7)
        
        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for bx, by in dl:
                bx, by = bx.to(CONFIG['DEVICE']), by.to(CONFIG['DEVICE'])
                opt.zero_grad()
                out = model(bx)
                loss = loss_fn(out if self.is_cls else out[0].squeeze(), by)
                loss.backward()
                opt.step()
                total_loss += loss.item()
            
            scheduler.step()
            if (epoch+1) % 5 == 0 or epoch == epochs-1:
                print(f"   Epoch {epoch+1:02d}/{epochs} | Loss: {total_loss/len(dl):.6f} | LR: {opt.param_groups[0]['lr']:.2e}")
        
        self.trained_model = model
        return model

    def process(self, df_list, pretrained_path=None, production_mode=True):
        # è®­ç»ƒ
        model = self.train(df_list, pretrained_path=pretrained_path, production_mode=production_mode)
        model.eval()
        
        # æ¨ç† (ç”Ÿæˆå› å­)
        print("   -> å¼€å§‹ç”Ÿæˆå› å­æ•°æ®...")
        all_results_df = []
        
        for df in df_list:
            X_day, _ = self._prepare_single_day(df, fit=False, scaler=self.trained_scaler)
            if X_day is None: continue
            
            X_tensor = torch.FloatTensor(X_day).to(CONFIG['DEVICE'])
            outs = []
            with torch.no_grad():
                for i in range(0, len(X_tensor), 256):
                    batch = X_tensor[i:i+256]
                    res = model(batch)
                    if self.is_cls:
                        prob = torch.softmax(res, dim=1)
                        outs.append((prob[:,1]-prob[:,2]).cpu().numpy())
                    else:
                        outs.append(res[1].cpu().numpy())
            
            vals = np.concatenate(outs)
            day_res = pd.DataFrame(index=df.index[self.lookback:])
            
            min_l = min(len(vals), len(day_res))
            day_res = day_res.iloc[:min_l]
            vals = vals[:min_l]
            
            if self.is_cls:
                day_res[f'alpha_{self.name}_score'] = vals
            else:
                for k in range(vals.shape[1]): day_res[f'alpha_{self.name}_{k:03d}'] = vals[:, k]
            
            all_results_df.append(day_res)
            
        return pd.concat(all_results_df, axis=0)

# ==============================================================================
# 5. ä¸»æµç¨‹ (Multi-Day Pipeline)
# ==============================================================================
def main():
    # 1. æ‰«ææ‰€æœ‰æ—¥æœŸæ–‡ä»¶
    print(f"ğŸ“‚ æ‰«ææ•°æ®ç›®å½•: {CONFIG['DATA_DIR']} ...")
    main_files = DataLoaderService.get_daily_files(CONFIG['MAIN_SYMBOL'])
    aux_files = DataLoaderService.get_daily_files(CONFIG['AUX_SYMBOL'])
    
    # æ‰¾äº¤é›†æ—¥æœŸ
    common_dates = sorted(list(set(main_files.keys()) & set(aux_files.keys())))
    
    # --- æ»šåŠ¨çª—å£é€»è¾‘ (Rolling Window) ---
    if len(common_dates) > CONFIG['ROLLING_WINDOW_SIZE']:
        print(f"âœ‚ï¸ æ•°æ®è¶…è¿‡ {CONFIG['ROLLING_WINDOW_SIZE']} å¤©ï¼Œè¿›è¡Œæ»šåŠ¨æˆªæ–­...")
        training_dates = common_dates[-CONFIG['ROLLING_WINDOW_SIZE']:]
    else:
        training_dates = common_dates
        
    print(f"âœ… æœ€ç»ˆçº³å…¥è®¡ç®—æ—¥æœŸ: {training_dates[0]} ~ {training_dates[-1]} (å…± {len(training_dates)} å¤©)")
    
    if not training_dates:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®æ–‡ä»¶")
        return

    # 2. é€æ—¥åŠ è½½ã€åˆå¹¶ã€æ³¨å…¥å› å­ (é¢„å¤„ç†)
    daily_df_list = []
    manual_gen = ManualFactorGenerator()
    
    print("\nâš¡ [Preprocessing] é€æ—¥æ¸…æ´—ä¸å› å­æ³¨å…¥...")
    for date in training_dates:
        f_main = main_files[date]
        f_aux = aux_files[date]
        
        df_m = DataLoaderService.load_single_day(f_main, is_aux=False)
        df_a = DataLoaderService.load_single_day(f_aux, is_aux=True)
        
        if df_m is None or df_a is None: continue
        
        df_day = df_m.join(df_a, how='inner')
        if len(df_day) < 200: continue 
        
        df_manual = manual_gen.process(df_day)
        df_final = df_day.join(df_manual, how='inner')
        
        daily_df_list.append(df_final)
        
    print(f"ğŸ“Š é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆå¤©æ•°: {len(daily_df_list)}")
    if not daily_df_list: return

    # 3. ç¡®å®šç‰¹å¾åˆ—è¡¨
    sample_df = daily_df_list[0]
    excludes = ['tx_server_time', 'datetime']
    feats = [c for c in sample_df.columns if c not in excludes and np.issubdtype(sample_df[c].dtype, np.number)]
    print(f"ğŸ”¹ æ¨¡å‹ç‰¹å¾ç»´åº¦: {len(feats)}")

    # 4. æ£€æŸ¥æ˜¯å¦æœ‰æ—§æ¨¡å‹ (ç”¨äºçƒ­å¯åŠ¨)
    pretrained_path = CONFIG['ARTIFACT_NAME'] if os.path.exists(CONFIG['ARTIFACT_NAME']) else None
    
    # 5. åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    dir_mgr = DeepModelManager("direction", Direction, feats, lookback=CONFIG['MAX_LOOKBACK'], is_cls=True)
    miner_mgr = DeepModelManager("miner", HybridMinerNet, feats, lookback=60, n_factors=128, is_cls=False)

    # 6. è®­ç»ƒä¸æ¨ç† (Production Mode = True)
    # ä¼ å…¥ pretrained_path å°è¯•è¿›è¡Œå¾®è°ƒ
    res_dir = dir_mgr.process(daily_df_list, pretrained_path=pretrained_path, production_mode=True)#æ³¨æ„å›æµ‹æ”¹è¿™é‡Œ
    res_miner = miner_mgr.process(daily_df_list, pretrained_path=pretrained_path, production_mode=True)#è¿˜æœ‰è¿™é‡Œ
    
    # 7. æ‰‹å·¥å› å­åˆå¹¶
    manual_cols = [c for c in sample_df.columns if c.startswith('alpha_') or c.startswith('logic_')]
    res_manual_list = [day[manual_cols].iloc[CONFIG['MAX_LOOKBACK']:] for day in daily_df_list]
    res_manual = pd.concat(res_manual_list, axis=0)

    # 8. æœ€ç»ˆåˆå¹¶ä¸ç­›é€‰
    final_df = pd.concat([res_dir, res_miner, res_manual], axis=1).dropna()
    
    # æ„é€  Target è®¡ç®— IC
    target_list = []
    for day in daily_df_list:
        t = day['log_ret'].shift(-20).iloc[CONFIG['MAX_LOOKBACK']:]
        target_list.append(t)
    target = pd.concat(target_list, axis=0).reindex(final_df.index).fillna(0)
    
    print("\nğŸ” è®¡ç®— IC å¹¶ç­›é€‰...")
    ic_map = {}
    for c in final_df.columns:
        if final_df[c].std() == 0: continue
        corr = spearmanr(final_df[c].values, target.values)[0]
        if not np.isnan(corr): ic_map[c] = abs(corr)
    
    selected_factors = sorted(ic_map.keys(), key=lambda x: ic_map[x], reverse=True)[:135]
    final_output = final_df[selected_factors]

    # 9. ä¿å­˜æˆæœ
    strategy_artifact = {
        'meta': {
            'description': 'Multi-Day Hybrid Strategy (Rolling Updated)',
            'train_dates': training_dates,
            'rolling_window': CONFIG['ROLLING_WINDOW_SIZE'],
            'input_feature_count': len(feats),
            'output_factor_count': len(selected_factors)
        },
        'features': {'input_names': feats, 'output_names': selected_factors},
        'models': {
            'direction_state_dict': dir_mgr.trained_model.state_dict(),
            'direction_scaler': dir_mgr.trained_scaler,
            'miner_state_dict': miner_mgr.trained_model.state_dict(),
            'miner_scaler': miner_mgr.trained_scaler,
        }
    }
    
    torch.save(strategy_artifact, CONFIG['ARTIFACT_NAME'])
    final_output.to_csv(CONFIG['FACTOR_LIB_NAME'])
    
    print(f"\nâœ… å…¨éƒ¨å®Œæˆ! å·²æ›´æ–°æ¨¡å‹å¹¶ä¿å­˜å› å­åº“ã€‚")

if __name__ == "__main__":
    main()