import os
import glob
import warnings
import math
from datetime import time
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report

# ==========================================
# 0. å…¨å±€é…ç½® (V4 - å®ç›˜å¯äº¤æ˜“æ ‡å‡†)
# ==========================================
CONFIG = {
    # --- è·¯å¾„ä¸æ ‡çš„ ---
    "DATA_DIR": ".",              
    "MAIN_SYMBOL": "sz159920",
    "AUX_SYMBOL": "sh513130",     
    
    # --- æ—¶åºç»“æ„ ---
    "RESAMPLE_FREQ": "3S",        
    "PREDICT_HORIZON": 600,       # 30åˆ†é’Ÿ = 600 bars
    "LOOKBACK": 100,              # è¾“å…¥çª—å£
    
    # --- [V4 æ–°å¢] æ ¸å¿ƒäº¤æ˜“å‚æ•° ---
    "TRADE_COST": 0.0001,         # å•è¾¹ä¸‡1
    "MIN_PROFIT_THRESHOLD": 0.0002, # é¢å¤–å‡€åˆ©ç¼“å†² (2bps)ï¼Œç”¨äºè¦†ç›–æ»‘ç‚¹
    
    # --- è®­ç»ƒå‚æ•° ---
    "BATCH_SIZE": 256,
    "EPOCHS": 50,
    "LR": 2e-5,                   # è°ƒä½ LRï¼Œé€‚åº”å¤æ‚ Loss
    "DEVICE": "cuda" if torch.cuda.is_available() else "cpu",
    "PATIENCE": 15,
    "TIMEZONE": "Asia/Shanghai"
}

warnings.filterwarnings("ignore")
DEVICE = CONFIG["DEVICE"]

# ==========================================
# 1. å› å­å·¥å‚ (AlphaFactorCalculator)
# ==========================================
class AlphaFactorCalculator:
    def __init__(self, windows=[20, 100, 300]):
        self.windows = windows 
        
    def _safe_div(self, a, b):
        return a / (b + 1e-9)

    def _rolling_stats(self, series, window, name):
        roll = series.rolling(window=window)
        mean = roll.mean()
        std = roll.std()
        zscore = self._safe_div(series - mean, std)
        slope = self._safe_div(series - series.shift(window), window)
        return {
            f"{name}_{window}w_zscore": zscore,
            f"{name}_{window}w_slope": slope,
        }

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä¸»å…¥å£: è®¡ç®—æ‰€æœ‰å› å­"""
        mid = (df["bp1"] + df["sp1"]) / 2
        
        # [ä¿®æ”¹] å¿…é¡»ä¿ç•™åŸå§‹ä¹°å–ä»·åˆ—ï¼Œç”¨äºåç»­ Taker-Taker æ‰“æ ‡
        factors = pd.DataFrame(index=df.index)
        factors["mid"] = mid
        factors["bp1"] = df["bp1"]
        factors["sp1"] = df["sp1"]
        
        if "bp1_aux" in df.columns:
            df["mid_aux"] = (df["bp1_aux"] + df["sp1_aux"]) / 2
            factors["mid_aux"] = df["mid_aux"]
            factors["bp1_aux"] = df["bp1_aux"]
            factors["sp1_aux"] = df["sp1_aux"]
        
        # --- A. å¾®è§‚ç»“æ„å› å­ ---
        ofi_buy = (df["bv1"] + 0.8*df["bv2"] + 0.6*df["bv3"]) 
        ofi_sell = (df["sv1"] + 0.8*df["sv2"] + 0.6*df["sv3"])
        factors["iOFI"] = self._safe_div(ofi_buy - ofi_sell, ofi_buy + ofi_sell)
        factors["QI"] = self._safe_div(df["bv1"] - df["sv1"], df["bv1"] + df["sv1"])
        factors["spread_bps"] = self._safe_div(df["sp1"] - df["bp1"], mid) * 10000
        
        # --- B. èµ„é‡‘æµå› å­ ---
        factors["sentiment"] = df["sentiment"]
        
        # --- C. æœŸè´§å› å­ ---
        if "fut_imb" in df.columns:
            factors["fut_imb"] = df["fut_imb"]
            factors["FSB"] = (np.log(df["fut_price"]) - np.log(mid)) * np.sign(df["fut_imb"])
            factors["FLP"] = df["fut_pct"] * df["fut_imb"]
        
        # --- D. å¥—åˆ©å› å­ ---
        factors["premium_rate"] = df["premium_rate"]
        
        # --- E. è·¨å“ç§ ---
        if "mid_aux" in df.columns:
            factors["LLT_rs"] = df["mid_aux"].pct_change() - mid.pct_change()
            factors["price_ratio"] = self._safe_div(mid, df["mid_aux"])

        # --- F. è‡ªåŠ¨è¡ç”Ÿ ---
        core_bases = ["sentiment", "iOFI", "QI", "premium_rate"]
        if "fut_imb" in df.columns: core_bases += ["fut_imb", "FSB"]
        if "LLT_rs" in factors.columns: core_bases += ["LLT_rs"]
        
        derived_list = []
        for col in core_bases:
            if col not in factors.columns: continue
            derived_list.append(factors[col].diff().rename(f"{col}_delta"))
            for w in self.windows:
                stats = self._rolling_stats(factors[col], w, col)
                derived_list.append(pd.DataFrame(stats))
                
        # æ³¢åŠ¨ç‡çŠ¶æ€
        ret = mid.pct_change()
        for w in self.windows:
            derived_list.append(ret.rolling(w).std().rename(f"volatility_{w}w"))
            
        all_factors = pd.concat([factors] + derived_list, axis=1)
        return all_factors.fillna(0.0).replace([np.inf, -np.inf], 0.0)

# ==========================================
# 2. æ·±åº¦é»‘ç›’æŒ–æ˜æœº (V4 - VICReg å»å†—ä½™)
# ==========================================
class DeepFactorMiner(nn.Module):
    def __init__(self, input_dim, latent_dim=16):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128), nn.LayerNorm(128), nn.GELU(),
            nn.Linear(128, 64), nn.GELU(),
            nn.Linear(64, latent_dim), nn.BatchNorm1d(latent_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 64), nn.GELU(),
            nn.Linear(64, 128), nn.GELU(),
            nn.Linear(128, input_dim)
        )
        self.predictor = nn.Sequential(
            nn.Linear(latent_dim, 32), nn.GELU(),
            nn.Linear(32, 3)
        )

    def forward(self, x):
        factors = self.encoder(x)
        recon = self.decoder(factors)
        pred = self.predictor(factors)
        return factors, recon, pred

def off_diagonal(x):
    """[V4 æ–°å¢] æå–éå¯¹è§’çº¿å…ƒç´ """
    n, m = x.shape
    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()

def train_miner(X_train, y_train, input_dim, latent_dim=16, epochs=10):
    print(f"\nâ›ï¸ [Miner] å¯åŠ¨æ·±åº¦æŒ–æ˜ (VICReg æ­£äº¤åŒ–æ¨¡å¼)...")
    miner = DeepFactorMiner(input_dim, latent_dim).to(DEVICE)
    optimizer = optim.AdamW(miner.parameters(), lr=1e-3)
    
    xt = torch.FloatTensor(X_train).to(DEVICE)
    yt = torch.LongTensor(y_train).to(DEVICE)
    dl = DataLoader(torch.utils.data.TensorDataset(xt, yt), batch_size=2048, shuffle=True)
    
    loss_ce = nn.CrossEntropyLoss()
    loss_mse = nn.MSELoss()
    
    for epoch in range(epochs):
        miner.train()
        total_loss = 0
        for bx, by in dl:
            optimizer.zero_grad()
            factors, recon, pred = miner(bx)
            
            # 1. é¢„æµ‹ä¸è¿˜åŸ
            l_pred = loss_ce(pred, by)
            l_recon = loss_mse(recon, bx)
            
            # 2. [V4] æ­£äº¤æƒ©ç½š (VICReg Covariance)
            factors_norm = factors - factors.mean(dim=0)
            factors_std = factors.std(dim=0) + 1e-6
            factors_norm = factors_norm / factors_std
            cov_mat = (factors_norm.T @ factors_norm) / (factors.shape[0] - 1)
            l_ortho = off_diagonal(cov_mat).pow(2).sum()
            
            # å¤åˆ Loss
            loss = 0.7 * l_pred + 0.2 * l_recon + 0.1 * l_ortho
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
    return miner

def extract_deep_factors(miner, X_data):
    miner.eval()
    with torch.no_grad():
        xt = torch.FloatTensor(X_data).to(DEVICE)
        factors, _, _ = miner(xt)
    return factors.cpu().numpy()

# ==========================================
# 3. æ•°æ®ç®¡é“ (V4 - Taker-Taker PnL Labeling)
# ==========================================
class AlphaForgeV4:
    def __init__(self, cfg):
        self.cfg = cfg
        self.scaler = RobustScaler() 

    def _load_symbol_files(self, symbol):
        pattern = os.path.join(self.cfg["DATA_DIR"], "**", f"{symbol}*.csv")
        files = sorted(glob.glob(pattern, recursive=True))
        if not files:
             pattern = os.path.join(".", "**", f"{symbol}*.csv")
             files = sorted(glob.glob(pattern, recursive=True))
        return files

    def _read_and_clean(self, fpath):
        try:
            usecols = ["tx_local_time", "bp1", "bv1", "sp1", "sv1", 
                       "bp2", "bv2", "sp2", "sv2", "bp3", "bv3", "sp3", "sv3",
                       "sentiment", "tick_vol", "tick_vwap", "premium_rate", 
                       "fut_price", "fut_imb", "fut_pct"]
            
            preview = pd.read_csv(fpath, nrows=1)
            valid_cols = [c for c in usecols if c in preview.columns]
            df = pd.read_csv(fpath, usecols=valid_cols)
            if "tx_local_time" not in df.columns: return None

            df["datetime"] = pd.to_datetime(df["tx_local_time"], unit="ms", utc=True)\
                               .dt.tz_convert(self.cfg["TIMEZONE"]).dt.tz_localize(None)
            for c in [c for c in valid_cols if c != "tx_local_time"]:
                df[c] = pd.to_numeric(df[c], errors="coerce")

            df = df.sort_values("datetime").drop_duplicates("datetime", keep="last")
            df = df.set_index("datetime")
            return df.resample(self.cfg["RESAMPLE_FREQ"]).last().dropna()
        except: return None

    def load_and_align(self):
        print("ğŸš€ [Forge] åŒæµå¯¹é½åŠ è½½ (ä¸»+è¾…)...")
        main_files = self._load_symbol_files(self.cfg["MAIN_SYMBOL"])
        aux_files = self._load_symbol_files(self.cfg["AUX_SYMBOL"])
        
        def get_date(fname):
            try: return os.path.basename(fname).split("-")[-3] + "-" + os.path.basename(fname).split("-")[-2] + "-" + os.path.basename(fname).split("-")[-1].split(".")[0]
            except: return "unknown"

        aux_map = {get_date(f): f for f in aux_files}
        full_list = []
        
        for mf in main_files:
            df_main = self._read_and_clean(mf)
            if df_main is None or len(df_main) < 100: continue
            
            date_key = get_date(mf)
            if date_key in aux_map:
                df_aux = self._read_and_clean(aux_map[date_key])
                if df_aux is not None:
                    df_aux = df_aux.add_suffix("_aux")
                    df_main = pd.merge_asof(df_main.sort_index(), df_aux.sort_index(), 
                                            left_index=True, right_index=True, 
                                            tolerance=pd.Timedelta("10s"), direction="backward")
            full_list.append(df_main)
            
        return pd.concat(full_list).sort_index() if full_list else None

    def process_pipeline(self):
        df = self.load_and_align()
        if df is None: raise ValueError("æ— æ•°æ®")
        
        print("âš—ï¸ [Forge] è®¡ç®—ç™½ç›’å› å­...")
        calc = AlphaFactorCalculator()
        df_factors = calc.process(df)
        
        # è¿‡æ»¤æ—¶é—´
        t = df_factors.index.time
        mask = ((t >= time(9, 30)) & (t <= time(10, 15))) | \
               ((t >= time(14, 0)) & (t <= time(14, 45)))
        df_factors = df_factors[mask]
        
        # --- [V4 æ ¸å¿ƒä¿®æ”¹] Taker-Taker å‡€æ”¶ç›Šæ‰“æ ‡ ---
        horizon = self.cfg["PREDICT_HORIZON"]
        comm = self.cfg["TRADE_COST"] * 2 # åŒè¾¹è´¹ç‡
        threshold = self.cfg["MIN_PROFIT_THRESHOLD"]
        
        # é€»è¾‘ï¼šæˆ‘ç°åœ¨ä¹°å…¥(å¯¹æ‰‹ä»·=Ask)ï¼Œè¦çœ‹æœªæ¥èƒ½ä¸èƒ½åœ¨(Bid)å–å‡ºèµšé’±
        # åšå¤šå‡€åˆ© = ln(æœªæ¥Bid) - ln(å½“å‰Ask) - è´¹ç”¨
        future_bp1 = df_factors["bp1"].shift(-horizon)
        curr_sp1 = df_factors["sp1"]
        long_pnl = np.log(future_bp1) - np.log(curr_sp1) - comm
        
        # åšç©ºå‡€åˆ© = ln(å½“å‰Bid) - ln(æœªæ¥Ask) - è´¹ç”¨
        future_sp1 = df_factors["sp1"].shift(-horizon)
        curr_bp1 = df_factors["bp1"]
        short_pnl = np.log(curr_bp1) - np.log(future_sp1) - comm
        
        labels = np.zeros(len(df_factors), dtype=int)
        labels[long_pnl > threshold] = 1   # Buy
        labels[short_pnl > threshold] = 2  # Sell
        
        df_factors["label"] = labels
        # è®°å½•æ¯ç¬”å†³ç­–å¦‚æœåšå¯¹çš„ç†è®ºæ”¶ç›Šï¼Œç”¨äºå›æµ‹è¯„ä¼°
        df_factors["executable_pnl"] = np.where(labels==1, long_pnl, np.where(labels==2, short_pnl, 0.0))
        
        # åˆ‡åˆ†
        df_factors = df_factors.dropna()
        n = len(df_factors)
        train_sz, val_sz = int(n * 0.8), int(n * 0.9)
        train, val, test = df_factors.iloc[:train_sz], df_factors.iloc[train_sz:val_sz], df_factors.iloc[val_sz:]
        
        # æ’é™¤éç‰¹å¾åˆ—
        exclude = ["label", "executable_pnl", "mid", "mid_aux", "bp1", "sp1", "bp1_aux", "sp1_aux", "sp1", "sv1"] 
        # æ³¨æ„: æ’é™¤ bp1/sp1 ç­‰åŸå§‹ä»·æ ¼ï¼Œé˜²æ­¢æ¨¡å‹æ‹Ÿåˆç»å¯¹ä»·æ ¼
        feat_cols = [c for c in df_factors.columns if c not in exclude and c not in ["bp1","sp1","bp1_aux","sp1_aux"]]
        
        print(f"ğŸ§  [Forge] åŸºç¡€ç‰¹å¾ç»´åº¦: {len(feat_cols)}")
        
        # æ ‡å‡†åŒ–
        X_train = self.scaler.fit_transform(train[feat_cols])
        X_val = self.scaler.transform(val[feat_cols])
        X_test = self.scaler.transform(test[feat_cols])
        
        # --- [V4] è®­ç»ƒé»‘ç›’æŒ–æ˜æœº ---
        miner = train_miner(X_train, train["label"].values, input_dim=len(feat_cols))
        
        # æå–é»‘ç›’å› å­
        f_tr = extract_deep_factors(miner, X_train)
        f_val = extract_deep_factors(miner, X_val)
        f_te = extract_deep_factors(miner, X_test)
        
        X_train = np.hstack([X_train, f_tr])
        X_val = np.hstack([X_val, f_val])
        X_test = np.hstack([X_test, f_te])
        
        print(f"ğŸ§¬ [Forge] æ··åˆç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
        
        return (X_train, train), (X_val, val), (X_test, test)

# ==========================================
# 4. æ¨¡å‹ä¸è¯„ä¼° (V4 - Cooldown Validation)
# ==========================================
class FeatureTransformer(nn.Module):
    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2, num_classes=3):
        super().__init__()
        self.projector = nn.Sequential(nn.Linear(input_dim, d_model), nn.LayerNorm(d_model), nn.GELU())
        self.pos_embedding = nn.Parameter(torch.randn(1, CONFIG["LOOKBACK"], d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256, batch_first=True, dropout=0.3)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Sequential(nn.Linear(d_model, 64), nn.GELU(), nn.Dropout(0.2), nn.Linear(64, num_classes))
        
    def forward(self, x):
        x = self.projector(x) + self.pos_embedding
        x = self.transformer(x)
        return self.fc(x[:, -1, :])

class TimeSeriesDataset(Dataset):
    def __init__(self, X, df, lookback):
        self.X = torch.FloatTensor(X)
        self.y = torch.LongTensor(df["label"].values)
        self.pnl = torch.FloatTensor(df["executable_pnl"].values)
        self.lookback = lookback
    def __len__(self): return len(self.X) - self.lookback - 1
    def __getitem__(self, idx):
        return self.X[idx:idx+self.lookback], self.y[idx+self.lookback], self.pnl[idx+self.lookback]

def validate_cio_v4(model, loader, threshold=0.6):
    """
    [V4 éªŒè¯] å¢åŠ  Cooldown é€»è¾‘ï¼Œæ¨¡æ‹ŸçœŸå®æŒä»“å ç”¨
    """
    model.eval()
    preds, labels, pnls = [], [], []
    
    with torch.no_grad():
        for x, y, p in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            logits = model(x)
            probs = torch.softmax(logits, dim=1)
            max_p, p_cls = torch.max(probs, dim=1)
            
            # ç½®ä¿¡åº¦è¿‡æ»¤
            final_p = torch.where(max_p > threshold, p_cls, torch.tensor(0).to(DEVICE))
            
            preds.extend(final_p.cpu().numpy())
            labels.extend(y.cpu().numpy())
            pnls.extend(p.cpu().numpy())
            
    # --- Cooldown æ¨¡æ‹Ÿæ‰§è¡Œ ---
    # è§„åˆ™: æ¯æ¬¡å¼€ä»“åï¼Œå‡è®¾å ç”¨èµ„é‡‘ 20 ä¸ª bars (1åˆ†é’Ÿ)ï¼ŒæœŸé—´ä¸é‡å¤å¼€ä»“
    executed_trades = 0
    total_pnl = 0.0
    cooldown = 0
    n_buys, n_sells = 0, 0
    
    for i in range(len(preds)):
        if cooldown > 0:
            cooldown -= 1
            continue
            
        action = preds[i]
        
        if action == 1: # Buy Signal
            executed_trades += 1
            # åªæœ‰å½“ Label ä¹Ÿæ˜¯ 1 æ—¶ï¼Œæˆ‘ä»¬æ‰æ‹¿åˆ°äº†ç†è®ºä¸Šçš„ long_pnl
            # å¦‚æœ Label æ˜¯ 0 æˆ– 2ï¼Œè¯´æ˜åšå¤šæ˜¯é”™çš„/äºçš„ã€‚
            # ä¸ºäº†ä¸¥è°¨ï¼Œæˆ‘ä»¬ç›´æ¥è¯»å– 'executable_pnl' å¹¶ä¸å¤Ÿï¼Œå› ä¸ºé‚£åªæ˜¯"å¦‚æœåšå¯¹"çš„é’±ã€‚
            # ç®€åŒ–å›æµ‹ï¼šæˆ‘ä»¬ç›´æ¥çœ‹ pnls[i]ã€‚
            # ä½†æ³¨æ„ï¼šDataset é‡Œçš„ pnl å­˜çš„æ˜¯ (Label==1?long:short)ã€‚
            # å¦‚æœæ¨¡å‹é¢„æµ‹ 1 ä½† Label æ˜¯ 2ï¼ŒçœŸå®æ”¶ç›Šåº”è¯¥æ˜¯è´Ÿçš„ã€‚
            # è¿™é‡Œåšç®€å•è¿‘ä¼¼ï¼šå¦‚æœ Pred == Labelï¼Œèµš pnlï¼›å¦åˆ™äº Trade Costã€‚
            
            if labels[i] == 1:
                total_pnl += pnls[i] # èµšåˆ°äº†
            else:
                # é¢„æµ‹é”™äº†æ–¹å‘æˆ–åŠ¨èƒ½ä¸è¶³ï¼ŒäºæŸ = æ‰‹ç»­è´¹ + å¯èƒ½çš„ä»·å·®äºæŸ
                # ç®€å•æƒ©ç½šï¼šäºæ‰åŒè¾¹æ‰‹ç»­è´¹
                total_pnl -= CONFIG["TRADE_COST"] * 2
            
            n_buys += 1
            cooldown = 20 # é”å®š 1 åˆ†é’Ÿ
            
        elif action == 2: # Sell Signal
            executed_trades += 1
            if labels[i] == 2:
                total_pnl += pnls[i]
            else:
                total_pnl -= CONFIG["TRADE_COST"] * 2
            n_sells += 1
            cooldown = 20
            
    avg_pnl_bps = (total_pnl / executed_trades * 10000) if executed_trades > 0 else 0
    
    report = classification_report(labels, preds, output_dict=True, zero_division=0)
    b_prec = report['1']['precision'] if '1' in report else 0
    s_prec = report['2']['precision'] if '2' in report else 0
    
    score = (b_prec + s_prec) + (avg_pnl_bps * 0.2)
    if executed_trades == 0: score = 0
    
    msg = (f"Score:{score:.2f} | PnL:{avg_pnl_bps:.1f}bps | Trades:{executed_trades} "
           f"(B:{n_buys} S:{n_sells}) | Prec B:{b_prec:.2f} S:{s_prec:.2f}")
    return score, msg

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    print(f"ğŸš€ å¯åŠ¨ CIO-V4 (Taker-Taker PnLç‰ˆ + VICReg) | è®¾å¤‡: {DEVICE}")
    forge = AlphaForgeV4(CONFIG)
    
    try:
        (X_tr, df_tr), (X_val, df_val), (X_te, df_te) = forge.process_pipeline()
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

    # é‡‡æ ·å™¨
    lookback = CONFIG["LOOKBACK"]
    labels = df_tr["label"].values[lookback:-1]
    counts = np.bincount(labels, minlength=3)
    # é˜²æ­¢é™¤0
    weights = 1. / (counts + 1e-6)
    # æ˜ å°„å›æ¯ä¸ªæ ·æœ¬
    sample_weights = weights[labels]
    
    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))
    
    ds_train = TimeSeriesDataset(X_tr, df_tr, lookback)
    ds_val = TimeSeriesDataset(X_val, df_val, lookback)
    ds_test = TimeSeriesDataset(X_te, df_te, lookback)
    
    dl_train = DataLoader(ds_train, batch_size=CONFIG["BATCH_SIZE"], sampler=sampler)
    dl_val = DataLoader(ds_val, batch_size=CONFIG["BATCH_SIZE"], shuffle=False)
    
    print(f"ğŸ—ï¸ æ„å»º FeatureTransformer (Input Dim: {X_tr.shape[1]})")
    model = FeatureTransformer(input_dim=X_tr.shape[1]).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG["LR"], weight_decay=1e-3)
    criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 2.0]).to(DEVICE))
    
    best_score = -999
    patience = 0
    
    for epoch in range(CONFIG["EPOCHS"]):
        model.train()
        losses = []
        for x, y, _ in dl_train:
            x, y = x.to(DEVICE), y.to(DEVICE)
            optimizer.zero_grad()
            out = model(x)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
            
        score, msg = validate_cio_v4(model, dl_val)
        print(f"Epoch {epoch+1:02d} | Loss {np.mean(losses):.4f} | {msg}")
        
        if score > best_score:
            best_score = score
            patience = 0
            torch.save(model.state_dict(), "best_model_v4.pth")
            print("   >>> âœ… æ–°çºªå½•!")
        else:
            patience += 1
            if patience >= CONFIG["PATIENCE"]:
                print("â¹ï¸ æ—©åœ")
                break

    print("\nğŸ”® æœ€ç»ˆæµ‹è¯• (OOS)")
    if os.path.exists("best_model_v4.pth"):
        model.load_state_dict(torch.load("best_model_v4.pth"))
        dl_test = DataLoader(ds_test, batch_size=CONFIG["BATCH_SIZE"], shuffle=False)
        _, msg = validate_cio_v4(model, dl_test)
        print(f"TEST RESULT: {msg}")

if __name__ == "__main__":
    main()