import os
import re
import glob
import warnings
import math
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

warnings.filterwarnings("ignore")

# ==========================================
# 1) å…¨å±€é…ç½®
# ==========================================
CONFIG = {
    # --- è·¯å¾„é…ç½® ---
    "DATA_DIR": "./data",
    "MAIN_SYMBOL": "sz159920",
    "AUX_SYMBOL": "sh513130",

    # --- é‡‡æ ·/æ ‡ç­¾ ---
    "RESAMPLE_FREQ": "3S",         # 3ç§’é‡é‡‡æ ·
    "PREDICT_HORIZON": 60,         # æœªæ¥çª—å£(æ¡) -> 60*3s=180s
    "LOOKBACK": 60,                # è¾“å…¥å›çœ‹çª—å£(æ¡)

    # --- æ‰“æ ‡ä¸æˆæœ¬ ---
    "TRADE_COST": 0.0001,          # å•è¾¹è´¹ç‡(å¯è°ƒ)
    "COST_THRESHOLD": 0.0004,      # å‡€æ”¶ç›Šé˜ˆå€¼ï¼ˆå·²è€ƒè™‘ç‚¹å·®ä¸æˆæœ¬åä»è¦è¾¾æ ‡ï¼‰

    # --- å¤–éƒ¨æ•°æ®é—¨æ§ï¼ˆæ¯«ç§’ï¼‰ ---
    "IDX_DELAY_CUTOFF_MS": 3000,
    "FUT_DELAY_CUTOFF_MS": 3000,

    # --- å›æµ‹/æ‰§è¡Œ ---
    "INITIAL_CAP": 200000,
    "CONF_OPEN": 0.60,             # è¿Ÿæ»ï¼šå¼€ä»“é˜ˆå€¼
    "CONF_CLOSE": 0.45,            # è¿Ÿæ»ï¼šå¹³ä»“é˜ˆå€¼ï¼ˆSigLostï¼‰
    "MAX_POSITION": 0.90,
    "STOP_LOSS_PCT": 0.008,
    "MIN_HOLD_BARS": 10,           # æœ€çŸ­æŒä»“(æ¡) -> 10*3s=30s
    "EXEC_DELAY_BARS": 1,          # ä¿¡å· -> æˆäº¤ å»¶è¿Ÿ(æ¡)
    "LIQ_PARTICIPATION": 0.10,     # å•æ¬¡åƒæ‰å–ä¸€/ä¹°ä¸€æ·±åº¦çš„æ¯”ä¾‹ä¸Šé™
    "MIN_TRADE_AMT": 1000,
    "LOT_SIZE": 100,

    # --- åˆ‡åˆ†ï¼ˆæŒ‰å¤©ï¼‰ ---
    "VAL_DAYS": 1,
    "TEST_DAYS": 1,

    # --- æ—¶åŒºï¼ˆç”¨äº tx_local_time è½¬æ¢åˆ°äº¤æ˜“æ‰€æœ¬åœ°æ—¶é—´ï¼‰ ---
    "TIMEZONE": "Asia/Shanghai",

    # --- è®­ç»ƒå‚æ•° ---
    "BATCH_SIZE": 512,
    "EPOCHS": 100,
    "LR": 1e-4,
    "WEIGHT_DECAY": 1e-4,
    "DEVICE": "cuda" if torch.cuda.is_available() else "cpu",
    "PATIENCE": 200,
    "WARMUP_EPOCHS": 10,
}

# ==========================================
# 2) æ•°æ®å·¥å‚ï¼šAlphaForge
# ==========================================
class AlphaForge:
    """
    ç›®æ ‡ï¼š
    - ä¿ç•™åŸ HybridDeepLOB æ¨¡å‹æ¶æ„
    - é‡ç‚¹ä¿®å¤ï¼šæ—¶é—´è½´ã€æ•°æ®å¯¹é½ã€é—¨æ§ã€æ ‡ç­¾ä¸å›æµ‹ä¸€è‡´æ€§
    - å¢åŠ é€‚é… 3~5min é¢‘ç‡çš„â€œçŠ¶æ€/ç»“æ„â€å› å­ï¼ˆçº¯åè§†æ»šåŠ¨ï¼Œä¸å¼•å…¥æœªæ¥ï¼‰
    """
    def __init__(self, cfg: Dict):
        self.cfg = cfg
        self.weights = np.array([1.0, 0.8, 0.6, 0.4, 0.2], dtype=np.float32)

    def load_and_split(self):
        print(f"ğŸš€ [AlphaForge] æ‰«æ: {self.cfg['DATA_DIR']}")
        pairs = self._match_files()
        pairs.sort(key=lambda x: x[0])
        if len(pairs) < (1 + self.cfg["VAL_DAYS"] + self.cfg["TEST_DAYS"]):
            raise ValueError("æ•°æ®å¤©æ•°ä¸è¶³ä»¥åˆ‡åˆ† train/val/test")

        n_val = self.cfg["VAL_DAYS"]
        n_test = self.cfg["TEST_DAYS"]
        train_pairs = pairs[: -(n_val + n_test)]
        val_pairs = pairs[-(n_val + n_test): -n_test]
        test_pairs = pairs[-n_test:]

        print(f"è®­ç»ƒé›†: {train_pairs[0][0]} ~ {train_pairs[-1][0]}")
        print(f"éªŒè¯é›†: {val_pairs[0][0]} ~ {val_pairs[-1][0]}")
        print(f"æµ‹è¯•é›†: {test_pairs[0][0]} ~ {test_pairs[-1][0]}")

        train_df = self._process_batch(train_pairs)
        val_df = self._process_batch(val_pairs)
        test_df = self._process_batch(test_pairs)
        return train_df, val_df, test_df

    def _process_batch(self, pairs: List[Tuple[str, str, str]]) -> pd.DataFrame:
        dfs = []
        for date, mf, af in pairs:
            try:
                df = self._load_pair(mf, af, date)
                if df is None or len(df) < 300:
                    continue

                df = self._calc_factors(df)
                df = self._make_labels(df)

                # åªå¯¹â€œç»å¯¹å¿…è¦åˆ—â€åš dropnaï¼Œé¿å…å¤–éƒ¨åˆ—ç¼ºå¤±å¯¼è‡´é€‰æ‹©åå·®
                required = ["mid", "bp1", "sp1", "label"]
                df = df.replace([np.inf, -np.inf], np.nan)
                df = df.dropna(subset=[c for c in required if c in df.columns])

                dfs.append(df)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ {date}: {e}")

        return pd.concat(dfs).sort_index() if dfs else pd.DataFrame()

    def _match_files(self):
        m_pattern = os.path.join(self.cfg["DATA_DIR"], "**", f"{self.cfg['MAIN_SYMBOL']}*.csv")
        a_pattern = os.path.join(self.cfg["DATA_DIR"], "**", f"{self.cfg['AUX_SYMBOL']}*.csv")
        m_files = glob.glob(m_pattern, recursive=True)
        a_files = glob.glob(a_pattern, recursive=True)

        def get_date(p: str) -> str:
            match = re.search(r"(\d{4}-\d{2}-\d{2})", p)
            if match:
                return match.group(1)
            return ""

        m_map = {get_date(p): p for p in m_files}
        a_map = {get_date(p): p for p in a_files}
        common = sorted(list(set(m_map.keys()) & set(a_map.keys())))
        return [(d, m_map[d], a_map[d]) for d in common]

    # -----------------------------
    # è¯»å– & å¯¹é½
    # -----------------------------
    def _load_pair(self, m_path: str, a_path: str, date_str: str) -> Optional[pd.DataFrame]:
        def _read(p: str) -> pd.DataFrame:
            d = pd.read_csv(p)

            # 1) ç”¨ tx_local_time ä½œä¸ºå”¯ä¸€ä¸»æ—¶é—´è½´ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ° tx_server_timeï¼‰
            if "tx_local_time" in d.columns and d["tx_local_time"].notna().any():
                dt_utc = pd.to_datetime(d["tx_local_time"], unit="ms", utc=True, errors="coerce")
                tz = self.cfg.get("TIMEZONE", "Asia/Shanghai")
                d["datetime"] = dt_utc.dt.tz_convert(tz).dt.tz_localize(None)
            else:
                d["datetime"] = pd.to_datetime(date_str + " " + d["tx_server_time"], errors="coerce")

            # 2) å¼ºåˆ¶æ•°å€¼ç±»å‹ï¼ˆç©ºä¸²/è„å­—ç¬¦ -> NaNï¼‰
            numeric_like = set([
                "price","iopv","premium_rate","index_price","fx_rate","sentiment",
                "tick_vol","tick_amt","tick_vwap","interval_s",
                "idx_delay_ms","fut_delay_ms","data_flags",
                "fut_price","fut_mid","fut_imb","fut_delta_vol","fut_pct",
                "fut_local_time","fut_tick_time",
            ])
            # LOB
            for s in ["bp","bv","sp","sv"]:
                for i in range(1,6):
                    numeric_like.add(f"{s}{i}")

            cols = [c for c in d.columns if c in numeric_like]
            if cols:
                d[cols] = d[cols].apply(pd.to_numeric, errors="coerce")

            d = d.sort_values("datetime")
            d = d.drop_duplicates(subset="datetime", keep="last")
            d = d.set_index("datetime").sort_index()
            return d

        df_m, df_a = _read(m_path), _read(a_path)

        # --- ä¸»æ ‡çš„èšåˆï¼šå¿…é¡»åŒ…å« tick_amtï¼Œé‡ç®— bin_vwap ---
        agg = {
            "price": "last",
            "tick_vol": "sum",
            "tick_amt": "sum",
            "bp1": "last", "sp1": "last", "bp2": "last", "sp2": "last",
            "bp3": "last", "sp3": "last", "bp4": "last", "sp4": "last",
            "bp5": "last", "sp5": "last",
            "bv1": "last", "sv1": "last", "bv2": "last", "sv2": "last",
            "bv3": "last", "sv3": "last", "bv4": "last", "sv4": "last",
            "bv5": "last", "sv5": "last",
            # å¤–éƒ¨åˆ—ï¼ˆå­˜åœ¨åˆ™åŠ è¿›å»ï¼‰
            "index_price": "last",
            "premium_rate": "last",
            "sentiment": "last",
            "fx_rate": "last",
            "tick_vwap": "last",
            "interval_s": "last",
            "idx_delay_ms": "max",
            "fut_delay_ms": "max",
            "data_flags": "max",
            "fut_price": "last",
            "fut_mid": "last",
            "fut_imb": "last",
            "fut_delta_vol": "last",
            "fut_pct": "last",
        }
        # åªä¿ç•™ df_m é‡ŒçœŸæ­£å­˜åœ¨çš„åˆ—
        agg = {k:v for k,v in agg.items() if k in df_m.columns}

        df_m = df_m.resample(self.cfg["RESAMPLE_FREQ"]).agg(agg)

        # tick_vol/tick_amt é˜²å¾¡æ€§ä¿®æ­£
        if "tick_vol" in df_m.columns:
            df_m["tick_vol"] = df_m["tick_vol"].clip(lower=0)
        if "tick_amt" in df_m.columns:
            df_m["tick_amt"] = df_m["tick_amt"].clip(lower=0)

        # é‡ç®— bin_vwapï¼ˆæ›´ç¬¦åˆ 3S èšåˆç»Ÿè®¡æ„ä¹‰ï¼‰
        if "tick_vol" in df_m.columns and "tick_amt" in df_m.columns:
            denom = df_m["tick_vol"].replace(0, np.nan)
            df_m["tick_vwap_bin"] = (df_m["tick_amt"] / denom).fillna(df_m.get("price"))
        else:
            df_m["tick_vwap_bin"] = df_m.get("tick_vwap", df_m.get("price"))

        # --- è¾…æ ‡çš„èšåˆï¼šæ›´å®½å®¹ï¼ˆleft + ffillï¼‰ ---
        agg_a = {"price": "last", "tick_vol": "sum"}
        agg_a = {k:v for k,v in agg_a.items() if k in df_a.columns}
        df_a = df_a.resample(self.cfg["RESAMPLE_FREQ"]).agg(agg_a)
        df_a = df_a.rename(columns={"price":"peer_price", "tick_vol":"peer_vol"})

        df = df_m.join(df_a, how="left")
        # åªç”¨è¿‡å»å€¼å¡«å……ï¼Œé¿å…æœªæ¥
        if "peer_price" in df.columns:
            df["peer_price"] = df["peer_price"].ffill()
        if "peer_vol" in df.columns:
            df["peer_vol"] = df["peer_vol"].fillna(0)

        df = df.dropna(subset=["price", "bp1", "sp1"])
        return df

    # -----------------------------
    # å› å­
    # -----------------------------
    def _calc_factors(self, df: pd.DataFrame) -> pd.DataFrame:
        # ---------- åŸºç¡€ ----------
        df["mid"] = (df["bp1"] + df["sp1"]) / 2.0
        df["spread"] = (df["sp1"] - df["bp1"]).clip(lower=0)

        # Meta time: æ˜ å°„åˆ° [0,1]
        # ç®€åŒ–ï¼šæŒ‰ 09:30-15:00(å«åˆä¼‘) çº¿æ€§æ˜ å°„ï¼Œä½œä¸ºâ€œæ—¥å†…ä½ç½®â€ç‰¹å¾
        t = df.index
        seconds = t.hour*3600 + t.minute*60 + t.second
        start = 9*3600 + 30*60
        end = 15*3600
        df["meta_time"] = np.clip((seconds - start) / (end - start), 0, 1)

        # ---------- å¾®è§‚ç»“æ„ï¼šåŸ micro_pressure ----------
        wb = (df[[f"bv{i}" for i in range(1,6)]].values * self.weights).sum(axis=1)
        wa = (df[[f"sv{i}" for i in range(1,6)]].values * self.weights).sum(axis=1)
        denom = (wb + wa)
        df["feat_micro_pressure"] = np.where(denom == 0, 0.0, (wb - wa) / denom)

        # ---------- æ–°å¢ï¼š3~5min é¢‘ç‡æ›´ç¨³çš„å› å­ ----------
        # 1) ç‚¹å·®æˆæœ¬å æ¯”
        df["feat_spread_pct"] = (df["spread"] / df["mid"]).replace([np.inf, -np.inf], np.nan)

        # 2) ä¸€æ¡£æ·±åº¦ä¸æ·±åº¦ä¸å¹³è¡¡
        depth1 = (df["bv1"] + df["sv1"]).replace(0, np.nan)
        df["feat_depth1_log"] = np.log1p((df["bv1"] + df["sv1"]).clip(lower=0))
        df["feat_depth_imb1"] = ((df["bv1"] - df["sv1"]) / depth1).fillna(0.0)

        # 3) TFIï¼šæˆäº¤æµå¤±è¡¡ï¼ˆç”¨ tick_vwap_bin ç›¸å¯¹ mid åˆ¤æ–­ä¸»åŠ¨æ–¹å‘ï¼‰
        tv = df["tick_vwap_bin"].fillna(df["mid"])
        vol = df.get("tick_vol", pd.Series(0, index=df.index)).fillna(0).clip(lower=0)
        df["feat_tfi"] = np.sign(tv - df["mid"]).fillna(0.0) * np.log1p(vol)

        # 4) OFIï¼ˆç®€åŒ– Cont-OFIï¼Œé€‚é… 3s å¿«ç…§ï¼‰
        bp1, bv1 = df["bp1"], df["bv1"]
        sp1, sv1 = df["sp1"], df["sv1"]
        bp1_prev, bv1_prev = bp1.shift(1), bv1.shift(1)
        sp1_prev, sv1_prev = sp1.shift(1), sv1.shift(1)

        ofi_bid = np.where(bp1 > bp1_prev, bv1,
                    np.where(bp1 == bp1_prev, bv1 - bv1_prev, -bv1_prev))
        ofi_ask = np.where(sp1 < sp1_prev, sv1,
                    np.where(sp1 == sp1_prev, sv1 - sv1_prev, -sv1_prev))
        ofi_raw = np.nan_to_num(ofi_bid) - np.nan_to_num(ofi_ask)
        df["feat_ofi1"] = np.sign(ofi_raw) * np.log1p(np.abs(ofi_raw))

        # 5) LOB skewï¼šä»·å·®å½¢æ€ååº¦
        bid_span = (df["bp1"] - df["bp5"]).clip(lower=0)
        ask_span = (df["sp5"] - df["sp1"]).clip(lower=0)
        denom2 = (bid_span + ask_span).replace(0, np.nan)
        df["feat_lob_skew"] = ((bid_span - ask_span) / denom2).fillna(0.0)

        # 6) Book slopeï¼šè¿œç«¯æŒ‚å•â€œé™¡å³­ç¨‹åº¦â€
        df["feat_ask_slope"] = ((df["sp5"] - df["sp1"]) / df["mid"]).replace([np.inf, -np.inf], np.nan)
        df["feat_bid_slope"] = ((df["bp1"] - df["bp5"]) / df["mid"]).replace([np.inf, -np.inf], np.nan)

        # ---------- åŸæ ¸å¿ƒ Alpha å› å­æºåˆ— ----------
        # premium: ä¼˜å…ˆç”¨é‡‡é›† premium_rateï¼Œå¦åˆ™ç”¨ index-mid è¿‘ä¼¼
        if "premium_rate" in df.columns and df["premium_rate"].notna().any():
            df["feat_premium_rate"] = df["premium_rate"]
        else:
            idx = df.get("index_price")
            if idx is not None:
                df["feat_premium_rate"] = (idx - df["mid"]) / df["mid"]
            else:
                df["feat_premium_rate"] = 0.0

        if "sentiment" in df.columns:
            df["feat_sentiment"] = df["sentiment"]
        else:
            df["feat_sentiment"] = 0.0

        if "fut_imb" in df.columns:
            df["feat_fut_imb"] = df["fut_imb"]
        else:
            df["feat_fut_imb"] = 0.0

        # Flow forceï¼ˆç”¨ bin_vwap + volï¼‰
        df["feat_flow_force"] = (tv - df["mid"]) * np.log1p(vol)

        # ---------- 3~5min çŠ¶æ€å› å­ï¼šçº¯åè§† rolling ----------
        # window sizes based on RESAMPLE_FREQ
        # 5min: 300s
        freq = pd.to_timedelta(self.cfg["RESAMPLE_FREQ"])
        w5 = int(pd.Timedelta("5min") / freq)
        w3 = int(pd.Timedelta("3min") / freq)
        w15 = int(pd.Timedelta("15min") / freq)

        mid = df["mid"]
        logret = np.log(mid).diff()

        # 5min æ–¹å‘/åŠ¨é‡
        df["feat_ret_5m"] = mid.pct_change(w5)
        # 5min RV
        df["feat_rv_5m"] = np.sqrt((logret**2).rolling(w5).sum())
        # 5min ä»·æ ¼æ•ˆç‡æ¯”
        hi5 = mid.rolling(w5).max()
        lo5 = mid.rolling(w5).min()
        denom3 = (hi5 - lo5).replace(0, np.nan)
        df["feat_eff_5m"] = (mid - mid.shift(w5)).abs() / denom3

        # 5min å¹³å‡ç‚¹å·®/æ·±åº¦/æµåŠ¨æ€§çŠ¶æ€
        df["feat_spread_mean_5m"] = df["spread"].rolling(w5).mean()
        df["feat_depth1_mean_5m"] = (df["bv1"] + df["sv1"]).rolling(w5).mean()

        # 5min è®¢å•æµèšåˆ
        df["feat_ofi_5m"] = df["feat_ofi1"].rolling(w5).sum()
        df["feat_tfi_5m"] = df["feat_tfi"].rolling(w5).sum()

        # é‡èƒ½ z-scoreï¼ˆç”¨æ›´é•¿çª—å£åšå‡å€¼æ–¹å·®ï¼‰
        bar_vol_5m = vol.rolling(w5).sum()
        mu = bar_vol_5m.rolling(w15).mean()
        sd = bar_vol_5m.rolling(w15).std().replace(0, np.nan)
        df["feat_volz_5m"] = ((bar_vol_5m - mu) / sd).fillna(0.0)

        # ---------- Peer æ®‹å·®åŠ¨é‡ï¼ˆæ»šåŠ¨ betaï¼‰ ----------
        if "peer_price" in df.columns and df["peer_price"].notna().any():
            peer_mid = df["peer_price"]
            r = mid.pct_change()
            rp = peer_mid.pct_change()
            # beta: cov(r, rp)/var(rp)
            cov = (r * rp).rolling(w15).mean() - r.rolling(w15).mean() * rp.rolling(w15).mean()
            var = (rp**2).rolling(w15).mean() - (rp.rolling(w15).mean()**2)
            beta = (cov / var.replace(0, np.nan)).fillna(1.0)
            df["feat_peer_resid"] = (r - beta * rp).fillna(0.0)
        else:
            df["feat_peer_resid"] = 0.0

        # ---------- Oracleï¼ˆå¤–éƒ¨å› å­ï¼‰ + é—¨æ§ ----------
        # æŒ‡æ•°åŠ¨é‡ã€æœŸè´§é¢†å…ˆç­‰ï¼šåªæœ‰åœ¨ delay ä¸åæ—¶æ‰â€œå¼€æ”¾â€
        # å…¼å®¹æ—§æ•°æ®ï¼šæ²¡æœ‰ delay å­—æ®µæ—¶é»˜è®¤å¯ç”¨
        idx_delay = df.get("idx_delay_ms", pd.Series(np.nan, index=df.index))
        fut_delay = df.get("fut_delay_ms", pd.Series(np.nan, index=df.index))
        flags = df.get("data_flags", pd.Series(0, index=df.index)).fillna(0)

        df["feat_idx_staleness"] = np.log1p(idx_delay.fillna(999999))
        df["feat_fut_staleness"] = np.log1p(fut_delay.fillna(999999))

        bad_idx = (idx_delay > self.cfg["IDX_DELAY_CUTOFF_MS"]) | (flags > 0)
        bad_fut = (fut_delay > self.cfg["FUT_DELAY_CUTOFF_MS"]) | (flags > 0)

        if "index_price" in df.columns:
            df["feat_oracle_idx_mom"] = df["index_price"].pct_change(2)
            df["feat_oracle_basis"] = (df["index_price"] - df["mid"]) / df["mid"]
            df.loc[bad_idx.fillna(False), ["feat_oracle_idx_mom", "feat_oracle_basis"]] = np.nan
        else:
            df["feat_oracle_idx_mom"] = 0.0
            df["feat_oracle_basis"] = 0.0

        if "fut_price" in df.columns:
            # æœŸè´§â€œé¢†å…ˆâ€ç®€åŒ–ï¼šæœŸè´§çŸ­åŠ¨é‡ - ç°è´§çŸ­åŠ¨é‡
            fut_mom = df["fut_price"].pct_change(2)
            spot_mom = df["mid"].pct_change(2)
            df["feat_oracle_fut_lead"] = (fut_mom - spot_mom)
            df.loc[bad_fut.fillna(False), ["feat_oracle_fut_lead"]] = np.nan
        else:
            df["feat_oracle_fut_lead"] = 0.0

        # æ¸…ç†æç«¯å€¼
        for c in [c for c in df.columns if c.startswith("feat_")]:
            df[c] = df[c].replace([np.inf, -np.inf], np.nan)

        return df

    # -----------------------------
    # æ ‡ç­¾ï¼šä¸å›æµ‹æˆäº¤å£å¾„ä¸€è‡´ï¼ˆAsk ä¹° / Bid å–ï¼‰
    # -----------------------------
    def _make_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        horizon = int(self.cfg["PREDICT_HORIZON"])
        thr = float(self.cfg["COST_THRESHOLD"])
        cost = float(self.cfg["TRADE_COST"])

        ask = df["sp1"]
        bid = df["bp1"]

        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=horizon)

        # æœªæ¥æœ€é«˜ Bidï¼šæˆ‘ç°åœ¨æŒ‰ Ask ä¹°ï¼Œæœªæ¥èƒ½å¦æŒ‰ Bid å–å‡ºèµšé’±ï¼Ÿ
        future_max_bid = bid.rolling(window=indexer).max()
        ret_buy = (future_max_bid / ask) - 1.0 - 2.0 * cost

        # æœªæ¥æœ€ä½ Bidï¼šå¦‚æœæœªæ¥ä¼šå‡ºç°æ˜æ˜¾å›æ’¤ï¼Œä½œä¸ºâ€œå–å‡º/é¿é™©â€æ ‡ç­¾
        future_min_bid = bid.rolling(window=indexer).min()
        ret_drawdown = (future_min_bid / bid) - 1.0

        label = np.zeros(len(df), dtype=np.int64)
        label[ret_buy > thr] = 1
        label[ret_drawdown < -thr] = 2

        # åŒæ—¶ç»™å›æµ‹/åˆ†æä¿ç•™ä¸€ä¸ªæœªæ¥ç‚¹æ”¶ç›Šï¼ˆä¸ç”¨äºæ‰“æ ‡å†³ç­–ï¼‰
        future_mid = df["mid"].shift(-horizon)
        df["real_future_ret"] = (future_mid / df["mid"]) - 1.0

        df["label"] = label
        return df


# ==========================================
# 3) æ¨¡å‹ï¼šHybridDeepLOBï¼ˆä¿æŒåŸæ¶æ„ï¼‰
# ==========================================
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=8):
        super().__init__()
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
    def forward(self, x):
        b, c, _ = x.shape
        y = self.pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y

class InceptionBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=1, padding=0)
        self.conv3 = nn.Conv1d(in_ch, out_ch, kernel_size=3, padding=1)
        self.conv5 = nn.Conv1d(in_ch, out_ch, kernel_size=5, padding=2)
        self.bn = nn.BatchNorm1d(out_ch * 3)
    def forward(self, x):
        y1 = F.relu(self.conv1(x))
        y3 = F.relu(self.conv3(x))
        y5 = F.relu(self.conv5(x))
        y = torch.cat([y1, y3, y5], dim=1)
        return F.relu(self.bn(y))

class TemporalAttention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.W = nn.Linear(hidden_dim, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)
    def forward(self, x):
        # x: (B, T, H)
        scores = self.v(torch.tanh(self.W(x))).squeeze(-1)
        weights = torch.softmax(scores, dim=1).unsqueeze(-1)
        return (x * weights).sum(dim=1)

class HybridDeepLOB(nn.Module):
    def __init__(self, num_exp_features, num_classes=3):
        super().__init__()
        # CNN branch (DeepLOB-like)
        self.conv1 = nn.Conv1d(20, 32, kernel_size=3, padding=1)
        self.se1 = SEBlock(32)
        self.inception1 = InceptionBlock(32, 16)
        self.inception2 = InceptionBlock(48, 16)

        # Expert branch
        self.expert = nn.Sequential(
            nn.Linear(num_exp_features, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 32),
            nn.ReLU()
        )

        # Fusion + LSTM + Attention
        self.lstm = nn.LSTM(input_size=48 + 32, hidden_size=64, num_layers=1,
                            batch_first=True, bidirectional=True)
        self.attention = TemporalAttention(hidden_dim=128)
        self.fc = nn.Linear(128, num_classes)

    def forward(self, x_lob, x_exp):
        # x_lob: (B, T, 20) -> (B, 20, T)
        x = x_lob.permute(0, 2, 1)
        feat = F.relu(self.conv1(x))
        feat = self.se1(feat)
        feat = self.inception1(feat)
        feat = self.inception2(feat)
        feat = feat.permute(0, 2, 1)  # (B, T, C)

        # expert on each timestep
        B, T, Fexp = x_exp.shape
        exp = self.expert(x_exp.reshape(-1, Fexp)).reshape(B, T, -1)

        combined = torch.cat([feat, exp], dim=2)
        lstm_out, _ = self.lstm(combined)
        context = self.attention(lstm_out)
        return self.fc(context)


# ==========================================
# 4) æ•°æ®é›†ï¼ˆä¿®å¤ï¼šåªåœ¨ train ä¸Š fit scaler + ç»Ÿä¸€å¡«å……ç­–ç•¥ï¼‰
# ==========================================
class ETFDataset(Dataset):
    def __init__(self, df: pd.DataFrame, lookback: int,
                 scaler: Optional[StandardScaler]=None,
                 imputer: Optional[Dict[str, float]]=None):
        self.lookback = int(lookback)

        lob_cols = [f"{s}{i}" for i in range(1,6) for s in ["bp","sp"]] + \
                   [f"{s}{i}" for i in range(1,6) for s in ["bv","sv"]]
        exp_cols = [c for c in df.columns if c.startswith("feat_") or c.startswith("meta_")]

        # --- LOB tensor ---
        mid = df["mid"].values.reshape(-1, 1)
        safe_mid = np.where(mid == 0, 1.0, mid)
        lob_data = df[lob_cols].values.astype(np.float32)

        # price levels -> relative bps
        lob_data[:, :10] = (lob_data[:, :10] - mid) / safe_mid * 10000.0
        # sizes -> log1p
        lob_data[:, 10:] = np.log1p(np.clip(lob_data[:, 10:], a_min=0, a_max=None))

        self.X_lob = np.nan_to_num(lob_data).astype(np.float32)

        # --- Expert features ---
        exp_df = df[exp_cols].copy()

        # ç¼ºå¤±æŒ‡ç¤ºï¼ˆè®©æ¨¡å‹çŸ¥é“â€œä¸å¯ç”¨/è¢«é—¨æ§â€ï¼‰
        miss_flags = exp_df.isna().astype(np.float32)
        miss_flags.columns = [c + "_isna" for c in miss_flags.columns]

        # imputeï¼šåªç”¨ train ç»Ÿè®¡çš„ä¸­ä½æ•°
        if imputer is None:
            self.imputer = {c: float(exp_df[c].median(skipna=True)) if exp_df[c].notna().any() else 0.0 for c in exp_cols}
        else:
            self.imputer = imputer

        for c in exp_cols:
            exp_df[c] = exp_df[c].fillna(self.imputer.get(c, 0.0))

        exp_full = pd.concat([exp_df, miss_flags], axis=1)
        self.exp_feature_names = exp_full.columns.tolist()
        exp_data = exp_full.values.astype(np.float32)

        if scaler is None:
            self.scaler = StandardScaler()
            self.X_exp = self.scaler.fit_transform(exp_data).astype(np.float32)
        else:
            self.scaler = scaler
            self.X_exp = self.scaler.transform(exp_data).astype(np.float32)

        self.Y = df["label"].values.astype(np.int64)
        self.raw_ret = df.get("real_future_ret", pd.Series(np.nan, index=df.index)).values.astype(np.float32)

    def __len__(self):
        # è®©æœ€åä¸€ä¸ªæ ‡ç­¾ä¹Ÿå¯ç”¨
        return max(0, len(self.Y) - self.lookback + 1)

    def __getitem__(self, i):
        s = i
        e = i + self.lookback
        return (
            torch.from_numpy(self.X_lob[s:e]),
            torch.from_numpy(self.X_exp[s:e]),
            torch.tensor(self.Y[e-1], dtype=torch.long),
            torch.tensor(self.raw_ret[e-1], dtype=torch.float32),
        )

# ==========================================
# 5) å›æµ‹å¼•æ“ï¼ˆè¿Ÿæ» + æ·±åº¦çº¦æŸ + æœ€å°æŒä»“ + å»¶è¿Ÿæˆäº¤ï¼‰
# ==========================================
@torch.no_grad()
def backtest_evaluate(model: nn.Module, dataloader: DataLoader, cfg: Dict, raw_df: Optional[pd.DataFrame]=None) -> float:
    model.eval()
    device = cfg["DEVICE"]

    all_probs = []
    for x_lob, x_exp, _, _ in dataloader:
        x_lob = x_lob.to(device)
        x_exp = x_exp.to(device)
        logits = model(x_lob, x_exp)
        probs = torch.softmax(logits, dim=1).cpu().numpy()
        all_probs.append(probs)

    if not all_probs or raw_df is None:
        return 0.0

    probs_stream = np.concatenate(all_probs, axis=0)
    # é¢„æµ‹å¯¹é½ï¼šDataset çš„ç¬¬ä¸€ä¸ªé¢„æµ‹å¯¹åº” raw_df çš„ index = lookback-1
    lookback = cfg["LOOKBACK"]
    sim_df = raw_df.iloc[lookback-1: lookback-1 + len(probs_stream)].copy()
    if len(sim_df) != len(probs_stream):
        # å…œåº•ï¼šå°¾éƒ¨å¯¹é½
        sim_df = raw_df.tail(len(probs_stream)).copy()

    ask = sim_df["sp1"].values
    bid = sim_df["bp1"].values
    ask_v = sim_df.get("sv1", pd.Series(np.inf, index=sim_df.index)).values
    bid_v = sim_df.get("bv1", pd.Series(np.inf, index=sim_df.index)).values
    times = sim_df.index

    initial_cap = cfg["INITIAL_CAP"]
    cash = float(initial_cap)
    shares = 0.0
    is_holding = False
    entry_price = 0.0
    entry_idx = -1

    cost_rate = cfg["TRADE_COST"]
    open_th = cfg["CONF_OPEN"]
    close_th = cfg["CONF_CLOSE"]
    max_pos = cfg["MAX_POSITION"]
    stop_loss = cfg["STOP_LOSS_PCT"]
    min_hold = cfg["MIN_HOLD_BARS"]
    delay = int(cfg["EXEC_DELAY_BARS"])
    lot = int(cfg["LOT_SIZE"])
    min_amt = float(cfg["MIN_TRADE_AMT"])
    part = float(cfg["LIQ_PARTICIPATION"])

    def is_eod(ts):
        return (ts.hour == 14 and ts.minute >= 55) or (ts.hour >= 15)

    # æ—¥å¿—
    log_path = "backtest_log.txt"
    with open(log_path, "w", encoding="utf-8") as f:
        f.write("[Backtest] start\n")

        for t in range(len(probs_stream) - delay):
            ts = times[t]
            p_hold, p_buy, p_sell = probs_stream[t]

            # å»¶è¿Ÿæˆäº¤ç”¨ t+delay çš„ç›˜å£
            ex_ask = ask[t + delay]
            ex_bid = bid[t + delay]
            ex_ask_v = ask_v[t + delay] if np.isfinite(ask_v[t + delay]) else np.inf
            ex_bid_v = bid_v[t + delay] if np.isfinite(bid_v[t + delay]) else np.inf

            # EOD å¼ºåˆ¶æ¸…ä»“
            if is_eod(ts):
                if is_holding:
                    revenue = shares * ex_bid * (1 - cost_rate)
                    cash += revenue
                    pnl = revenue - shares * entry_price * (1 + cost_rate)
                    f.write(f"[{ts}] SELL(EOD) @ {ex_bid:.4f} shares={shares:.0f} pnl={pnl:+.2f}\n")
                    shares = 0.0
                    is_holding = False
                continue

            # å†³ç­–ï¼ˆè¿Ÿæ»ï¼‰ï¼šåªå¯¹ Buy åšå¤šï¼ŒSell ä½œä¸ºé€€å‡º/é£æ§ä¿¡å·
            want_buy = (p_buy > p_hold) and (p_buy > p_sell) and (p_buy >= open_th)
            want_exit = (p_sell > p_buy and p_sell >= open_th) or (p_buy <= close_th)

            if not is_holding:
                if want_buy:
                    # åŠ¨æ€ä»“ä½
                    confidence = float(p_buy)
                    pos = max_pos * (confidence - open_th) / (1.0 - open_th)
                    pos = float(np.clip(pos, 0.0, max_pos))

                    budget = cash * pos
                    if budget < min_amt:
                        continue

                    # æµåŠ¨æ€§çº¦æŸï¼šæœ€å¤šåƒæ‰å–ä¸€æ·±åº¦çš„ä¸€éƒ¨åˆ†
                    max_shares_liq = math.floor((ex_ask_v * part) / lot) * lot if np.isfinite(ex_ask_v) else 10**12
                    # èµ„é‡‘çº¦æŸ
                    max_shares_cash = math.floor((budget / (ex_ask * (1 + cost_rate))) / lot) * lot
                    buy_shares = max(0, min(max_shares_liq, max_shares_cash))

                    if buy_shares >= lot:
                        cost = buy_shares * ex_ask * (1 + cost_rate)
                        cash -= cost
                        shares = float(buy_shares)
                        entry_price = float(ex_ask)
                        entry_idx = t
                        is_holding = True
                        f.write(f"[{ts}] BUY @ {ex_ask:.4f} shares={shares:.0f} p_buy={p_buy:.3f}\n")

            else:
                # æœ€çŸ­æŒä»“
                if (t - entry_idx) < min_hold:
                    continue

                # æ­¢æŸï¼ˆæŒ‰ Bid ä¼°å€¼ï¼‰
                pnl_pct = (ex_bid - entry_price) / entry_price
                if pnl_pct <= -stop_loss:
                    revenue = shares * ex_bid * (1 - cost_rate)
                    cash += revenue
                    pnl = revenue - shares * entry_price * (1 + cost_rate)
                    f.write(f"[{ts}] SELL(StopLoss) @ {ex_bid:.4f} pnl={pnl:+.2f} ({pnl_pct:.2%})\n")
                    shares = 0.0
                    is_holding = False
                    continue

                if want_exit:
                    revenue = shares * ex_bid * (1 - cost_rate)
                    cash += revenue
                    pnl = revenue - shares * entry_price * (1 + cost_rate)
                    f.write(f"[{ts}] SELL(Exit) @ {ex_bid:.4f} pnl={pnl:+.2f} p_buy={p_buy:.3f} p_sell={p_sell:.3f}\n")
                    shares = 0.0
                    is_holding = False
                    continue

        # ç»“ç®—
        nav = cash
        if is_holding:
            nav += shares * bid[-1] * (1 - cost_rate)

        f.write(f"[Backtest] final_nav={nav:.2f} ret={(nav/initial_cap-1):.4%}\n")

    return (nav / initial_cap) - 1.0


# ==========================================
# 6) è®­ç»ƒ
# ==========================================
def train_system(cfg: Dict = CONFIG):
    forge = AlphaForge(cfg)
    train_df, val_df, test_df = forge.load_and_split()

    if train_df.empty or val_df.empty or test_df.empty:
        raise ValueError("train/val/test æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®ç›®å½•ä¸æ–‡ä»¶å‘½å")

    ds_train = ETFDataset(train_df, cfg["LOOKBACK"], scaler=None, imputer=None)
    ds_val = ETFDataset(val_df, cfg["LOOKBACK"], scaler=ds_train.scaler, imputer=ds_train.imputer)
    ds_test = ETFDataset(test_df, cfg["LOOKBACK"], scaler=ds_train.scaler, imputer=ds_train.imputer)

    dl_train = DataLoader(ds_train, batch_size=cfg["BATCH_SIZE"], shuffle=True, drop_last=True)
    dl_val = DataLoader(ds_val, batch_size=cfg["BATCH_SIZE"], shuffle=False)
    dl_test = DataLoader(ds_test, batch_size=cfg["BATCH_SIZE"], shuffle=False)

    device = cfg["DEVICE"]
    model = HybridDeepLOB(num_exp_features=ds_train.X_exp.shape[1], num_classes=3).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=cfg["LR"], weight_decay=cfg["WEIGHT_DECAY"])
    criterion = nn.CrossEntropyLoss()

    best_val = -1e9
    patience = 0

    for epoch in range(cfg["EPOCHS"]):
        model.train()
        losses = []
        for x_lob, x_exp, y, _ in dl_train:
            x_lob = x_lob.to(device)
            x_exp = x_exp.to(device)
            y = y.to(device)

            optimizer.zero_grad()
            logits = model(x_lob, x_exp)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

        # --- éªŒè¯ï¼šç”¨æ›´ä¸¥æ ¼çš„å›æµ‹åšé€‰æ¨¡ ---
        val_ret = backtest_evaluate(model, dl_val, cfg, raw_df=val_df)
        print(f"Epoch {epoch+1:03d} | loss={np.mean(losses):.4f} | val_ret={val_ret:.4%}")

        if epoch < cfg["WARMUP_EPOCHS"]:
            continue

        if val_ret > best_val:
            best_val = val_ret
            patience = 0
            torch.save(model.state_dict(), "alpha_model_hybriddeeplob.pth")
            print(f"âœ… ä¿å­˜æœ€å¥½æ¨¡å‹: val_ret={best_val:.4%}")
        else:
            patience += 1
            if patience >= cfg["PATIENCE"]:
                print("â¹ï¸ æ—©åœè§¦å‘")
                break

    # --- æµ‹è¯•è¯„ä¼° ---
    model.load_state_dict(torch.load("alpha_model_hybriddeeplob.pth", map_location=device))
    test_ret = backtest_evaluate(model, dl_test, cfg, raw_df=test_df)
    print(f"\n[Test] ret={test_ret:.4%}")

    # é¢å¤–ï¼šåˆ†ç±»æŠ¥å‘Šï¼ˆä»…ä¾›è¯Šæ–­ï¼Œä¸åšé€‰æ¨¡ä¾æ®ï¼‰
    model.eval()
    ys, yp = [], []
    with torch.no_grad():
        for x_lob, x_exp, y, _ in dl_test:
            x_lob = x_lob.to(device); x_exp = x_exp.to(device)
            logits = model(x_lob, x_exp)
            pred = torch.argmax(logits, dim=1).cpu().numpy()
            ys.append(y.numpy()); yp.append(pred)
    ys = np.concatenate(ys); yp = np.concatenate(yp)
    print(classification_report(ys, yp, digits=4))

    return model


if __name__ == "__main__":
    train_system(CONFIG)
