"""
HSI HFT V3 - å¯å¾®åˆ†ç‰¹å¾å·¥ç¨‹
æŠ€æœ¯å®æ–½æ–¹æ¡ˆ

ä¼˜å…ˆçº§ï¼šğŸŸ  é«˜ï¼ˆTier 1ï¼‰
çŠ¶æ€ï¼šå¾…å®æ–½
æ¥æºï¼šç¬¬äºŒä»½è¯„ä¼°çš„æ–°äº®ç‚¹

æ ¸å¿ƒæ€æƒ³ï¼š
è®©whitebox.pyä¸­çš„ç¡¬ç¼–ç æƒé‡å˜æˆnn.Parameterï¼Œ
é€šè¿‡åå‘ä¼ æ’­è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜æƒé‡ï¼ŒåŒæ—¶ä¿ç•™å¯è§£é‡Šæ€§ã€‚

ç®€å•æœ‰æ•ˆï¼šæ”¹åŠ¨é‡å°ï¼Œæ”¶ç›Šå¤§
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List
from collections import deque


class LearnableDepthWeights(nn.Module):
    """
    å¯å­¦ä¹ çš„æ¡£ä½æƒé‡

    åŸä»£ç ï¼ˆwhitebox.pyï¼‰ï¼š
    weights = [1.0, 0.8, 0.6, 0.4, 0.2]  # ç¡¬ç¼–ç 

    æ”¹è¿›ï¼š
    self.weights = nn.Parameter(torch.tensor([1.0, 0.8, 0.6, 0.4, 0.2]))
    """

    def __init__(self, num_levels=5):
        super().__init__()

        # åˆå§‹åŒ–ä¸ºç»éªŒæƒé‡
        init_weights = torch.linspace(1.0, 0.2, num_levels)
        self.weights = nn.Parameter(init_weights)

    def forward(self):
        """è¿”å›å½’ä¸€åŒ–åçš„æƒé‡"""
        # Softmaxå½’ä¸€åŒ–ï¼ˆç¡®ä¿å’Œä¸º1ï¼‰
        return torch.softmax(self.weights, dim=0)

    def get_raw_weights(self):
        """è¿”å›åŸå§‹æƒé‡ï¼ˆç”¨äºåˆ†æï¼‰"""
        return self.weights.detach().cpu().numpy()


class LearnableDecayRate(nn.Module):
    """
    å¯å­¦ä¹ çš„æ—¶é—´è¡°å‡ç‡

    åŸä»£ç ï¼š
    decay_rate = 0.5  # å›ºå®š

    æ”¹è¿›ï¼š
    è®©æ¨¡å‹å­¦ä¹ æœ€ä¼˜çš„è¡°å‡é€Ÿåº¦
    """

    def __init__(self, init_rate=0.5):
        super().__init__()

        # ç”¨sigmoidç¡®ä¿åœ¨(0, 1)
        self.logit_rate = nn.Parameter(
            torch.tensor(np.log(init_rate / (1 - init_rate)))
        )

    def forward(self):
        """è¿”å›(0, 1)èŒƒå›´çš„è¡°å‡ç‡"""
        return torch.sigmoid(self.logit_rate)


class LearnableWindowScales(nn.Module):
    """
    å¯å­¦ä¹ çš„æ»šåŠ¨çª—å£å¤§å°

    åŸä»£ç ï¼ˆwhitebox.pyï¼‰ï¼š
    W_set = [20, 100, 600]  # å›ºå®š

    æ”¹è¿›ï¼š
    è®©æ¨¡å‹å¾®è°ƒçª—å£å¤§å°ï¼ˆåœ¨åˆç†èŒƒå›´å†…ï¼‰
    """

    def __init__(self, base_windows=[20, 100, 600]):
        super().__init__()

        # å­¦ä¹ åç§»é‡ï¼ˆÂ±20%ï¼‰
        self.base_windows = torch.tensor(base_windows, dtype=torch.float32)
        self.scale_factors = nn.Parameter(torch.ones(len(base_windows)))

    def forward(self):
        """è¿”å›è°ƒæ•´åçš„çª—å£å¤§å°"""
        # é™åˆ¶scaleåœ¨[0.8, 1.2]
        scales = torch.clamp(self.scale_factors, 0.8, 1.2)
        windows = self.base_windows * scales
        return windows.int()


class LearnableWhiteBoxFactory(nn.Module):
    """
    å¯å¾®åˆ†çš„ç™½ç›’ç‰¹å¾å·¥å‚ï¼ˆéƒ¨åˆ†å¯å­¦ä¹ ï¼‰

    ç­–ç•¥ï¼š
    é˜¶æ®µ1ï¼šåªè®©å…³é”®æƒé‡å¯å­¦ä¹ ï¼ˆæ¨èï¼‰
    - æ¡£ä½æƒé‡
    - æ—¶é—´è¡°å‡ç‡
    - ï¼ˆå¯é€‰ï¼‰çª—å£å¤§å°å¾®è°ƒ

    é˜¶æ®µ2ï¼šå…¨å¯å¾®ï¼ˆéœ€è°¨æ…ï¼‰
    - æ‰€æœ‰å‚æ•°å¯å­¦ä¹ 
    - é£é™©ï¼šå¯èƒ½ç ´åå¯è§£é‡Šæ€§
    """

    def __init__(self, full_differentiable=False):
        super().__init__()

        self.full_differentiable = full_differentiable

        # ========================================
        # å¯å­¦ä¹ çš„å‚æ•°
        # ========================================

        # 1. æ¡£ä½æƒé‡
        self.depth_weights = LearnableDepthWeights(num_levels=5)

        # 2. æ—¶é—´è¡°å‡ç‡
        self.decay_rate = LearnableDecayRate(init_rate=0.5)

        # 3. çª—å£å¤§å°ï¼ˆå¯é€‰ï¼‰
        if full_differentiable:
            self.window_scales = LearnableWindowScales([20, 100, 600])
        else:
            # å›ºå®šçª—å£
            self.window_set = [20, 100, 600]

    def compute_ofi(self, bids, asks, use_learnable=True):
        """
        è®¡ç®—è®¢å•æµä¸å¹³è¡¡ï¼ˆOFIï¼‰

        Args:
            bids: List[(price, volume)] ä¹°ç›˜
            asks: List[(price, volume)] å–ç›˜
            use_learnable: æ˜¯å¦ä½¿ç”¨å¯å­¦ä¹ æƒé‡

        Returns:
            ofi: åŠ æƒè®¢å•æµä¸å¹³è¡¡
        """
        if use_learnable:
            weights = self.depth_weights()
        else:
            weights = torch.tensor([1.0, 0.8, 0.6, 0.4, 0.2])

        # è®¡ç®—OFI
        bid_weighted = sum(w * vol for w, (_, vol) in zip(weights, bids[:5]))
        ask_weighted = sum(w * vol for w, (_, vol) in zip(weights, asks[:5]))

        ofi = (bid_weighted - ask_weighted) / (bid_weighted + ask_weighted + 1e-9)
        return ofi

    def get_learnable_params_summary(self) -> Dict:
        """è¿”å›å¯å­¦ä¹ å‚æ•°çš„å½“å‰å€¼"""
        summary = {
            "depth_weights": self.depth_weights.get_raw_weights(),
            "decay_rate": self.decay_rate().item(),
        }

        if self.full_differentiable:
            summary["window_scales"] = self.window_scales().cpu().numpy()

        return summary


# ========================================
# è®­ç»ƒæ—¶çš„æ­£åˆ™åŒ–
# ========================================


class LearnableParamsRegularization:
    """
    é˜²æ­¢å¯å­¦ä¹ å‚æ•°divergeçš„æ­£åˆ™åŒ–

    ç­–ç•¥ï¼š
    1. L2æ­£åˆ™ï¼ˆæƒé‡ä¸è¦åç¦»åˆå§‹å€¼å¤ªè¿œï¼‰
    2. å•è°ƒæ€§çº¦æŸï¼ˆæ¡£ä½æƒé‡åº”è¯¥é€’å‡ï¼‰
    3. èŒƒå›´çº¦æŸï¼ˆå·²é€šè¿‡nn.Parameterçš„é™åˆ¶å®ç°ï¼‰
    """

    @staticmethod
    def l2_regularization(model: LearnableWhiteBoxFactory, weight=0.01):
        """L2æ­£åˆ™ï¼šæƒ©ç½šæƒé‡åç¦»åˆå§‹å€¼"""
        reg_loss = 0.0

        # æ¡£ä½æƒé‡æ­£åˆ™
        init_depth_weights = torch.linspace(1.0, 0.2, 5)
        reg_loss += weight * torch.norm(
            model.depth_weights.weights - init_depth_weights
        )

        # è¡°å‡ç‡æ­£åˆ™
        init_decay = 0.5
        reg_loss += weight * (model.decay_rate() - init_decay) ** 2

        return reg_loss

    @staticmethod
    def monotonicity_constraint(model: LearnableWhiteBoxFactory, weight=0.1):
        """
        å•è°ƒæ€§çº¦æŸï¼šæ¡£ä½æƒé‡åº”è¯¥é€’å‡

        æƒ©ç½šè¿åw[i] > w[i+1]çš„æƒ…å†µ
        """
        weights = model.depth_weights.weights

        # è®¡ç®—ç›¸é‚»æƒé‡å·®
        diffs = weights[:-1] - weights[1:]

        # æƒ©ç½šè´Ÿå·®å€¼ï¼ˆè¿åé€’å‡ï¼‰
        violations = torch.relu(-diffs)  # è´Ÿå·®å€¼å˜æ­£ï¼Œæ­£å·®å€¼ä¸º0

        return weight * violations.sum()


# ========================================
# é›†æˆåˆ°è®­ç»ƒæµç¨‹
# ========================================


def train_with_learnable_features(model, dataloader, optimizer, epochs=10):
    """
    è®­ç»ƒæ—¶åŒæ—¶ä¼˜åŒ–æ¨¡å‹å’Œç™½ç›’å‚æ•°

    Args:
        model: åŒ…å«LearnableWhiteBoxFactoryçš„æ¨¡å‹
        dataloader: è®­ç»ƒæ•°æ®
        optimizer: ä¼˜åŒ–å™¨ï¼ˆåŒ…å«ç™½ç›’å‚æ•°ï¼‰
        epochs: è®­ç»ƒè½®æ•°
    """

    for epoch in range(epochs):
        total_loss = 0

        for batch in dataloader:
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            output = model(batch)

            # ä»»åŠ¡æŸå¤±ï¼ˆå¦‚MSEï¼‰
            task_loss = compute_task_loss(output, batch["target"])

            # æ­£åˆ™åŒ–æŸå¤±
            reg_loss = LearnableParamsRegularization.l2_regularization(
                model.white_box_factory, weight=0.01
            )
            reg_loss += LearnableParamsRegularization.monotonicity_constraint(
                model.white_box_factory, weight=0.1
            )

            # æ€»æŸå¤±
            loss = task_loss + reg_loss

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # æ¯ä¸ªepochåæ‰“å°å‚æ•°
        if epoch % 2 == 0:
            print(f"\nEpoch {epoch}")
            print(f"Loss: {total_loss:.4f}")
            print("Learnable Params:")
            summary = model.white_box_factory.get_learnable_params_summary()
            for key, val in summary.items():
                print(f"  {key}: {val}")


# ========================================
# å¯è§£é‡Šæ€§åˆ†æ
# ========================================


def analyze_learned_weights(model: LearnableWhiteBoxFactory):
    """
    åˆ†æå­¦ä¹ åˆ°çš„æƒé‡æ˜¯å¦åˆç†

    æ£€æŸ¥ï¼š
    1. æ¡£ä½æƒé‡æ˜¯å¦å•è°ƒé€’å‡
    2. è¡°å‡ç‡æ˜¯å¦åœ¨åˆç†èŒƒå›´
    3. çª—å£å¤§å°æ˜¯å¦åˆç†
    """
    summary = model.get_learnable_params_summary()

    print("=== Learned Parameters Analysis ===")

    # 1. æ¡£ä½æƒé‡
    depth_weights = summary["depth_weights"]
    print(f"\nDepth Weights: {depth_weights}")

    is_monotonic = all(
        depth_weights[i] >= depth_weights[i + 1] for i in range(len(depth_weights) - 1)
    )
    print(f"Monotonic: {is_monotonic} {'âœ“' if is_monotonic else 'âœ—'}")

    # 2. è¡°å‡ç‡
    decay = summary["decay_rate"]
    print(f"\nDecay Rate: {decay:.3f}")
    print(
        f"Reasonable (0.3-0.7): {0.3 <= decay <= 0.7} {'âœ“' if 0.3 <= decay <= 0.7 else 'âœ—'}"
    )

    # 3. çª—å£å¤§å°ï¼ˆå¦‚æœæœ‰ï¼‰
    if "window_scales" in summary:
        windows = summary["window_scales"]
        print(f"\nWindow Scales: {windows}")


# ========================================
# ä½¿ç”¨ç¤ºä¾‹
# ========================================

if __name__ == "__main__":
    # 1. åˆ›å»ºå¯å¾®åˆ†ç™½ç›’å·¥å‚
    white_box = LearnableWhiteBoxFactory(full_differentiable=False)

    # 2. æŸ¥çœ‹åˆå§‹å‚æ•°
    print("=== Initial Parameters ===")
    print(white_box.get_learnable_params_summary())

    # 3. æ¨¡æ‹Ÿè®­ç»ƒï¼ˆå‚æ•°ä¼šæ›´æ–°ï¼‰
    optimizer = torch.optim.Adam(white_box.parameters(), lr=0.01)

    for step in range(10):
        # æ¨¡æ‹ŸæŸå¤±
        loss = torch.randn(1, requires_grad=True).sum()

        # åŠ æ­£åˆ™
        loss = loss + LearnableParamsRegularization.l2_regularization(white_box)
        loss = loss + LearnableParamsRegularization.monotonicity_constraint(white_box)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # 4. æŸ¥çœ‹å­¦ä¹ åçš„å‚æ•°
    print("\n=== After Training ===")
    analyze_learned_weights(white_box)

    # 5. ä½¿ç”¨å­¦ä¹ åˆ°çš„æƒé‡è®¡ç®—OFI
    bids = [(4.50, 10000), (4.49, 8000), (4.48, 6000), (4.47, 5000), (4.46, 4000)]
    asks = [(4.51, 9000), (4.52, 7000), (4.53, 5500), (4.54, 4500), (4.55, 3500)]

    ofi = white_box.compute_ofi(bids, asks, use_learnable=True)
    print(f"\nComputed OFI: {ofi:.4f}")
