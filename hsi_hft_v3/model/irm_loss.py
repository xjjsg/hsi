"""
HSI HFT V3 - IRM (ä¸å˜é£é™©æœ€å°åŒ–) æŸå¤±å‡½æ•°
æŠ€æœ¯å®æ–½æ–¹æ¡ˆ

ä¼˜å…ˆçº§ï¼šğŸŸ¡ ä¸­ï¼ˆTier 2ï¼Œéœ€é‡è®­ç»ƒï¼‰çŠ¶æ€ï¼šå¾…å®æ–½
æ¥æºï¼šç¬¬äºŒä»½è¯„ä¼°ï¼Œä¼˜äºç¬¬ä¸€ä»½çš„T-VICReg

æ ¸å¿ƒæ€æƒ³ï¼š
å¯»æ‰¾åœ¨æ‰€æœ‰å¸‚åœºç¯å¢ƒï¼ˆä½/ä¸­/é«˜æ³¢åŠ¨ï¼‰ä¸‹æ¢¯åº¦æ–¹å‘ä¸€è‡´çš„å‚æ•°ï¼Œ
è‡ªåŠ¨å‰”é™¤åªåœ¨ç‰¹å®šä½“åˆ¶æœ‰æ•ˆçš„"ä¼ªå› å­"ã€‚

ç†è®ºåŸºç¡€ï¼š
IRM (Invariant Risk Minimization) - å› æœMLçš„SOTAæ–¹æ³•
ç›®æ ‡ï¼šmin âˆ‘Error_e + Î»Â·||âˆ‡Error_e||Â²
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple
from collections import defaultdict


class IRMLoss(nn.Module):
    """
    ä¸å˜é£é™©æœ€å°åŒ–æŸå¤±

    æ ¸å¿ƒæœºåˆ¶ï¼š
    1. å°†æ•°æ®æŒ‰ç¯å¢ƒåˆ’åˆ†ï¼ˆä½/ä¸­/é«˜æ³¢åŠ¨ï¼‰
    2. æ¯ä¸ªç¯å¢ƒç‹¬ç«‹è®¡ç®—æŸå¤±å’Œæ¢¯åº¦
    3. æƒ©ç½šè·¨ç¯å¢ƒçš„æ¢¯åº¦ä¸ä¸€è‡´æ€§
    4. è¿«ä½¿æ¨¡å‹å­¦ä¹ ç¯å¢ƒä¸å˜çš„å› æœç‰¹å¾
    """

    def __init__(
        self, penalty_weight=1.0, penalty_anneal_epochs=10, compute_grads_every_k=1
    ):
        super().__init__()

        self.penalty_weight = penalty_weight
        self.penalty_anneal_epochs = penalty_anneal_epochs
        self.compute_grads_every_k = compute_grads_every_k

        self.epoch = 0
        self.grad_computation_count = 0

    def forward(
        self,
        model: nn.Module,
        data_by_env: Dict[str, Tuple[torch.Tensor, torch.Tensor]],
        base_loss_fn: nn.Module = nn.MSELoss(),
    ) -> Dict:
        """
        è®¡ç®—IRMæŸå¤±

        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            data_by_env: {
                'low_vol': (X_low, y_low),
                'mid_vol': (X_mid, y_mid),
                'high_vol': (X_high, y_high)
            }
            base_loss_fn: åŸºç¡€æŸå¤±å‡½æ•°ï¼ˆMSE/BCEç­‰ï¼‰

        Returns:
            {
                'total_loss': æ€»æŸå¤±,
                'env_losses': å„ç¯å¢ƒæŸå¤±,
                'grad_penalty': æ¢¯åº¦æƒ©ç½š,
                'current_penalty_weight': å½“å‰æƒ©ç½šæƒé‡
            }
        """
        env_losses = []
        env_names = []

        # ========================================
        # 1. è®¡ç®—æ¯ä¸ªç¯å¢ƒçš„æŸå¤±
        # ========================================
        for env_name, (X, y) in data_by_env.items():
            pred = model(X)
            loss_e = base_loss_fn(pred, y)

            env_losses.append(loss_e)
            env_names.append(env_name)

        # ç¯å¢ƒå¹³å‡æŸå¤±
        mean_env_loss = sum(env_losses) / len(env_losses)

        # ========================================
        # 2. è®¡ç®—æ¢¯åº¦æƒ©ç½šï¼ˆè®¡ç®—å¯†é›†ï¼Œä¸æ˜¯æ¯æ­¥éƒ½ç®—ï¼‰
        # ========================================

        if self.grad_computation_count % self.compute_grads_every_k == 0:
            grad_penalty = self._compute_grad_penalty(model, env_losses)
        else:
            # å¤ç”¨ä¸Šæ¬¡çš„æ¢¯åº¦æƒ©ç½šï¼ˆè¿‘ä¼¼ï¼‰
            grad_penalty = torch.tensor(0.0, device=env_losses[0].device)

        self.grad_computation_count += 1

        # ========================================
        # 3. Annealingï¼šé€æ¸å¢åŠ æƒ©ç½šæƒé‡
        # ========================================
        # å‰å‡ ä¸ªepochè®©æ¨¡å‹å…ˆå­¦ä¹ åŸºç¡€æ¨¡å¼ï¼Œå†å¼ºåˆ¶ä¸å˜æ€§

        if self.epoch < self.penalty_anneal_epochs:
            current_penalty_weight = self.penalty_weight * (
                self.epoch / self.penalty_anneal_epochs
            )
        else:
            current_penalty_weight = self.penalty_weight

        # ========================================
        # 4. æ€»æŸå¤±
        # ========================================
        total_loss = mean_env_loss + current_penalty_weight * grad_penalty

        return {
            "total_loss": total_loss,
            "mean_env_loss": mean_env_loss,
            "env_losses": {
                name: loss.item() for name, loss in zip(env_names, env_losses)
            },
            "grad_penalty": (
                grad_penalty.item()
                if isinstance(grad_penalty, torch.Tensor)
                else grad_penalty
            ),
            "current_penalty_weight": current_penalty_weight,
        }

    def _compute_grad_penalty(
        self, model: nn.Module, env_losses: List[torch.Tensor]
    ) -> torch.Tensor:
        """
        è®¡ç®—è·¨ç¯å¢ƒçš„æ¢¯åº¦æƒ©ç½š

        æ ¸å¿ƒæ€æƒ³ï¼š
        å¦‚æœæŸä¸ªå‚æ•°åœ¨ç¯å¢ƒAä¸­æ¢¯åº¦ä¸ºæ­£ï¼Œåœ¨ç¯å¢ƒBä¸­æ¢¯åº¦ä¸ºè´Ÿï¼Œ
        è¯´æ˜è¯¥å‚æ•°ä¾èµ–ç¯å¢ƒç‰¹å®šçš„æ¨¡å¼ï¼ˆä¼ªå› å­ï¼‰ï¼Œåº”è¯¥æƒ©ç½šã€‚
        """
        env_grads = []

        # è·å–æ¨¡å‹å‚æ•°
        params = [p for p in model.parameters() if p.requires_grad]

        # è®¡ç®—æ¯ä¸ªç¯å¢ƒçš„æ¢¯åº¦
        for loss_e in env_losses:
            grads_e = torch.autograd.grad(
                loss_e, params, create_graph=True, retain_graph=True  # å…è®¸äºŒé˜¶å¯¼æ•°
            )
            env_grads.append(grads_e)

        # è®¡ç®—æ¢¯åº¦ä¸ä¸€è‡´æ€§æƒ©ç½š
        penalty = 0.0
        num_envs = len(env_grads)

        for i in range(num_envs):
            for j in range(i + 1, num_envs):
                # å¯¹æ¯ä¸ªå‚æ•°ï¼Œè®¡ç®—ç¯å¢ƒiå’Œç¯å¢ƒjçš„æ¢¯åº¦å·®
                for grad_i, grad_j in zip(env_grads[i], env_grads[j]):
                    penalty += torch.norm(grad_i - grad_j)

        # å½’ä¸€åŒ–
        num_pairs = num_envs * (num_envs - 1) / 2
        penalty = penalty / num_pairs

        return penalty

    def step_epoch(self):
        """æ¯ä¸ªepochç»“æŸæ—¶è°ƒç”¨"""
        self.epoch += 1


# ========================================
# ç¯å¢ƒåˆ’åˆ†ç­–ç•¥
# ========================================


class EnvironmentSplitter:
    """
    å°†æ•°æ®æŒ‰å¸‚åœºç¯å¢ƒåˆ’åˆ†

    ç­–ç•¥ï¼š
    1. æŒ‰æ³¢åŠ¨ç‡åˆ’åˆ†ï¼ˆæ¨èï¼‰
    2. æŒ‰æˆäº¤é‡åˆ’åˆ†
    3. æŒ‰æ—¶é—´æ®µåˆ’åˆ†
    4. è‡ªå®šä¹‰è§„åˆ™
    """

    @staticmethod
    def split_by_volatility(
        data: pd.DataFrame, returns_col="returns", thresholds=[0.01, 0.03], window=20
    ) -> Dict[str, pd.DataFrame]:
        """
        æŒ‰æ³¢åŠ¨ç‡åˆ’åˆ†ç¯å¢ƒ

        Args:
            data: DataFrameåŒ…å«returnsåˆ—
            returns_col: æ”¶ç›Šç‡åˆ—å
            thresholds: [ä½æ³¢ç•Œé™, é«˜æ³¢ç•Œé™]
            window: æ»šåŠ¨çª—å£

        Returns:
            {
                'low_vol': ä½æ³¢åŠ¨æ•°æ®,
                'mid_vol': ä¸­ç­‰æ³¢åŠ¨æ•°æ®,
                'high_vol': é«˜æ³¢åŠ¨æ•°æ®
            }
        """
        # è®¡ç®—æ»šåŠ¨æ³¢åŠ¨ç‡
        vol = data[returns_col].rolling(window).std()
        data = data.copy()
        data["volatility"] = vol

        # åˆ’åˆ†
        low_vol = data[vol < thresholds[0]].copy()
        mid_vol = data[(vol >= thresholds[0]) & (vol < thresholds[1])].copy()
        high_vol = data[vol >= thresholds[1]].copy()

        return {"low_vol": low_vol, "mid_vol": mid_vol, "high_vol": high_vol}

    @staticmethod
    def split_by_time_period(
        data: pd.DataFrame, time_col="timestamp"
    ) -> Dict[str, pd.DataFrame]:
        """
        æŒ‰æ—¶é—´æ®µåˆ’åˆ†ç¯å¢ƒ

        é€‚ç”¨åœºæ™¯ï¼šé¿å…æ—¶åºæ³„éœ²ï¼Œç¡®ä¿ç¯å¢ƒç‹¬ç«‹

        Args:
            data: DataFrameåŒ…å«æ—¶é—´åˆ—
            time_col: æ—¶é—´æˆ³åˆ—å

        Returns:
            {
                'env_1': ç¬¬ä¸€æ—¶é—´æ®µ,
                'env_2': ç¬¬äºŒæ—¶é—´æ®µ,
                'env_3': ç¬¬ä¸‰æ—¶é—´æ®µ
            }
        """
        data = data.sort_values(time_col)
        n = len(data)

        env_1 = data.iloc[: n // 3]
        env_2 = data.iloc[n // 3 : 2 * n // 3]
        env_3 = data.iloc[2 * n // 3 :]

        return {"env_1": env_1, "env_2": env_2, "env_3": env_3}


# ========================================
# è®­ç»ƒæµç¨‹é›†æˆ
# ========================================


def train_with_irm(model, train_data, val_data, epochs=50, device="cpu"):
    """
    ä½¿ç”¨IRMæŸå¤±è®­ç»ƒæ¨¡å‹

    Steps:
    1. åˆ’åˆ†ç¯å¢ƒ
    2. ä¸ºæ¯ä¸ªç¯å¢ƒåˆ›å»ºDataLoaderï¼ˆæˆ–ä½¿ç”¨æ‰¹æ¬¡é‡‡æ ·ï¼‰
    3. IRMæŸå¤±è®¡ç®—
    4. åå‘ä¼ æ’­
    """

    # 1. åˆ’åˆ†è®­ç»ƒæ•°æ®ä¸ºç¯å¢ƒ
    train_envs = EnvironmentSplitter.split_by_volatility(train_data)

    print("Environment Statistics:")
    for env_name, env_data in train_envs.items():
        print(f"  {env_name}: {len(env_data)} samples")

    # 2. å‡†å¤‡ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    irm_loss = IRMLoss(penalty_weight=1.0, penalty_anneal_epochs=10)
    base_loss_fn = nn.MSELoss()

    # 3. è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        model.train()

        # ä»æ¯ä¸ªç¯å¢ƒé‡‡æ ·batch
        data_by_env = {}
        for env_name, env_data in train_envs.items():
            # ç®€åŒ–ï¼šæ¯ä¸ªepochéšæœºé‡‡æ ·ä¸€ä¸ªbatch
            batch_size = min(128, len(env_data))
            indices = np.random.choice(len(env_data), batch_size, replace=False)

            # å‡è®¾env_dataæœ‰Xå’Œy
            X = torch.tensor(env_data.iloc[indices]["features"].values, device=device)
            y = torch.tensor(env_data.iloc[indices]["target"].values, device=device)

            data_by_env[env_name] = (X, y)

        # IRMæŸå¤±è®¡ç®—
        optimizer.zero_grad()

        loss_dict = irm_loss(
            model=model, data_by_env=data_by_env, base_loss_fn=base_loss_fn
        )

        total_loss = loss_dict["total_loss"]
        total_loss.backward()
        optimizer.step()

        # æ‰“å°è¿›åº¦
        if epoch % 5 == 0:
            print(f"\nEpoch {epoch}/{epochs}")
            print(f"Total Loss: {total_loss.item():.4f}")
            print(f"Env Losses: {loss_dict['env_losses']}")
            print(f"Grad Penalty: {loss_dict['grad_penalty']:.4f}")
            print(f"Penalty Weight: {loss_dict['current_penalty_weight']:.2f}")

        irm_loss.step_epoch()

    return model


# ========================================
# ä¸T-VICRegå¯¹æ¯”
# ========================================


def compare_irm_vs_tvicreg():
    """
    IRM vs T-VICReg å¯¹æ¯”åˆ†æ
    """
    comparison = {
        "IRM": {
            "ç›®æ ‡": "è·¨ç¯å¢ƒæ¢¯åº¦ä¸€è‡´æ€§",
            "ä¼˜åŒ–å¯¹è±¡": "æ¨¡å‹å‚æ•°çš„å› æœæ€§",
            "ç†è®ºåŸºç¡€": "å› æœMLï¼Œå¯»æ‰¾ä¸å˜é¢„æµ‹",
            "è®¡ç®—å¤æ‚åº¦": "é«˜ï¼ˆéœ€è¦äºŒé˜¶å¯¼æ•°ï¼‰",
            "é€‚ç”¨åœºæ™¯": "ç¯å¢ƒæ˜ç¡®å¯åˆ’åˆ†",
            "ä¼˜åŠ¿": "è‡ªåŠ¨å‰”é™¤ä¼ªå› å­",
            "åŠ£åŠ¿": "è®­ç»ƒæ…¢ï¼Œè¶…å‚æ•æ„Ÿ",
        },
        "T-VICReg [Research]": {
            "ç›®æ ‡": "è·¨ç¯å¢ƒåæ–¹å·®ä¸€è‡´æ€§",
            "ä¼˜åŒ–å¯¹è±¡": "è¡¨ç¤ºçš„ç»Ÿè®¡ç»“æ„",
            "ç†è®ºåŸºç¡€": "è‡ªç›‘ç£å­¦ä¹ ï¼Œè¡¨ç¤ºè§£è€¦",
            "è®¡ç®—å¤æ‚åº¦": "ä¸­ï¼ˆåªéœ€ä¸€é˜¶å¯¼æ•°ï¼‰",
            "é€‚ç”¨åœºæ™¯": "é¢„è®­ç»ƒé˜¶æ®µ",
            "ä¼˜åŠ¿": "è®­ç»ƒå¿«ï¼Œç¨³å®š",
            "åŠ£åŠ¿": "ç†è®ºä¸å¦‚IRMå¼º",
        },
    }

    print("=== IRM vs T-VICReg ===")
    for method, props in comparison.items():
        print(f"\n{method}:")
        for key, val in props.items():
            print(f"  {key}: {val}")


# ========================================
# ä½¿ç”¨å»ºè®®
# ========================================

"""
ä½•æ—¶ä½¿ç”¨IRMï¼š
1. æ•°æ®é‡å……è¶³ï¼ˆ>10kæ ·æœ¬ï¼‰
2. ç¯å¢ƒåˆ’åˆ†æ˜ç¡®ï¼ˆæ³¢åŠ¨ç‡ã€æ—¶é—´æ®µç­‰ï¼‰
3. è¿½æ±‚æ¨¡å‹é²æ£’æ€§ï¼Œå¯ä»¥æ¥å—è®­ç»ƒæ…¢

ä½•æ—¶ä½¿ç”¨T-VICRegï¼š
1. é¢„è®­ç»ƒé˜¶æ®µ
2. æ•°æ®é‡æœ‰é™
3. è¿½æ±‚è®­ç»ƒæ•ˆç‡

æ¨èç­–ç•¥ï¼š
- é¢„è®­ç»ƒï¼šT-VICReg
- å¾®è°ƒï¼šIRMï¼ˆåœ¨æ ‡æ³¨æ•°æ®ä¸Šï¼‰
"""


if __name__ == "__main__":
    # æ¼”ç¤ºIRMæŸå¤±è®¡ç®—

    # 1. æ¨¡æ‹Ÿæ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Linear(10, 1)

        def forward(self, x):
            return self.fc(x)

    model = SimpleModel()

    # 2. æ¨¡æ‹Ÿå¤šç¯å¢ƒæ•°æ®
    data_by_env = {
        "low_vol": (torch.randn(32, 10), torch.randn(32, 1)),
        "mid_vol": (torch.randn(32, 10), torch.randn(32, 1)),
        "high_vol": (torch.randn(32, 10), torch.randn(32, 1)),
    }

    # 3. è®¡ç®—IRMæŸå¤±
    irm = IRMLoss(penalty_weight=1.0)

    loss_dict = irm(model, data_by_env)

    print("=== IRM Loss Computation ===")
    print(f"Total Loss: {loss_dict['total_loss'].item():.4f}")
    print(f"Env Losses: {loss_dict['env_losses']}")
    print(f"Grad Penalty: {loss_dict['grad_penalty']:.4f}")

    # 4. å¯¹æ¯”åˆ†æ
    print("\n")
    compare_irm_vs_tvicreg()
