import os
import glob
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta

# ==========================================
# 1. æ¨¡å‹å®šä¹‰ (ä¿æŒä¸å˜)
# ==========================================
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

class InceptionBlock(nn.Module):
    def __init__(self, in_chan, out_chan):
        super().__init__()
        self.b1 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(0.01), nn.BatchNorm2d(out_chan))
        self.b2 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(0.01), 
                                nn.Conv2d(out_chan, out_chan, (3,1), padding=(1,0)), nn.LeakyReLU(0.01), nn.BatchNorm2d(out_chan))
        self.b3 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(0.01),
                                nn.Conv2d(out_chan, out_chan, (5,1), padding=(2,0)), nn.LeakyReLU(0.01), nn.BatchNorm2d(out_chan))
        self.b4 = nn.Sequential(nn.MaxPool2d((3,1), stride=1, padding=(1,0)),
                                nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(0.01), nn.BatchNorm2d(out_chan))
        self.se = SEBlock(out_chan * 4)
    def forward(self, x):
        out = torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)
        return self.se(out)

class TemporalAttention(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size, bias=False)
        self.value = nn.Linear(hidden_size, hidden_size, bias=False)
        self.scale = float(hidden_size) ** 0.5

    def forward(self, lstm_output):
        last_step = lstm_output[:, -1, :].unsqueeze(1)
        scores = torch.bmm(self.query(last_step), self.key(lstm_output).transpose(1, 2)) / self.scale
        attn_weights = F.softmax(scores, dim=-1)
        context = torch.bmm(attn_weights, self.value(lstm_output))
        return context.squeeze(1)

class HybridDeepLOB(nn.Module):
    def __init__(self, num_expert):
        super().__init__()
        c_chan = 32
        m_hid = 64
        l_hid = 128
        self.compress = nn.Sequential(
            nn.Conv2d(1, c_chan, (1, 2), stride=(1, 2)), nn.LeakyReLU(), nn.BatchNorm2d(c_chan),
            nn.Conv2d(c_chan, c_chan, (4, 1), padding='same'), nn.LeakyReLU(), nn.BatchNorm2d(c_chan),
            nn.Conv2d(c_chan, c_chan, (1, 2), stride=(1, 2)), nn.LeakyReLU(), nn.BatchNorm2d(c_chan),
            nn.Conv2d(c_chan, c_chan, (1, 5), stride=(1, 5)), nn.LeakyReLU(), nn.BatchNorm2d(c_chan),
        )
        self.inception1 = InceptionBlock(c_chan, c_chan)
        self.inception2 = InceptionBlock(128, 64)
        self.expert = nn.Sequential(
            nn.Linear(num_expert, m_hid), nn.LeakyReLU(), nn.BatchNorm1d(m_hid), nn.Dropout(0.2)
        )
        fusion_dim = 256 + m_hid
        self.lstm = nn.LSTM(fusion_dim, l_hid, num_layers=2, batch_first=True, dropout=0.5)
        self.attention = TemporalAttention(l_hid)
        self.dropout = nn.Dropout(0.5)
        self.head = nn.Sequential(
            nn.Linear(l_hid, 64), nn.LeakyReLU(), nn.Linear(64, 3)
        )

    def forward(self, x_lob, x_exp):
        x = x_lob.unsqueeze(1)
        feat = self.compress(x)
        feat = self.inception1(feat)
        feat = self.inception2(feat)
        feat = feat.squeeze(-1).permute(0, 2, 1)
        if feat.shape[1] != x_exp.shape[1]:
            feat = feat.permute(0, 2, 1)
            feat = nn.functional.adaptive_avg_pool1d(feat, x_exp.shape[1])
            feat = feat.permute(0, 2, 1)
        B, T, F = x_exp.shape
        exp = self.expert(x_exp.reshape(-1, F)).reshape(B, T, -1)
        combined = torch.cat([feat, exp], dim=2)
        lstm_out, _ = self.lstm(combined)
        context = self.attention(lstm_out)
        return self.head(self.dropout(context))

# ==========================================
# 2. å®æ—¶æµå¤„ç†å¼•æ“ (ä¿®æ­£ç‰ˆ)
# ==========================================
class TradingEngine:
    def __init__(self, model_path, config):
        self.cfg = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.model_path = model_path
        self.model = None 
        
        # äº¤æ˜“çŠ¶æ€
        self.cash = self.cfg['INITIAL_CAPITAL']
        self.shares = 0.0 # æŒä»“ä»½é¢ (float)
        self.initial_capital = self.cash
        self.cost_rate = self.cfg['TRADE_COST']
        
        # æ•°æ®å¤„ç†
        self.scaler = StandardScaler()
        self.weights = np.array([1.0, 0.8, 0.6, 0.4, 0.2]) # å‹åŠ›å› å­æƒé‡

    def warm_up_scaler(self, prev_date_files):
        """
        åŠ è½½å‰ä¸€å¤©çš„æ•°æ®ï¼Œè®¡ç®—ç‰¹å¾ï¼Œå¹¶æ‹Ÿåˆ StandardScalerã€‚
        """
        print(f"ğŸ”¥ ç³»ç»Ÿé¢„çƒ­: ä½¿ç”¨å†å²æ•°æ®æ‹Ÿåˆ Scaler... ({os.path.basename(prev_date_files[0])})")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        df_raw = self._load_and_process_pair(prev_date_files[0], prev_date_files[1])
        
        # 2. è®¡ç®—ç‰¹å¾
        df_features = self._calc_factors_stream(df_raw)
        
        # 3. æ¸…æ´—æ•°æ® (æ‰¹é‡æ¨¡å¼ä¸‹ï¼Œä¸¢å¼ƒ pct_change äº§ç”Ÿçš„ NaN)
        df_features = df_features.replace([np.inf, -np.inf], np.nan).dropna()
        
        # 4. æå–ç‰¹å¾åˆ—
        exp_cols = [c for c in df_features.columns if c.startswith('feat_') or c.startswith('meta_')]
        self.exp_cols = exp_cols
        
        if len(exp_cols) == 0:
            raise ValueError("âŒ é”™è¯¯: æœªæ‰¾åˆ°ç‰¹å¾åˆ—ã€‚è¯·æ£€æŸ¥ _calc_factors_stream æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚")
            
        print(f"   -> æ‰¾åˆ° {len(exp_cols)} ä¸ªç‰¹å¾åˆ—")
        
        # 5. æ‹Ÿåˆ Scaler
        self.scaler.fit(df_features[exp_cols].values)
        print("âœ… Scaler æ‹Ÿåˆå®Œæˆ")
        
        # 6. åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹
        self.model = HybridDeepLOB(num_expert=len(exp_cols)).to(self.device)
        try:
            self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
            self.model.eval()
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            exit()

    def run_simulation(self, main_file, aux_file):
        """æ¨¡æ‹Ÿå®æ—¶äº¤æ˜“ä¸»å¾ªç¯"""
        print(f"ğŸ¬ å¼€å§‹æ¨¡æ‹Ÿäº¤æ˜“: {os.path.basename(main_file)}")
        
        # 1. åŠ è½½æ•°æ®æµ
        df_raw = self._load_and_process_pair(main_file, aux_file)
        
        # [å…³é”®ä¿®å¤] ä¸ºåŸå§‹æ•°æ®è®¡ç®— midï¼Œä¾› execute_trade ä½¿ç”¨
        df_raw['mid'] = (df_raw['bp1'] + df_raw['sp1']) / 2
        
        timestamps = df_raw.index
        lookback = self.cfg['LOOKBACK']
        
        # 2. å¾ªç¯å›æ”¾
        buffer_size = lookback + 5
        if len(df_raw) < buffer_size:
            print("âš ï¸ æ•°æ®è¿‡çŸ­")
            return

        print(f"â³ æ•°æ®æµå›æ”¾ä¸­... (å…± {len(df_raw)} ä¸ª Tick)")
        
        # è®°å½•æœ€åä¸€ä¸ªæœ‰æ•ˆä»·æ ¼ç”¨äºç»“ç®—
        self.last_known_price = 0.0

        for i in range(buffer_size, len(df_raw)):
            current_time = timestamps[i]
            current_row = df_raw.iloc[i] 
            self.last_known_price = current_row['mid']
            
            # --- æ¨¡æ‹Ÿåªçœ‹å¾—åˆ°è¿‡å» ---
            slice_start = max(0, i - lookback - 10) 
            df_slice = df_raw.iloc[slice_start : i+1].copy()
            
            # --- å®æ—¶ç‰¹å¾å·¥ç¨‹ ---
            df_features = self._calc_factors_stream(df_slice)
            
            if len(df_features) < lookback: continue
                
            # å–æœ€å lookback è¡Œä½œä¸ºè¾“å…¥
            input_df = df_features.iloc[-lookback:]
            
            # --- æ¨ç†ä¸æ‰§è¡Œ ---
            x_lob, x_exp = self._prepare_tensor(input_df)
            signal, confidence, probs = self._infer(x_lob, x_exp)
            
            # è¿™é‡Œä¼ å…¥çš„ current_row æ­¤æ—¶å·²ç»åŒ…å«äº† 'mid'
            self._execute_trade(signal, confidence, current_row, current_time)
            
        self._report_final()

    def _execute_trade(self, signal, confidence, row, time):
        """ç®€å•çš„æ‰§è¡Œé€»è¾‘"""
        # [ä¿®å¤] è¿™é‡Œçš„ row['mid'] ç°åœ¨æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ run_simulation é‡ŒåŠ ä¸Šäº†
        current_price = row['mid']
        if current_price <= 0: return

        conf_thresh = self.cfg['CONF_THRESHOLD']
        max_pos = self.cfg['MAX_POSITION']
        
        market_value = self.shares * current_price
        total_asset = self.cash + market_value
        
        # Buy Signal (1)
        if signal == 1:
             # è®¡ç®—ç›®æ ‡ä»“ä½
             scale = min((confidence - conf_thresh) / (1 - conf_thresh), max_pos)
             target_val = total_asset * scale
             cost = target_val - market_value # éœ€è¦è¡¥å¤šå°‘é’±
             
             if self.shares == 0 and cost > 2000: # ç®€åŒ–ï¼šåªåšä»0å¼€ä»“ï¼Œæˆ–åŠ ä»“
                 buy_shares = cost / current_price 
                 fee = cost * self.cost_rate
                 self.shares += buy_shares
                 self.cash -= (cost + fee)
                 print(f"[{time}] ğŸ”´ BUY  @{current_price:.3f} | Conf:{confidence:.2f} | Cash:{self.cash:.0f}")

        # Sell Signal (2)
        elif signal == 2 and self.shares > 0:
             # ç®€åŒ–ï¼šæ¸…ä»“
             revenue = self.shares * current_price
             fee = revenue * self.cost_rate
             self.cash += (revenue - fee)
             self.shares = 0
             print(f"[{time}] ğŸŸ¢ SELL @{current_price:.3f} | Conf:{confidence:.2f} | Cash:{self.cash:.0f}")

    def _infer(self, x_lob, x_exp):
        with torch.no_grad():
            x_lob = x_lob.unsqueeze(0).to(self.device)
            x_exp = x_exp.unsqueeze(0).to(self.device)
            
            logits = self.model(x_lob, x_exp)
            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
            
            p_hold, p_buy, p_sell = probs
            signal = 0; conf = 0.0
            
            if p_buy > self.cfg['CONF_THRESHOLD'] and p_buy > p_sell:
                signal = 1; conf = p_buy
            elif p_sell > self.cfg['CONF_THRESHOLD'] and p_sell > p_buy:
                signal = 2; conf = p_sell
                
            return signal, conf, probs

    def _prepare_tensor(self, df):
        lob_cols = [f'{s}{i}' for i in range(1,6) for s in ['bp','sp']] + [f'{s}{i}' for i in range(1,6) for s in ['bv','sv']]
        
        mid = df['mid'].values.reshape(-1, 1)
        safe_mid = np.where(mid==0, 1.0, mid)
        
        lob_data = df[lob_cols].values.copy()
        lob_data[:, :10] = (lob_data[:, :10] - mid) / safe_mid * 10000
        lob_data[:, 10:] = np.log1p(lob_data[:, 10:])
        
        exp_data = df[self.exp_cols].values
        exp_data = np.nan_to_num(exp_data) 
        exp_data_scaled = self.scaler.transform(exp_data)
        
        return torch.tensor(lob_data, dtype=torch.float32), torch.tensor(exp_data_scaled, dtype=torch.float32)

    def _calc_factors_stream(self, df):
        """è®¡ç®—å› å­"""
        df = df.copy()
        
        # 1. Meta Factors
        sec = df.index.hour * 3600 + df.index.minute * 60 + df.index.second
        df['meta_time'] = np.clip(np.where(sec <= 41400, (sec-34200)/14400, 0.5+(sec-46800)/14400), 0, 1)
        
        # 2. Micro Factors
        mid = (df['bp1'] + df['sp1']) / 2
        df['mid'] = mid
        
        wb = sum(df[f'bv{i}']*self.weights[i-1] for i in range(1,6))
        wa = sum(df[f'sv{i}']*self.weights[i-1] for i in range(1,6))
        df['feat_micro_pressure'] = (wb - wa) / (wb + wa + 1e-8)
        
        # 3. Oracle Factors
        if 'index_price' in df.columns:
            # [ä¿®å¤] è­¦å‘Šä¿®å¤: method='ffill' -> ffill()
            safe_mid = mid.replace(0, np.nan).ffill()
            df['feat_oracle_basis'] = (df['index_price'] - safe_mid) / safe_mid
            df['feat_oracle_idx_mom'] = df['index_price'].pct_change(2)
            
        if 'fut_price' in df.columns:
            df['feat_oracle_fut_lead'] = df['fut_price'].pct_change()

        # 4. Peer Factors
        df['feat_peer_diff'] = df['price'].pct_change() - df['peer_price'].pct_change()
        
        # æµå¼å¤„ç†ä¸­ç”¨0å¡«å……
        if len(df) < 200: 
             df = df.fillna(0)
             
        return df

    def _load_and_process_pair(self, m_path, a_path):
        """è¯»å–å¹¶æŒ‰ 3s é‡é‡‡æ ·å¯¹é½"""
        date_str = os.path.basename(m_path).split('sz159920-')[-1].replace('.csv', '')
        
        def _read(p):
            d = pd.read_csv(p)
            d['datetime'] = pd.to_datetime(date_str + ' ' + d['tx_server_time'])
            return d.set_index('datetime').sort_index().groupby(level=0).last()
        
        df_m, df_a = _read(m_path), _read(a_path)
        
        agg = {
            'price': 'last', 'tick_vol': 'sum',
            'bp1': 'last', 'sp1': 'last', 'bp2': 'last', 'sp2': 'last', 
            'bp3': 'last', 'sp3': 'last', 'bp4': 'last', 'sp4': 'last', 
            'bp5': 'last', 'sp5': 'last', 'bv1': 'last', 'sv1': 'last',
            'bv2': 'last', 'sv2': 'last', 'bv3': 'last', 'sv3': 'last',
            'bv4': 'last', 'sv4': 'last', 'bv5': 'last', 'sv5': 'last',
        }
        for c in ['index_price', 'fut_price', 'fut_imb']:
            if c in df_m.columns: agg[c] = 'last'
            
        # [ä¿®å¤] è­¦å‘Šä¿®å¤: '3S' -> '3s' (lowercase s)
        df_m = df_m.resample(self.cfg['RESAMPLE_FREQ']).agg(agg)
        df_a = df_a.resample(self.cfg['RESAMPLE_FREQ']).agg({'price': 'last', 'tick_vol': 'sum'})
        df_a.columns = ['peer_price', 'peer_vol']
        
        # [ä¿®å¤] è­¦å‘Šä¿®å¤: fillna(method='ffill') -> ffill()
        df = df_m.join(df_a, how='inner').ffill().dropna()
        return df

    def _report_final(self):
        # ä½¿ç”¨æœ€åè®°å½•çš„ä»·æ ¼è®¡ç®—å‡€å€¼
        last_price = self.last_known_price if hasattr(self, 'last_known_price') and self.last_known_price > 0 else 1.0
        
        market_val = self.shares * last_price
        total_asset = self.cash + market_val
        pnl = total_asset - self.initial_capital
        ret = (pnl / self.initial_capital) * 100
        
        print("\n" + "="*40)
        print(f"ğŸ æ¨¡æ‹Ÿç»“æŸ")
        print(f"æœ€ç»ˆèµ„é‡‘: {self.cash:.2f}")
        print(f"æŒä»“ä»½é¢: {self.shares:.2f} (å¸‚å€¼: {market_val:.2f})")
        print(f"æ€»èµ„äº§  : {total_asset:.2f}")
        print(f"æ€»æ”¶ç›Š  : {pnl:.2f} ({ret:.2f}%)")
        print("="*40)

# ==========================================
# 3. è¿è¡Œå…¥å£
# ==========================================
CONFIG = {
    'DATA_DIR': './data',          
    'MAIN_SYMBOL': 'sz159920',     
    'AUX_SYMBOL': 'sh513130',      
    'RESAMPLE_FREQ': '3s',         # [ä¿®å¤] ä½¿ç”¨å°å†™ 's'
    'LOOKBACK': 60,                
    'TRADE_COST': 0.0001,          
    'INITIAL_CAPITAL': 200000,      
    'CONF_THRESHOLD': 0.75,        
    'MAX_POSITION': 0.9,           
}

def find_files_recursive(data_dir, main_sym, aux_sym):
    """è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°å’Œæ¬¡æ–°çš„æ•°æ®"""
    m_pattern = os.path.join(data_dir, "**", f"{main_sym}*.csv")
    m_files = sorted(glob.glob(m_pattern, recursive=True))
    
    dates = []
    date_map = {}
    
    for f in m_files:
        try:
            base = os.path.basename(f)
            d_str = base.split(f"{main_sym}-")[-1].replace('.csv', '')
            dates.append(d_str)
            date_map[d_str] = f
        except: pass
        
    dates.sort()
    if len(dates) < 2:
        raise ValueError(f"æ•°æ®ä¸è¶³ 2 å¤©ï¼Œæ‰¾åˆ°çš„æ—¥æœŸ: {dates}")
        
    latest = dates[-1]
    prev = dates[-2]
    
    def get_aux(d):
        m_file = date_map[d]
        aux_name = os.path.basename(m_file).replace(main_sym, aux_sym)
        aux_path = os.path.join(os.path.dirname(m_file), aux_name)
        if not os.path.exists(aux_path):
            aux_pattern = os.path.join(data_dir, "**", aux_name)
            found = glob.glob(aux_pattern, recursive=True)
            if found: aux_path = found[0]
            else: raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¾…åŠ©æ–‡ä»¶: {aux_name}")
        return aux_path

    return (date_map[latest], get_aux(latest)), (date_map[prev], get_aux(prev))

if __name__ == "__main__":
    try:
        latest_pair, prev_pair = find_files_recursive(CONFIG['DATA_DIR'], CONFIG['MAIN_SYMBOL'], CONFIG['AUX_SYMBOL'])
        print(f"ğŸ“… ç›®æ ‡æ—¥: {os.path.basename(latest_pair[0])}")
        print(f"ğŸ“… é¢„çƒ­æ—¥: {os.path.basename(prev_pair[0])}")
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æŸ¥æ‰¾å¤±è´¥: {e}")
        exit()

    model_file = 'alpha_model_v8_stable.pth'
    if not os.path.exists(model_file):
        found = glob.glob(f"**/{model_file}", recursive=True)
        if found: model_file = found[0]
        else:
            print("âŒ æ‰¾ä¸åˆ° .pth æ¨¡å‹æ–‡ä»¶")
            exit()

    engine = TradingEngine(model_file, CONFIG)
    engine.warm_up_scaler(prev_pair)
    engine.run_simulation(latest_pair[0], latest_pair[1])