import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
import joblib
import math

# æ£€æµ‹ GPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ è®¡ç®—è®¾å¤‡: {DEVICE}")

# ==============================================================================
# 1. ä¸“ä¸šçº§æ•°æ®å¤„ç†å™¨ (é’ˆå¯¹ 34 åˆ—æ•°æ®å®šåˆ¶)
# ==============================================================================
class DataProcessorPro:
    def __init__(self, filepath, lookback=60, horizon=20):
        self.filepath = filepath
        self.lookback = lookback  # è¾“å…¥è¿‡å» 3åˆ†é’Ÿ (60 * 3s)
        self.horizon = horizon    # é¢„æµ‹æœªæ¥ 1åˆ†é’Ÿ
        self.scaler = StandardScaler()

    def load_and_process(self):
        print(f"âš¡ æ­£åœ¨è¯»å–å…¨é‡æ•°æ®: {self.filepath}")
        try:
            # 1. è¯»å–æ•°æ®
            raw = pd.read_csv(self.filepath)
            
            # 2. æ—¶é—´ç´¢å¼•å¤„ç† (å…³é”®)
            # ä½¿ç”¨ tx_local_time (æ¯«ç§’æ—¶é—´æˆ³) æœ€å‡†ç¡®ï¼Œtx_server_time æ˜¯å­—ç¬¦ä¸²ï¼Œä¸å¥½å¤„ç†
            raw['datetime'] = pd.to_datetime(raw['tx_local_time'], unit='ms')
            
            # è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (å¦‚æœæœºå™¨æ˜¯UTC)
            if raw['datetime'].dt.tz is None:
                raw['datetime'] = raw['datetime'].dt.tz_localize('UTC').dt.tz_convert('Asia/Shanghai')
            
            df = raw.set_index('datetime').sort_index()

            # 3. æ•°æ®é‡é‡‡æ · (Resampling) - å¿…é¡»è¦†ç›–æ‰€æœ‰å…³é”®å­—æ®µ
            # æˆ‘ä»¬å°†æ•°æ®é™é¢‘åˆ° 3ç§’ ä¸€ä¸ªç‚¹ï¼Œä»¥å‡å°‘å™ªéŸ³
            agg_rules = {
                # ä»·æ ¼ä¸åŸºç¡€
                'price': 'last', 'tick_vol': 'sum', 'tick_amt': 'sum', 'tick_vwap': 'mean',
                'premium_rate': 'last', 'iopv': 'last', 'index_price': 'last', 
                'fx_rate': 'last', 'sentiment': 'last', 'interval_s': 'sum',
                
                # äº”æ¡£ç›˜å£ (L1-L5)
                'bp1':'last', 'bv1':'last', 'sp1':'last', 'sv1':'last',
                'bp2':'last', 'bv2':'last', 'sp2':'last', 'sv2':'last',
                'bp3':'last', 'bv3':'last', 'sp3':'last', 'sv3':'last',
                'bp4':'last', 'bv4':'last', 'sp4':'last', 'sv4':'last',
                'bp5':'last', 'bv5':'last', 'sp5':'last', 'sv5':'last',
            }
            
            # å…¼å®¹æœŸè´§å­—æ®µ (å¦‚æœæœ‰)
            if 'fut_price' in df.columns:
                agg_rules.update({
                    'fut_price': 'last', 'fut_mid': 'last', 
                    'fut_imb': 'mean', 'fut_delta_vol': 'sum', 'fut_pct': 'last'
                })

            # æ‰§è¡Œé‡é‡‡æ ·
            df = df.resample('3s').agg(agg_rules).ffill().dropna()

            # 4. === æ·±åº¦ç‰¹å¾å·¥ç¨‹ (Hardcore Feature Engineering) ===
            print("æ­£åœ¨æ„å»ºé«˜é˜¶ç‰¹å¾...")

            # --- A. åŸºç¡€å¾®è§‚ç‰¹å¾ ---
            df['mid_price'] = (df['bp1'] + df['sp1']) / 2
            df['log_ret'] = np.log(df['mid_price'] / df['mid_price'].shift(1)).fillna(0)
            
            # --- B. Smart Money ç—•è¿¹ (VWAP Bias) ---
            # å¦‚æœæˆäº¤å‡ä»· > ä¸­é—´ä»·ï¼Œè¯´æ˜ä¹°æ–¹åœ¨ä¸»åŠ¨å‘ä¸Šåƒå•
            df['vwap_bias'] = (df['tick_vwap'] - df['mid_price']) / df['mid_price'] * 10000

            # --- C. æ·±åº¦å¤±è¡¡ (Weighted Depth Imbalance) ---
            # è¶Šé è¿‘ç›˜å£çš„æŒ‚å•ï¼Œæƒé‡è¶Šå¤§
            weights = [1.0, 0.8, 0.6, 0.4, 0.2]
            sum_bid = sum(df[f'bv{i}'] * w for i, w in zip(range(1, 6), weights))
            sum_ask = sum(df[f'sv{i}'] * w for i, w in zip(range(1, 6), weights))
            df['depth_imb'] = (sum_bid - sum_ask) / (sum_bid + sum_ask + 1e-6)

            # --- D. ç›˜å£æ–œç‡ (Order Book Slope) ---
            # åˆ¤æ–­ L1 åˆ° L5 çš„ä»·æ ¼åˆ†å¸ƒæ˜¯å¦é™¡å³­
            # æ–œç‡è¶Šå°ï¼Œè¯´æ˜æŒ‚å•è¶Šå¯†é›†ï¼Œæ”¯æ’‘/å‹åŠ›è¶Šå¼º
            df['bid_slope'] = (df['bp1'] - df['bp5']) / 5
            df['ask_slope'] = (df['sp5'] - df['sp1']) / 5
            
            # --- E. å¹¿ä¹‰æµåŠ¨æ€§ (Total Liquidity) ---
            df['total_depth'] = np.log(sum_bid + sum_ask + 1)

            # --- F. æœŸç°è”åŠ¨ (Futures Basis) ---
            if 'fut_price' in df.columns:
                # åŸºå·®ç‡
                df['basis_rate'] = (df['fut_price'] - df['price']) / df['price']
                # æœŸè´§ä¹°å–å‹åŠ›
                df['fut_pressure'] = df['fut_imb']

            # --- G. æƒ…ç»ªåŠ é€Ÿ (Sentiment Momentum) ---
            df['sent_acc'] = df['sentiment'].diff().fillna(0)

            # 5. å‰”é™¤æ— ç”¨åˆ—ï¼Œä¿ç•™çº¯æ•°å€¼ç‰¹å¾
            # æˆ‘ä»¬ä¸éœ€è¦ tx_server_time ç­‰å­—ç¬¦ä¸²äº†
            # å°†æ‰€æœ‰è®¡ç®—å¥½çš„ç‰¹å¾æ”¾å…¥ feature_cols
            drop_cols = ['tx_server_time', 'tx_local_time', 'bd_server_time', 'bd_local_time']
            # åªä¿ç•™æ•°å€¼ç±»å‹çš„åˆ—
            numeric_df = df.select_dtypes(include=[np.number])
            
            # 6. æ„å»ºé¢„æµ‹ç›®æ ‡ (Target): æœªæ¥æ³¢åŠ¨ç‡
            # é¢„æµ‹æœªæ¥ Horizon å†…çš„å¯¹æ•°æ”¶ç›Šç‡æ ‡å‡†å·®
            indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=self.horizon)
            target_vol = df['log_ret'].rolling(window=indexer).std() * 10000 # æ”¾å¤§ä¸º bp
            
            # æœ€ç»ˆæ¸…æ´—
            final_df = numeric_df.copy()
            final_df['target'] = target_vol
            final_df = final_df.replace([np.inf, -np.inf], 0).dropna()
            
            self.feature_cols = [c for c in final_df.columns if c != 'target']
            print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œè¾“å…¥ç»´åº¦: {len(self.feature_cols)} (å«L5æ·±åº¦+æœŸè´§æ•°æ®)")
            
            return final_df, self.feature_cols
            
        except Exception as e:
            print(f"æ•°æ®å¤„ç†å‡ºé”™: {e}")
            return None, []

    def get_tensors(self, df, feature_cols, fit_scaler=False):
        data = df[feature_cols].values
        target = df['target'].values
        
        if fit_scaler:
            data = self.scaler.fit_transform(data)
        else:
            data = self.scaler.transform(data)
            
        X, y = [], []
        # æ»‘åŠ¨çª—å£åˆ‡ç‰‡
        for i in range(self.lookback, len(data)):
            X.append(data[i-self.lookback : i])
            y.append(target[i])
            
        return np.array(X), np.array(y)

# ==============================================================================
# 2. æ··åˆæŒ–æ˜æ¨¡å‹ (Hybrid Miner Architecture)
# ==============================================================================
class AlphaLayer(nn.Module):
    """ æ¨¡æ‹Ÿ Quant æ‰‹å·¥æŒ–æ˜: è‡ªåŠ¨è®¡ç®—æ»šåŠ¨å‡å€¼ã€æ³¢åŠ¨ç‡ """
    def __init__(self, input_dim, window=20):
        super(AlphaLayer, self).__init__()
        self.window = window
        self.pool = nn.AvgPool1d(kernel_size=window, stride=1, padding=0)
    
    def forward(self, x):
        x = x.permute(0, 2, 1) # (Batch, Feat, Seq)
        # Padding
        pad = torch.zeros(x.shape[0], x.shape[1], self.window-1).to(x.device)
        x_pad = torch.cat([pad, x], dim=2)
        
        mean = self.pool(x_pad)
        x2_pad = torch.cat([pad, x**2], dim=2)
        mean_sq = self.pool(x2_pad)
        std = torch.sqrt(torch.clamp(mean_sq - mean**2, min=1e-6))
        
        # æ‹¼æ¥: åŸå§‹ + å‡å€¼ + æ³¢åŠ¨ç‡
        out = torch.cat([x, mean, std], dim=1)
        return out.permute(0, 2, 1)

class TemporalBlock(nn.Module):
    """ TCN Block: æ•æ‰å±€éƒ¨çªå˜ """
    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout=0.2):
        super(TemporalBlock, self).__init__()
        padding = (kernel_size - 1) * dilation
        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, padding=padding, dilation=dilation)
        self.act1 = nn.GELU()
        self.do1 = nn.Dropout(dropout)
        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, padding=padding, dilation=dilation)
        self.act2 = nn.GELU()
        self.do2 = nn.Dropout(dropout)
        self.chomp = padding
        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None

    def forward(self, x):
        out = self.conv1(x)[:, :, :-self.chomp]
        out = self.act1(self.do1(out))
        out = self.conv2(out)[:, :, :-self.chomp]
        out = self.act2(self.do2(out))
        res = x if self.downsample is None else self.downsample(x)
        return out + res

class HybridMinerNet(nn.Module):
    def __init__(self, input_dim, d_model=128, n_factors=128):
        super(HybridMinerNet, self).__init__()
        
        # 1. ç‰¹å¾è£‚å˜ (Input -> 3x Input)
        self.alpha_layer = AlphaLayer(input_dim, window=20)
        
        # 2. å±€éƒ¨æ„ŸçŸ¥ (TCN)
        # å°†è£‚å˜åçš„ç‰¹å¾å‹ç¼©åˆ° d_model
        self.tcn = TemporalBlock(input_dim*3, d_model, kernel_size=3, dilation=1)
        
        # 3. å…¨å±€æ³¨æ„åŠ› (Transformer)
        self.pos_encoder = self._make_pos_encoding(d_model)
        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=256, dropout=0.1)
        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=3)
        
        # 4. å› å­ç”Ÿæˆå¤´ (è¾“å‡º 128 ä¸ªå› å­)
        self.factor_head = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.GELU(),
            nn.Linear(256, n_factors),
            nn.Tanh() # å½’ä¸€åŒ–åˆ° [-1, 1]
        )
        
        # 5. è¾…åŠ©é¢„æµ‹ (Lossæ¥æº)
        self.predictor = nn.Linear(n_factors, 1)

    def _make_pos_encoding(self, d_model, max_len=5000):
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        return nn.Parameter(pe.unsqueeze(0).transpose(0, 1), requires_grad=False)

    def forward(self, x):
        # x: (Batch, Seq, Feat)
        x = self.alpha_layer(x)       # -> (Batch, Seq, Feat*3)
        
        x = x.permute(0, 2, 1)        # -> (Batch, Channel, Seq) for TCN
        x = self.tcn(x)               # -> (Batch, d_model, Seq)
        
        x = x.permute(2, 0, 1)        # -> (Seq, Batch, d_model) for Transformer
        x = x + self.pos_encoder[:x.size(0), :]
        x = self.transformer(x)
        
        last_step = x[-1, :, :]       # (Batch, d_model)
        factors = self.factor_head(last_step) # (Batch, 128)
        pred = self.predictor(factors)
        
        return pred, factors

# ==============================================================================
# 3. è®­ç»ƒä¸æå–æµç¨‹
# ==============================================================================
def run_mining():
    FILE = 'sz159920.csv' # ç¡®ä¿ä½ çš„CSVåœ¨è¿™ä¸ªè·¯å¾„
    
    # 1. åŠ è½½å¤„ç†
    proc = DataProcessorPro(FILE)
    df, feat_cols = proc.load_and_process()
    
    if df is None: return

    # åˆ’åˆ†æ•°æ®é›†
    split = int(len(df) * 0.8)
    train_df = df.iloc[:split]
    test_df = df.iloc[split:]
    
    X_train, y_train = proc.get_tensors(train_df, feat_cols, fit_scaler=True)
    X_test, y_test = proc.get_tensors(test_df, feat_cols, fit_scaler=False)
    
    # ä¿å­˜ Scaler (ç”¨äºå®æ—¶å®ç›˜)
    joblib.dump(proc.scaler, 'miner_scaler.pkl')
    
    train_loader = DataLoader(
        TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train)),
        batch_size=128, shuffle=True
    )
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = HybridMinerNet(input_dim=len(feat_cols), d_model=128, n_factors=128).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()
    
    # 3. è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹æŒ–æ˜ (AlphaNet + TCN + Transformer)...")
    model.train()
    for epoch in range(50):
        total_loss = 0
        for X, y in train_loader:
            X, y = X.to(DEVICE), y.to(DEVICE)
            pred, _ = model(X)
            loss = criterion(pred.squeeze(), y)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}: Loss {total_loss/len(train_loader):.6f}")
        
    torch.save(model.state_dict(), 'miner_model_128.pth')
    print("æ¨¡å‹å·²ä¿å­˜ -> miner_model_128.pth")
    
    # 4. å¯¼å‡ºå› å­
    print("\næ­£åœ¨å¯¼å‡º 128 ç»´åˆæˆå› å­...")
    model.eval()
    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test).to(DEVICE)
        _, factors = model(X_test_tensor)
        factors_np = factors.cpu().numpy()
        
        # æ„é€  DataFrame
        cols = [f'Latent_{i:03d}' for i in range(128)]
        # æ³¨æ„ç´¢å¼•å¯¹é½ (lookback=60)
        valid_idx = test_df.index[60:]
        factor_df = pd.DataFrame(factors_np, columns=cols, index=valid_idx)
        factor_df['target_vol'] = y_test
        
        # ç®€å•éªŒè¯
        ic_scores = []
        for c in cols:
            ic = factor_df[c].corr(factor_df['target_vol'])
            ic_scores.append((c, abs(ic)))
        ic_scores.sort(key=lambda x: x[1], reverse=True)
        
        print(f"Top 3 å› å­ IC: {ic_scores[:3]}")
        factor_df.to_csv("mined_128_factors.csv")
        print("âœ… å› å­è¡¨å·²ä¿å­˜: mined_128_factors.csv")

if __name__ == "__main__":
    run_mining()