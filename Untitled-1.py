import os
import glob
import warnings
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

warnings.filterwarnings('ignore')

# ==========================================
# 1. å…¨å±€é…ç½® (Configuration)
# ==========================================
CONFIG = {
    # --- è·¯å¾„ ---
    'DATA_DIR': './data',          # æ•°æ®æ ¹ç›®å½•
    'MAIN_SYMBOL': 'sz159920',     # äº¤æ˜“æ ‡çš„
    'AUX_SYMBOL': 'sh513130',      # è¾…åŠ©æ ‡çš„
    
    # --- å› å­ä¸æ•°æ® ---
    'RESAMPLE_FREQ': '3S',         # 3ç§’é‡é‡‡æ ·
    'PREDICT_HORIZON': 60,         # é¢„æµ‹æœªæ¥ 60ä¸ªå‘¨æœŸ (180ç§’)
    'COST_THRESHOLD': 0.002,       # åˆ©æ¶¦é—¨æ§› (20bps)
    'LOB_DEPTH': 5,                # ç›˜å£æ·±åº¦
    
    # --- è®­ç»ƒå‚æ•° ---
    'BATCH_SIZE': 512,
    'EPOCHS': 30,
    'LR': 1e-4,
    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',
    'TRAIN_SPLIT': 0.8,            # å‰80%æ—¥æœŸè®­ç»ƒï¼Œå20%éªŒè¯
    'LOOKBACK': 60,                # æ¯ä¸€ä¸ªæ ·æœ¬å›çœ‹ 60 ä¸ªæ—¶é—´æ­¥ (180ç§’)
}

# ==========================================
# 2. æ•°æ®å¤„ç†ä¸å› å­ç†”ç‚‰ (Alpha Forge)
# ==========================================
class AlphaForge:
    def __init__(self, cfg):
        self.cfg = cfg
        # ç›˜å£åŠ æƒæƒé‡ (Level 1 -> Level 5)
        self.weights = np.array([1.0, 0.8, 0.6, 0.4, 0.2])

    def load_and_process(self):
        """ä¸»æµç¨‹ï¼šåŠ è½½æ‰€æœ‰æ–‡ä»¶å¹¶ç”Ÿæˆå…¨é‡æ•°æ®"""
        print(f"ğŸš€ [AlphaForge] å¯åŠ¨... æ‰«æç›®å½•: {self.cfg['DATA_DIR']}")
        
        pairs = self._match_files()
        all_dfs = []
        
        for date, main_f, aux_f in pairs:
            try:
                # 1. åŠ è½½ & å¯¹é½
                df = self._load_pair(main_f, aux_f, date)
                if df is None or len(df) < 200: continue
                
                # 2. è®¡ç®—å› å­
                df = self._calc_factors(df)
                
                # 3. ç”Ÿæˆæ ‡ç­¾
                df = self._make_labels(df)
                
                all_dfs.append(df.dropna())
                print(f"  -> {date}: æ ·æœ¬æ•° {len(df)} | Buyä¿¡å· {(df['label']==1).sum()}")
            except Exception as e:
                print(f"  -> {date} å¤„ç†å‡ºé”™: {e}")
                
        if not all_dfs:
            raise ValueError("æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆæ•°æ®ï¼")
            
        full_df = pd.concat(all_dfs)
        return full_df.sort_index()

    def _match_files(self):
        """æ–‡ä»¶é…å¯¹"""
        m_path = os.path.join(self.cfg['DATA_DIR'], self.cfg['MAIN_SYMBOL'], f"*-*.csv")
        a_path = os.path.join(self.cfg['DATA_DIR'], self.cfg['AUX_SYMBOL'], f"*-*.csv")
        m_files = {self._get_date(f): f for f in glob.glob(m_path)}
        a_files = {self._get_date(f): f for f in glob.glob(a_path)}
        common = sorted(list(set(m_files.keys()) & set(a_files.keys())))
        return [(d, m_files[d], a_files[d]) for d in common]

    def _get_date(self, path):
        # å‡è®¾æ–‡ä»¶å: sz159920-2025-12-05.csv
        return os.path.basename(path).split('.')[0].split('-')[-1] # å–æœ€åä¸€æ®µä½œä¸ºæ—¥æœŸï¼Œæˆ–è€…æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´

    def _load_pair(self, m_path, a_path, date_str):
        """è¯»å–åŒæµæ•°æ®å¹¶å†…è¿æ¥"""
        def read_one(path):
            d = pd.read_csv(path)
            # å…¼å®¹å¤šç§æ—¥æœŸæ ¼å¼ï¼Œè¿™é‡Œå‡è®¾æ–‡ä»¶åå·²åŒ…å«æ—¥æœŸï¼Œæˆ–è€…é€šè¿‡å‚æ•°ä¼ å…¥
            # ä¸ºäº†ç¨³å¥ï¼Œç›´æ¥æ‹¼åˆ
            base_date = os.path.basename(path).split('-')[1:] # å‡è®¾ sz159920-2025-12-05
            date_part = "-".join(base_date).replace('.csv','')
            
            d['datetime'] = pd.to_datetime(date_part + ' ' + d['tx_server_time'])
            d = d.set_index('datetime').sort_index()
            # å¿«ç…§å»é‡
            return d.groupby(level=0).last()

        df_m = read_one(m_path)
        df_a = read_one(a_path)
        
        # å®šä¹‰èšåˆè§„åˆ™
        agg_dict = {
            'price': 'last', 'tick_vol': 'sum',
            'bp1': 'last', 'sp1': 'last', # Level 1
            'bp2': 'last', 'sp2': 'last',
            'bp3': 'last', 'sp3': 'last',
            'bp4': 'last', 'sp4': 'last',
            'bp5': 'last', 'sp5': 'last',
            'bv1': 'last', 'sv1': 'last',
            'bv2': 'last', 'sv2': 'last',
            'bv3': 'last', 'sv3': 'last',
            'bv4': 'last', 'sv4': 'last',
            'bv5': 'last', 'sv5': 'last',
        }
        # æ£€æŸ¥å¯é€‰åˆ—
        for c in ['index_price', 'fut_price', 'fut_imb']:
            if c in df_m.columns: agg_dict[c] = 'last'
            
        # é‡é‡‡æ ·
        df_m_res = df_m.resample(self.cfg['RESAMPLE_FREQ']).agg(agg_dict)
        df_a_res = df_a.resample(self.cfg['RESAMPLE_FREQ']).agg({'price': 'last', 'tick_vol': 'sum'})
        df_a_res.columns = ['peer_price', 'peer_vol']
        
        # å†…è¿æ¥å¯¹é½
        return df_m_res.join(df_a_res, how='inner')

    def _calc_factors(self, df):
        """è®¡ç®—æ··åˆå› å­"""
        # --- 1. Meta Factors (æ—¶é—´/çŠ¶æ€) ---
        seconds = df.index.hour * 3600 + df.index.minute * 60 + df.index.second
        df['meta_time_norm'] = (seconds - 34200) / 14400 # ç®€å•å½’ä¸€åŒ–
        
        # --- 2. Micro Factors (å¾®è§‚ç›˜å£) ---
        mid = (df['bp1'] + df['sp1']) / 2
        # åŠ æƒå‹åŠ›
        wb = sum(df[f'bv{i}'] * self.weights[i-1] for i in range(1,6))
        wa = sum(df[f'sv{i}'] * self.weights[i-1] for i in range(1,6))
        df['feat_micro_pressure'] = (wb - wa) / (wb + wa + 1e-8)
        # OFI
        price_d = df['price'].diff()
        ofi = np.where(price_d>0, df['tick_vol'], np.where(price_d<0, -df['tick_vol'], 0))
        df['feat_micro_ofi'] = pd.Series(ofi, index=df.index).rolling(3).sum()
        
        # --- 3. Oracle Factors (ä¸Šå¸è§†è§’) ---
        if 'index_price' in df.columns:
            # åŸºå·® (åˆ©ç”¨æ»å)
            df['feat_oracle_basis'] = (df['index_price'] - mid) / mid
            # åŠ¨é‡
            df['feat_oracle_idx_mom'] = df['index_price'].pct_change(2) # 6s change
        
        if 'fut_price' in df.columns:
            df['feat_oracle_fut_lead'] = df['fut_price'].pct_change()
            
        # --- 4. Peer Factors (å…±æŒ¯) ---
        df['feat_peer_diff'] = df['price'].pct_change() - df['peer_price'].pct_change()
        
        return df

    def _make_labels(self, df):
        """ä¸‰é‡å±éšœæ‰“æ ‡"""
        mid = (df['bp1'] + df['sp1']) / 2
        # æœªæ¥ Horizon æ”¶ç›Šç‡
        fwd_ret = mid.shift(-self.cfg['PREDICT_HORIZON']) / mid - 1
        
        labels = np.zeros(len(df))
        labels[fwd_ret > self.cfg['COST_THRESHOLD']] = 1   # Buy
        labels[fwd_ret < -self.cfg['COST_THRESHOLD']] = 2  # Sell
        
        df['label'] = labels
        return df

# ==========================================
# 3. æ··åˆæ·±åº¦æ¨¡å‹ (Hybrid DeepLOB)
# ==========================================
class HybridDeepLOB(nn.Module):
    def __init__(self, num_expert_feats):
        super(HybridDeepLOB, self).__init__()
        
        # A. è§†è§‰æµ (CNNå¤„ç†LOB)
        self.conv_net = nn.Sequential(
            nn.Conv2d(1, 16, (1, 2), stride=(1, 2)), nn.LeakyReLU(), nn.BatchNorm2d(16),
            nn.Conv2d(16, 16, (4, 1)), nn.LeakyReLU(), nn.BatchNorm2d(16),
            nn.Conv2d(16, 16, (4, 1)), nn.LeakyReLU(), nn.BatchNorm2d(16),
        )
        
        # B. é€»è¾‘æµ (MLPå¤„ç†æ‰‹å·¥å› å­)
        self.expert_net = nn.Sequential(
            nn.Linear(num_expert_feats, 32),
            nn.LeakyReLU(),
            nn.BatchNorm1d(32)
        )
        
        # C. èåˆä¸æ—¶åº (LSTM)
        # CNN output approx 16 channels, need to flatten? 
        # DeepLOB standard output is (Batch, Time, Features)
        # ç®€åŒ–å¤„ç†ï¼šå‡è®¾CNNæœ€åè¾“å‡ºç»´åº¦ä¸º 16
        self.lstm = nn.LSTM(input_size=16+32, hidden_size=64, batch_first=True)
        self.classifier = nn.Linear(64, 3) # 3 Classes

    def forward(self, x_lob, x_exp):
        # x_lob: (N, T, 20) -> (N, 1, T, 20)
        x_lob = x_lob.unsqueeze(1)
        
        # CNN Forward
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº† DeepLOB çš„ Inception ç»“æ„ï¼Œç”¨æ ‡å‡† Conv æ¼”ç¤ºåŸç†
        # å®é™… output éœ€è¦ reshape æˆ (N, T, 16)
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾ç»è¿‡å·ç§¯å±‚åï¼Œç‰¹å¾ç»´è¢«å‹ç¼©ï¼Œä¿ç•™æ—¶é—´ç»´
        # åœ¨çœŸå®å®ç°ä¸­éœ€è¦ä»”ç»†è°ƒæ•´ Padding ä»¥ä¿æŒ Time ç»´åº¦ä¸å˜
        
        # Placeholder logic for dimension matching (In real code, calculate padding)
        # è¿™é‡Œä½¿ç”¨ AdaptivePool å¼ºè¡Œå¯¹é½æ—¶é—´ç»´åº¦ (T)ï¼Œä¿è¯æ‹¼æ¥
        feat_cnn = self.conv_net(x_lob) 
        # (N, 16, T', 1) -> (N, T', 16)
        feat_cnn = feat_cnn.permute(0, 2, 1, 3).squeeze(-1)
        
        # å¼ºåˆ¶å¯¹é½æ—¶é—´ç»´åº¦ (å¯èƒ½ä¼šæœ‰å°‘é‡æŸå¤±)
        target_len = x_exp.shape[1]
        feat_cnn = torch.nn.functional.adaptive_avg_pool1d(feat_cnn.permute(0,2,1), target_len).permute(0,2,1)
        
        # Expert Forward
        # Shared weights across time
        B, T, F = x_exp.shape
        feat_exp = self.expert_net(x_exp.reshape(-1, F)).reshape(B, T, -1)
        
        # Fusion
        combined = torch.cat([feat_cnn, feat_exp], dim=2)
        
        # LSTM
        out, _ = self.lstm(combined)
        # Take last step
        return self.classifier(out[:, -1, :])

# ==========================================
# 4. æ•°æ®é›†ä¸è®­ç»ƒå™¨ (Dataset & Trainer)
# ==========================================
class ETFDataset(Dataset):
    def __init__(self, df, lookback, scaler=None):
        self.lookback = lookback
        
        # æå–ç‰¹å¾åˆ—
        self.lob_cols = [f'{s}{i}' for i in range(1,6) for s in ['bp','sp']] + \
                        [f'{s}{i}' for i in range(1,6) for s in ['bv','sv']]
        self.exp_cols = [c for c in df.columns if c.startswith('feat_') or c.startswith('meta_')]
        
        # æ•°æ®é¢„å¤„ç†
        # 1. LOB å½’ä¸€åŒ– (Log Vol, Relative Price)
        mid = (df['bp1'] + df['sp1']) / 2
        lob_data = df[self.lob_cols].copy()
        for c in lob_data.columns:
            if 'b' in c and 'p' in c: lob_data[c] = (lob_data[c] - mid)/mid*10000
            if 'v' in c: lob_data[c] = np.log1p(lob_data[c])
        self.X_lob = lob_data.values.astype(np.float32)
        
        # 2. Expert å½’ä¸€åŒ– (StandardScaler)
        exp_data = df[self.exp_cols].values
        if scaler is None:
            self.scaler = StandardScaler()
            self.X_exp = self.scaler.fit_transform(exp_data).astype(np.float32)
        else:
            self.scaler = scaler
            self.X_exp = self.scaler.transform(exp_data).astype(np.float32)
            
        self.Y = df['label'].values.astype(np.int64)
        
    def __len__(self):
        return len(self.Y) - self.lookback

    def __getitem__(self, idx):
        # Time Window: [i : i+lookback]
        # Label: i+lookback-1 (prediction for next horizon)
        s, e = idx, idx + self.lookback
        return self.X_lob[s:e], self.X_exp[s:e], self.Y[e-1]

def train_model(train_df, val_df, cfg):
    print("\nğŸ§  [Trainer] å¼€å§‹æ„å»ºæ•°æ®é›†ä¸æ¨¡å‹...")
    
    # 1. æ„å»º Dataset
    ds_train = ETFDataset(train_df, cfg['LOOKBACK'])
    ds_val = ETFDataset(val_df, cfg['LOOKBACK'], scaler=ds_train.scaler)
    
    dl_train = DataLoader(ds_train, batch_size=cfg['BATCH_SIZE'], shuffle=True)
    dl_val = DataLoader(ds_val, batch_size=cfg['BATCH_SIZE'], shuffle=False)
    
    # 2. è®¡ç®— Class Weights (è§£å†³æ ·æœ¬ä¸å¹³è¡¡)
    labels = train_df['label'].values
    counts = np.bincount(labels.astype(int))
    # æƒé‡ = æ€»æ•° / (ç±»åˆ«æ•° * é¢‘æ¬¡)
    weights = torch.tensor([sum(counts)/c for c in counts], dtype=torch.float32).to(cfg['DEVICE'])
    print(f"  -> ç±»åˆ«åˆ†å¸ƒ: {counts}")
    print(f"  -> è‡ªåŠ¨æƒé‡: {weights.cpu().numpy()}")
    
    # 3. æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    model = HybridDeepLOB(num_expert_feats=len(ds_train.exp_cols)).to(cfg['DEVICE'])
    criterion = nn.CrossEntropyLoss(weight=weights)
    optimizer = optim.Adam(model.parameters(), lr=cfg['LR'])
    
    # 4. è®­ç»ƒå¾ªç¯
    best_f1 = 0
    
    for epoch in range(cfg['EPOCHS']):
        model.train()
        train_loss = 0
        for x_lob, x_exp, y in dl_train:
            x_lob, x_exp, y = x_lob.to(cfg['DEVICE']), x_exp.to(cfg['DEVICE']), y.to(cfg['DEVICE'])
            
            optimizer.zero_grad()
            pred = model(x_lob, x_exp)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # éªŒè¯
        model.eval()
        all_preds, all_labels = [], []
        with torch.no_grad():
            for x_lob, x_exp, y in dl_val:
                x_lob, x_exp, y = x_lob.to(cfg['DEVICE']), x_exp.to(cfg['DEVICE']), y.to(cfg['DEVICE'])
                pred = model(x_lob, x_exp)
                all_preds.extend(pred.argmax(1).cpu().numpy())
                all_labels.extend(y.cpu().numpy())
        
        # è¯„ä¼°æŠ¥å‘Š
        report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)
        macro_f1 = report['macro avg']['f1-score']
        buy_precision = report['1']['precision']
        
        print(f"Epoch {epoch+1}/{cfg['EPOCHS']} | Loss: {train_loss/len(dl_train):.4f} | "
              f"Val F1: {macro_f1:.4f} | Buy Precision: {buy_precision:.4f}")
        
        if macro_f1 > best_f1:
            best_f1 = macro_f1
            torch.save(model.state_dict(), 'best_model.pth')
            
    print("âœ… è®­ç»ƒå®Œæˆã€‚æœ€ä½³æ¨¡å‹å·²ä¿å­˜ã€‚")

# ==========================================
# 5. ä¸»ç¨‹åº (Main Execution)
# ==========================================
if __name__ == "__main__":
    # 1. ç†”ç‚¼æ•°æ®
    forge = AlphaForge(CONFIG)
    try:
        full_df = forge.load_and_process()
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        exit()
        
    # 2. åˆ‡åˆ†è®­ç»ƒ/éªŒè¯é›† (æŒ‰æ—¶é—´åˆ‡åˆ†ï¼Œä¸¥ç¦ Shuffle)
    split_idx = int(len(full_df) * CONFIG['TRAIN_SPLIT'])
    train_df = full_df.iloc[:split_idx]
    val_df = full_df.iloc[split_idx:]
    
    print(f"\nğŸ“Š æ•°æ®åˆ‡åˆ†: Train={len(train_df)}, Val={len(val_df)}")
    
    # 3. è®­ç»ƒæ¨¡å‹
    train_model(train_df, val_df, CONFIG)