# -*- coding: utf-8 -*-
"""
Alpha System Ultimate (é˜¿å°”æ³•ç³»ç»Ÿ - ç»ˆæå®Œæ•´ç‰ˆ)
-----------------------------------------------
åŠŸèƒ½å…¨é›†ï¼š
1. [æ•°æ®] è‡ªåŠ¨é…å¯¹ sz159920/sh513130ï¼Œæ‰§è¡Œ 3S é‡é‡‡æ ·ä¸æ¸…æ´—ã€‚
2. [å› å­] ç”Ÿæˆå¾®è§‚(Micro)ã€å®è§‚(Oracle)ã€å…±æŒ¯(Peer)ã€çŠ¶æ€(Meta)å››å¤§ç±»å› å­ã€‚
3. [æ ‡ç­¾] Triple Barrier Method (è§¦è¾¾æ­¢ç›ˆ)ï¼Œæ•æ‰è¿‡ç¨‹ä¸­çš„ 0.002 æ³¢åŠ¨ã€‚
4. [æ¨¡å‹] Hybrid DeepLOB (Inception-CNN + MLP + LSTM) åŒæµæ¶æ„ã€‚
5. [å›æµ‹] èµ„é‡‘ç®¡ç†å›æµ‹ (Kelly-style)ï¼ŒæŒ‰ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´ä»“ä½ã€‚
6. [è®­ç»ƒ] è‡ªåŠ¨é€†é¢‘ç‡åŠ æƒ + æ—©åœæœºåˆ¶ + å­¦ä¹ ç‡è¡°å‡ã€‚

@Ver: 7.0 Final Complete
"""

import os
import glob
import warnings
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# å¿½ç•¥ pandas çš„ SettingWithCopyWarning
warnings.filterwarnings('ignore')

# ==========================================
# 1. å…¨å±€é…ç½® (Configuration)
# ==========================================
CONFIG = {
    # --- è·¯å¾„é…ç½® (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹) ---
    'DATA_DIR': './data',          # æ•°æ®æ ¹ç›®å½•
    'MAIN_SYMBOL': 'sz159920',     # ä¸»æ ‡çš„
    'AUX_SYMBOL': 'sh513130',      # è¾…åŠ©æ ‡çš„
    
    # --- å› å­ä¸æ•°æ® ---
    'RESAMPLE_FREQ': '3S',         # 3ç§’é‡é‡‡æ · (å»å™ª+åŒ¹é…æ»å)
    'PREDICT_HORIZON': 60,         # é¢„æµ‹æœªæ¥ 60ä¸ªå‘¨æœŸ (180ç§’)
    'LOOKBACK': 60,                # å›çœ‹çª—å£é•¿åº¦ (180ç§’)
    
    # --- æ ‡ç­¾ç”Ÿæˆ ---
    # [å…³é”®] è®­ç»ƒé—¨æ§›é™è‡³ 0.0012 (è¦†ç›–æˆæœ¬å³å¯)ï¼Œè®©æ¨¡å‹æ•¢äºå¼€ä»“
    'COST_THRESHOLD': 0.0012,   
    
    # --- èµ„é‡‘ç®¡ç†å›æµ‹ ---
    'TRADE_COST': 0.0006,          # å•è¾¹æˆæœ¬ (ä¸‡6, å«ä½£é‡‘+æ»‘ç‚¹)
    'INITIAL_CAPITAL': 20000,      # åˆå§‹æœ¬é‡‘
    'CONF_THRESHOLD': 0.6,         # å¼€ä»“ç½®ä¿¡åº¦é—¨æ§› (æ¦‚ç‡ > 0.6 æ‰å¼€ä»“)
    'MAX_POSITION': 0.8,           # å•ç¬”æœ€å¤§ä»“ä½ (80% æœ¬é‡‘)
    
    # --- è®­ç»ƒå‚æ•° ---
    'BATCH_SIZE': 512,
    'EPOCHS': 50,
    'LR': 1e-4,
    'WEIGHT_DECAY': 1e-5,          # L2æ­£åˆ™åŒ–
    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',
    'PATIENCE': 20,                # æ—©åœè€å¿ƒ
    'WARMUP_EPOCHS': 10,           # çƒ­èº«æœŸ
}

# ==========================================
# 2. æ•°æ®å·¥å‚ï¼šAlpha Forge
# ==========================================
class AlphaForge:
    def __init__(self, cfg):
        self.cfg = cfg
        # ç›˜å£è¡°å‡æƒé‡ (Level 1 -> Level 5)
        self.weights = np.array([1.0, 0.8, 0.6, 0.4, 0.2])

    def load_and_split(self):
        """æ‰«æç›®å½•ï¼Œé…å¯¹æ–‡ä»¶ï¼ŒæŒ‰æ—¥æœŸåˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•é›†"""
        print(f"ğŸš€ [AlphaForge] å¯åŠ¨... æ‰«æ: {self.cfg['DATA_DIR']}")
        
        pairs = self._match_files()
        if len(pairs) < 2:
            raise ValueError(f"æ•°æ®ä¸è¶³ï¼æ‰¾åˆ° {len(pairs)} å¤©æ•°æ®ï¼Œè‡³å°‘éœ€è¦2å¤©è¿›è¡Œå›æµ‹ã€‚")
            
        # æŒ‰æ—¥æœŸæ’åº
        pairs.sort(key=lambda x: x[0])
        
        # æœ€åä¸€å¤©ä½œä¸ºæµ‹è¯•é›† (Walk-forward testing)
        train_pairs = pairs[:-1]
        test_pair = pairs[-1]
        
        print(f"ğŸ“… è®­ç»ƒé›†: {train_pairs[0][0]} ~ {train_pairs[-1][0]} ({len(train_pairs)}å¤©)")
        print(f"ğŸ“… æµ‹è¯•é›†: {test_pair[0]} (1å¤©)")
        
        train_df = self._process_batch(train_pairs)
        test_df = self._process_batch([test_pair])
        
        return train_df, test_df

    def _process_batch(self, pairs):
        dfs = []
        for date, mf, af in pairs:
            try:
                # 1. åŠ è½½ä¸å¯¹é½
                df = self._load_pair(mf, af, date)
                if df is None or len(df) < 200: continue
                
                # 2. è®¡ç®—å› å­
                df = self._calc_factors(df)
                
                # 3. ç”Ÿæˆæ ‡ç­¾ (Triple Barrier)
                df = self._make_labels(df)
                
                # 4. [å…³é”®] æ— ç©·å€¼æ¸…æ´—
                df = df.replace([np.inf, -np.inf], np.nan)
                
                dfs.append(df.dropna())
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡ {date}: {e}")
                
        if not dfs: return pd.DataFrame()
        return pd.concat(dfs).sort_index()

    def _match_files(self):
        """æ ¹æ®æ—¥æœŸåŒ¹é…ä¸»æ ‡çš„å’Œè¾…åŠ©æ ‡çš„çš„æ–‡ä»¶"""
        m_pattern = os.path.join(self.cfg['DATA_DIR'], "**", f"{self.cfg['MAIN_SYMBOL']}*.csv")
        a_pattern = os.path.join(self.cfg['DATA_DIR'], "**", f"{self.cfg['AUX_SYMBOL']}*.csv")
        
        m_files = glob.glob(m_pattern, recursive=True)
        a_files = glob.glob(a_pattern, recursive=True)
        
        def extract_date(path):
            try:
                # å‡è®¾æ ¼å¼åŒ…å« YYYY-MM-DD
                base = os.path.basename(path)
                parts = base.replace('.csv','').split('-')
                # å–æœ€åä¸‰æ®µç»„æˆæ—¥æœŸ
                if len(parts) >= 3:
                    return f"{parts[-3]}-{parts[-2]}-{parts[-1]}"
            except: pass
            return None

        m_map = {extract_date(f): f for f in m_files if extract_date(f)}
        a_map = {extract_date(f): f for f in a_files if extract_date(f)}
        
        common = sorted(list(set(m_map.keys()) & set(a_map.keys())))
        return [(d, m_map[d], a_map[d]) for d in common]

    def _load_pair(self, m_path, a_path, date_str):
        def _read(p):
            d = pd.read_csv(p)
            d['datetime'] = pd.to_datetime(date_str + ' ' + d['tx_server_time'])
            return d.set_index('datetime').sort_index().groupby(level=0).last()
        
        df_m = _read(m_path)
        df_a = _read(a_path)
        
        # èšåˆè§„åˆ™
        agg = {
            'price': 'last', 'tick_vol': 'sum',
            'bp1': 'last', 'sp1': 'last',
            'bp2': 'last', 'sp2': 'last', 'bp3': 'last', 'sp3': 'last',
            'bp4': 'last', 'sp4': 'last', 'bp5': 'last', 'sp5': 'last',
            'bv1': 'last', 'sv1': 'last',
            'bv2': 'last', 'sv2': 'last', 'bv3': 'last', 'sv3': 'last',
            'bv4': 'last', 'sv4': 'last', 'bv5': 'last', 'sv5': 'last',
        }
        # æ£€æŸ¥ä¸Šå¸è§†è§’æ•°æ®
        for c in ['index_price', 'fut_price', 'fut_imb']:
            if c in df_m.columns: agg[c] = 'last'
            
        # é‡é‡‡æ ·
        rule = self.cfg['RESAMPLE_FREQ']
        df_m = df_m.resample(rule).agg(agg)
        df_a = df_a.resample(rule).agg({'price': 'last', 'tick_vol': 'sum'})
        df_a.columns = ['peer_price', 'peer_vol']
        
        # å†…è¿æ¥å¯¹é½
        return df_m.join(df_a, how='inner')

    def _calc_factors(self, df):
        """æ ¸å¿ƒç‰¹å¾å·¥ç¨‹"""
        
        # 1. Meta Factors (æ—¶é—´çŠ¶æ€)
        sec = df.index.hour * 3600 + df.index.minute * 60 + df.index.second
        time_norm = np.where(sec <= 41400, (sec - 34200)/14400, 0.5 + (sec - 46800)/14400)
        df['meta_time'] = np.clip(time_norm, 0, 1)
        
        # 2. Micro Factors (L2 å¾®è§‚)
        mid = (df['bp1'] + df['sp1']) / 2
        df['mid'] = mid
        safe_mid = mid.replace(0, np.nan).fillna(method='ffill')

        wb = sum(df[f'bv{i}']*self.weights[i-1] for i in range(1,6))
        wa = sum(df[f'sv{i}']*self.weights[i-1] for i in range(1,6))
        df['feat_micro_pressure'] = (wb - wa) / (wb + wa + 1e-8)
        
        # 3. Oracle Factors (ä¸Šå¸è§†è§’)
        if 'index_price' in df.columns:
            df['feat_oracle_basis'] = (df['index_price'] - safe_mid) / safe_mid
            df['feat_oracle_idx_mom'] = df['index_price'].pct_change(2)
            
        if 'fut_price' in df.columns:
            df['feat_oracle_fut_lead'] = df['fut_price'].pct_change()
            
        # 4. Peer Factors (å…±æŒ¯)
        df['feat_peer_diff'] = df['price'].pct_change() - df['peer_price'].pct_change()
        
        return df

    def _make_labels(self, df):
        """
        [Triple Barrier Method]
        æ•æ‰è¿‡ç¨‹ä¸­çš„æœ€å¤§æ¶¨è·Œå¹…
        """
        mid = df['mid']
        horizon = self.cfg['PREDICT_HORIZON']
        threshold = self.cfg['COST_THRESHOLD']
        
        # ä½¿ç”¨ Forward Window è·å–æœªæ¥çª—å£å†…çš„ Max/Min
        indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=horizon)
        future_max = mid.rolling(window=indexer).max()
        future_min = mid.rolling(window=indexer).min()
        
        max_ret = future_max / mid - 1
        min_ret = future_min / mid - 1
        
        labels = np.zeros(len(df))
        
        # åªè¦è§¦ç¢°è¿‡æ­¢ç›ˆçº¿ï¼Œå°±è§†ä¸ºæœºä¼š
        mask_buy = max_ret > threshold
        mask_sell = min_ret < -threshold
        
        labels[mask_buy] = 1
        labels[mask_sell] = 2
        
        # å†²çªå¤„ç†ï¼šè°çš„ç©ºé—´å¤§å¬è°çš„
        conflict = mask_buy & mask_sell
        if conflict.any():
            c_max = max_ret[conflict]
            c_min = min_ret[conflict].abs()
            labels[conflict] = np.where(c_max > c_min, 1, 2)
            
        df['label'] = labels
        # ä¿ç•™ Point-to-Point æ”¶ç›Šç”¨äºä¿å®ˆå›æµ‹
        df['real_future_ret'] = mid.shift(-horizon) / mid - 1
        return df

# ==========================================
# 3. æ¨¡å‹æ ¸å¿ƒ: Inception Hybrid
# ==========================================
class InceptionBlock(nn.Module):
    def __init__(self, in_chan, out_chan):
        super().__init__()
        self.b1 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(), nn.BatchNorm2d(out_chan))
        self.b2 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(), 
                                nn.Conv2d(out_chan, out_chan, (3,1), padding=(1,0)), nn.LeakyReLU(), nn.BatchNorm2d(out_chan))
        self.b3 = nn.Sequential(nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(),
                                nn.Conv2d(out_chan, out_chan, (5,1), padding=(2,0)), nn.LeakyReLU(), nn.BatchNorm2d(out_chan))
        self.b4 = nn.Sequential(nn.MaxPool2d((3,1), stride=1, padding=(1,0)),
                                nn.Conv2d(in_chan, out_chan, 1), nn.LeakyReLU(), nn.BatchNorm2d(out_chan))
    def forward(self, x):
        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)

class HybridDeepLOB(nn.Module):
    def __init__(self, num_expert):
        super().__init__()
        
        # A. Visual Stream (LOB)
        # å‹ç¼©å®½åº¦: 20 -> 10 -> 5 -> 1
        self.compress = nn.Sequential(
            nn.Conv2d(1, 16, (1, 2), stride=(1, 2)), nn.LeakyReLU(), nn.BatchNorm2d(16),
            nn.Conv2d(16, 16, (4, 1), padding='same'), nn.LeakyReLU(), nn.BatchNorm2d(16), # Time conv
            nn.Conv2d(16, 16, (1, 2), stride=(1, 2)), nn.LeakyReLU(), nn.BatchNorm2d(16),
            nn.Conv2d(16, 16, (1, 5), stride=(1, 5)), nn.LeakyReLU(), nn.BatchNorm2d(16),
        )
        # Inception (N, 16, T, 1) -> (N, 64, T, 1)
        self.inception = InceptionBlock(16, 16) 
        
        # B. Expert Stream
        self.expert = nn.Sequential(
            nn.Linear(num_expert, 32), nn.LeakyReLU(), nn.BatchNorm1d(32)
        )
        
        # C. Fusion
        self.lstm = nn.LSTM(64 + 32, 128, batch_first=True, dropout=0.3) # å¢åŠ  dropout
        self.head = nn.Linear(128, 3)

    def forward(self, x_lob, x_exp):
        # x_lob: (N, T, 20) -> (N, 1, T, 20)
        x = x_lob.unsqueeze(1)
        
        # 1. å‹ç¼©ç›˜å£ (N, 16, T, 1)
        feat_lob = self.compress(x)
        
        # 2. å¤šå°ºåº¦æ„ŸçŸ¥
        feat_lob = self.inception(feat_lob) # (N, 64, T, 1)
        
        # 3. ç»´åº¦å˜æ¢ (N, T, 64)
        feat_lob = feat_lob.squeeze(-1).permute(0, 2, 1)
        
        # 4. æ—¶é—´å¯¹é½ (Adaptive Pooling)
        if feat_lob.shape[1] != x_exp.shape[1]:
            feat_lob = feat_lob.permute(0, 2, 1) # (N, C, T)
            feat_lob = nn.functional.adaptive_avg_pool1d(feat_lob, x_exp.shape[1])
            feat_lob = feat_lob.permute(0, 2, 1) # (N, T, C)
            
        # 5. å¤„ç†ä¸“å®¶å› å­
        B, T, F = x_exp.shape
        feat_exp = self.expert(x_exp.reshape(-1, F)).reshape(B, T, -1)
        
        # 6. èåˆä¸é¢„æµ‹
        combined = torch.cat([feat_lob, feat_exp], dim=2) # (N, T, 320)
        out, _ = self.lstm(combined)
        return self.head(out[:, -1, :])

# ==========================================
# 4. è®­ç»ƒå¼•æ“ (å¸¦èµ„é‡‘ç®¡ç†å›æµ‹)
# ==========================================
class ETFDataset(Dataset):
    def __init__(self, df, lookback, scaler=None):
        self.lookback = lookback
        
        # LOBåˆ—å
        lob_cols = [f'{s}{i}' for i in range(1,6) for s in ['bp','sp']] + \
                   [f'{s}{i}' for i in range(1,6) for s in ['bv','sv']]
        # ä¸“å®¶å› å­åˆ—å
        exp_cols = [c for c in df.columns if c.startswith('feat_') or c.startswith('meta_')]
        
        # --- å½’ä¸€åŒ– ---
        mid = df['mid'].values.reshape(-1, 1)
        safe_mid = np.where(mid==0, 1.0, mid) 
        
        lob_data = df[lob_cols].values
        lob_data[:, :10] = (lob_data[:, :10] - mid) / safe_mid * 10000
        lob_data[:, 10:] = np.log1p(lob_data[:, 10:])
        
        # äºŒæ¬¡æ¸…æ´—
        lob_data = np.nan_to_num(lob_data, nan=0.0, posinf=0.0, neginf=0.0)
        self.X_lob = lob_data.astype(np.float32)
        
        # Expert Norm
        exp_data = np.nan_to_num(df[exp_cols].values)
        if scaler is None:
            self.scaler = StandardScaler()
            self.X_exp = self.scaler.fit_transform(exp_data).astype(np.float32)
        else:
            self.scaler = scaler
            self.X_exp = self.scaler.transform(exp_data).astype(np.float32)
            
        self.Y = df['label'].values.astype(np.int64)
        self.raw_ret = df['real_future_ret'].values
        
    def __len__(self): return len(self.Y) - self.lookback
    def __getitem__(self, i):
        s, e = i, i + self.lookback
        return self.X_lob[s:e], self.X_exp[s:e], self.Y[e-1], self.raw_ret[e-1]

def backtest_evaluate(model, dataloader, cfg):
    """
    [èµ„é‡‘ç®¡ç†å›æµ‹] 
    Logic: ä¿¡å·è¶Šå¼ºï¼Œä»“ä½è¶Šé‡ (Kelly-style)
    """
    model.eval()
    
    cash = float(cfg['INITIAL_CAPITAL'])
    initial_cap = cash
    cost = cfg['TRADE_COST']
    conf_thresh = cfg['CONF_THRESHOLD']
    max_pos = cfg['MAX_POSITION']
    
    total_trades = 0
    wins = 0
    
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for x_lob, x_exp, y, real_ret in dataloader:
            x_lob, x_exp = x_lob.to(cfg['DEVICE']), x_exp.to(cfg['DEVICE'])
            
            # è·å–æ¦‚ç‡
            logits = model(x_lob, x_exp)
            probs = torch.softmax(logits, dim=1).cpu().numpy()
            real_ret = real_ret.numpy()
            y = y.numpy()
            
            for i in range(len(probs)):
                p_hold, p_buy, p_sell = probs[i]
                
                signal = 0
                confidence = 0.0
                
                # å†³ç­–: æ¦‚ç‡æœ€å¤§ä¸”è¶…è¿‡é˜ˆå€¼
                if p_buy > p_hold and p_buy > p_sell and p_buy > conf_thresh:
                    signal = 1
                    confidence = p_buy
                elif p_sell > p_hold and p_sell > p_buy and p_sell > conf_thresh:
                    signal = 2
                    confidence = p_sell
                
                all_preds.append(signal)
                all_labels.append(y[i])
                
                if signal == 0: continue
                
                # --- ä»“ä½ç®¡ç† ---
                # çº¿æ€§æ˜ å°„: (conf - thresh) / (1 - thresh)
                scale = (confidence - conf_thresh) / (1 - conf_thresh)
                scale = min(scale, max_pos) # å°é¡¶
                
                trade_val = cash * scale
                if trade_val < 2000: continue # èµ„é‡‘å¤ªå°‘ä¸å¼€ä»“(é¿å…æ‰‹ç»­è´¹ç£¨æŸ)
                
                # ç»“ç®—
                direction = 1 if signal == 1 else -1
                pnl = trade_val * (direction * real_ret[i] - 2 * cost)
                
                cash += pnl
                total_trades += 1
                if pnl > 0: wins += 1
                
    pnl_abs = cash - initial_cap
    roi = pnl_abs / initial_cap
    
    print("\n" + "="*40)
    print(f"ğŸ’° [èµ„é‡‘å›æµ‹] åˆå§‹: {initial_cap}")
    if total_trades == 0:
        print("âš ï¸ æ— äº¤æ˜“ (ä¿¡å·å¤ªå¼±)")
        return 0.0
        
    print(f"æœ€ç»ˆå‡€å€¼: {cash:.2f} (ROI: {roi:.2%})")
    print(f"äº¤æ˜“æ¬¡æ•°: {total_trades} | èƒœç‡: {wins/total_trades:.2%}")
    
    rep = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)
    print(f"Buy Precision: {rep['1']['precision']:.2f}")
    print("="*40)
    
    return pnl_abs

def train_system():
    forge = AlphaForge(CONFIG)
    try:
        train_df, test_df = forge.load_and_split()
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # æ ‡ç­¾åˆ†å¸ƒ
    c = np.bincount(train_df['label'].astype(int))
    print(f"ğŸ“Š Triple Barrier æ ‡ç­¾åˆ†å¸ƒ: Hold={c[0]}, Buy={c[1]}, Sell={c[2]}")
    
    ds_train = ETFDataset(train_df, CONFIG['LOOKBACK'])
    ds_test = ETFDataset(test_df, CONFIG['LOOKBACK'], scaler=ds_train.scaler)
    dl_train = DataLoader(ds_train, CONFIG['BATCH_SIZE'], shuffle=True)
    dl_test = DataLoader(ds_test, CONFIG['BATCH_SIZE'], shuffle=False)
    
    model = HybridDeepLOB(ds_train.X_exp.shape[1]).to(CONFIG['DEVICE'])
    
    # æ™ºèƒ½æƒé‡: æ¸©å’Œä¿®æ­£ (1:10:10)
    w_hold = 1.0
    # é˜²æ­¢æƒé‡è¿‡å¤§å¯¼è‡´æ¿€è¿›
    w_buy = min((c[0]/c[1]) * 0.5, 10.0) if c[1] > 0 else 1.0
    w_sell = min((c[0]/c[2]) * 0.5, 10.0) if c[2] > 0 else 1.0
    
    weights = torch.tensor([w_hold, w_buy, w_sell], dtype=torch.float32).to(CONFIG['DEVICE'])
    print(f"âš–ï¸ æ™ºèƒ½ä¿®æ­£æƒé‡: {weights.cpu().numpy()}")
    
    criterion = nn.CrossEntropyLoss(weight=weights)
    
    # [ä¼˜åŒ–] åŠ å…¥æƒé‡è¡°å‡ (L2 æ­£åˆ™)
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['LR'], weight_decay=CONFIG['WEIGHT_DECAY'])
    
    best_pnl = -np.inf
    patience = 0
    # [ä¼˜åŒ–] å¢åŠ è€å¿ƒå’Œçƒ­èº«
    max_patience = 20 
    warmup = 10
    
    print("\nğŸ”¥ å¼€å§‹ç»ˆæè®­ç»ƒ...")
    for epoch in range(CONFIG['EPOCHS']):
        model.train()
        loss_sum = 0
        for x_lob, x_exp, y, _ in dl_train:
            x_lob, x_exp, y = x_lob.to(CONFIG['DEVICE']), x_exp.to(CONFIG['DEVICE']), y.to(CONFIG['DEVICE'])
            optimizer.zero_grad()
            out = model(x_lob, x_exp)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
            loss_sum += loss.item()
            
        print(f"Epoch {epoch+1} | Loss: {loss_sum/len(dl_train):.4f}")
        pnl = backtest_evaluate(model, dl_test, CONFIG)
        
        if pnl > best_pnl:
            best_pnl = pnl
            patience = 0
            torch.save(model.state_dict(), 'alpha_model_v6.pth')
            print(">>> æ–°é«˜! æ¨¡å‹ä¿å­˜.")
        else:
            if epoch >= warmup:
                patience += 1
                print(f"   -> æœªæå‡ ({patience}/{max_patience})")
                if patience >= max_patience:
                    print("ğŸ›‘ æ—©åœ.")
                    break

if __name__ == "__main__":
    train_system()